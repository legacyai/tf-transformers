{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfda9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cb8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/TF_NEW/tf-transformers/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab8ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import glob\n",
    "import datasets\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.data import TFReader, TFWriter\n",
    "from tf_transformers.models import RobertaModel, EncoderDecoder\n",
    "from tf_transformers.losses import cross_entropy_loss, cross_entropy_loss_label_smoothing\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed256f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a810fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def write_tfrecord(data, \n",
    "                    batch_size, \n",
    "                    tokenizer, \n",
    "                    encoder_max_length, \n",
    "                    decoder_max_length, \n",
    "                    mode, \n",
    "                    tfrecord_dir, \n",
    "                    take_sample=False, \n",
    "                    verbose=10000):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        for f in data:            \n",
    "            inputs_hf = tokenizer(f['document'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=encoder_max_length)\n",
    "\n",
    "            input_ids  = inputs_hf['input_ids']\n",
    "            input_mask = inputs_hf['attention_mask']\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            decoder_input_ids = tokenizer(f['summary'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=decoder_max_seq_length)['input_ids']\n",
    "\n",
    "            decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "            result = {}\n",
    "            result['encoder_input_ids'] = input_ids\n",
    "            result['encoder_input_mask'] = input_mask\n",
    "            result['encoder_input_type_ids'] = input_type_ids\n",
    "            result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "            result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "\n",
    "            result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "            result['labels_mask'] = [1] * len(result['labels'])\n",
    "\n",
    "                # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "            yield result\n",
    "\n",
    "    schema = {\n",
    "        \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "        \"labels_mask\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                     )\n",
    "        data_train = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train))\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='eval',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                            )\n",
    "        data_eval = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval))\n",
    "        \n",
    "def read_tfrecord(tfrecord_dir, batch_size, shuffle=False, drop_remainder=False):\n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(tfrecord_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(tfrecord_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['encoder_input_ids', 'encoder_input_type_ids', 'encoder_input_mask', 'decoder_input_ids', 'decoder_input_type_ids']\n",
    "        y_keys = ['labels', 'labels_mask']\n",
    "        dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_remainder=drop_remainder\n",
    "                                          )\n",
    "        return dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8229894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0109c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "def get_model(is_training, use_dropout):\n",
    "    \n",
    "    def model_fn():\n",
    "        encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True, is_training=is_training,\n",
    "                                              use_dropout=use_dropout)\n",
    "        decoder = RobertaModel.from_pretrained(\"roberta-base\",use_decoder=True,\n",
    "                                               return_layer=True, mask_mode='causal', \n",
    "                                              is_training=is_training,\n",
    "                                              use_dropout=use_dropout)\n",
    "\n",
    "        # Assign all possible encoder variables to decoder\n",
    "        encoder_dict = {var.name: var for var in encoder.variables}\n",
    "        assigned_counter = 0\n",
    "        for var in decoder.variables:\n",
    "            if var.name in encoder_dict:\n",
    "                var.assign(encoder_dict[var.name])\n",
    "                assigned_counter += 1\n",
    "        print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "        del encoder_dict\n",
    "        print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "        model = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "        model = model.get_model()\n",
    "        print(\"Model variables {}\".format(len(model.variables)))\n",
    "\n",
    "        del encoder\n",
    "        del decoder\n",
    "\n",
    "        return model\n",
    "    return model_fn\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e646cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "encoder_max_seq_length = 512\n",
    "decoder_max_seq_length = 64\n",
    "\n",
    "take_sample = False\n",
    "train_batch_size = 32\n",
    "eval_batch_size  = 32\n",
    "\n",
    "# Trainer specifics\n",
    "device = \"gpu\"\n",
    "num_gpus = 2\n",
    "tpu_address = None\n",
    "dtype = \"fp32\"\n",
    "epochs = 3\n",
    "strategy = \"mirrored\"\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 1e-5\n",
    "loss_type = None\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = \"scientific_papers\"\n",
    "#num_classes = cfg.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = True\n",
    "use_dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18dc6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 132 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n",
      "Model variables 525\n"
     ]
    }
   ],
   "source": [
    "# Autoregressive model\n",
    "\n",
    "encoder = RobertaModel.from_pretrained(\"roberta-base\", return_layer=True)\n",
    "decoder = RobertaModel.from_pretrained(\"roberta-base\", mask_mode='causal', use_decoder=True, use_auto_regressive=True, return_layer=True)\n",
    "# Assign all possible encoder variables to decoder\n",
    "encoder_dict = {var.name: var for var in encoder.variables}\n",
    "assigned_counter = 0\n",
    "for var in decoder.variables:\n",
    "    if var.name in encoder_dict:\n",
    "        var.assign(encoder_dict[var.name])\n",
    "        assigned_counter += 1\n",
    "print(\"Assigned {} variables from encoder to decoder .\".format(assigned_counter))\n",
    "del encoder_dict\n",
    "print(\"ENncoder variables {} and Decoder variables {}\".format(len(encoder.variables), len(decoder.variables)))\n",
    "model_ar = EncoderDecoder(encoder=encoder, decoder=decoder, share_embeddings=True) \n",
    "print(\"Model variables {}\".format(len(model_ar.variables)))\n",
    "\n",
    "del encoder\n",
    "del decoder\n",
    "\n",
    "# Important\n",
    "model_ar = model_ar.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f61361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tqdm\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "class RougeCallback():\n",
    "    \n",
    "    def __init__(self, model, eval_dataset, original_summaries, tokenizer, eos_id, decoder_start_id, max_iterations):\n",
    "        \n",
    "        with tf.device('/device:GPU:0'):\n",
    "            self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.original_summaries = original_summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos_id = eos_id\n",
    "        self.decoder_start_id = decoder_start_id\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def __call__(self, train_kwargs):\n",
    "        \n",
    "        self.model.set_weights(train_kwargs['model'].get_weights())\n",
    "        decoder = TextDecoder(self.model, decoder_start_token_id=self.decoder_start_id, input_type_ids=0)\n",
    "        \n",
    "        # Predictions\n",
    "        predicted_summaries = []\n",
    "        for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "            del batch_inputs['decoder_input_ids']\n",
    "            decoder_outputs = decoder.decode(batch_inputs, mode='greedy', max_iterations=self.max_iterations, eos_id=self.eos_id)\n",
    "            predicted_ids = [item[0] for item in decoder_outputs['predicted_ids'].numpy().tolist()]\n",
    "\n",
    "            predicted_ids_sliced = []\n",
    "            for p_id in predicted_ids:\n",
    "                if self.eos_id in p_id:\n",
    "                    index = p_id.index(self.eos_id)\n",
    "                    p_id = p_id[:index]\n",
    "                predicted_ids_sliced.append(p_id)\n",
    "                predicted_summaries.append(self.tokenizer.decode(p_id))\n",
    "                \n",
    "        rouge = datasets.load_metric(\"rouge\")\n",
    "        rouge_output2 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        rouge_output1 = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "        rouge_outputL = rouge.compute(predictions=predicted_summaries, references=self.original_summaries, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "\n",
    "        rouge2 = {'rouge2_precision': rouge_output2.precision,\n",
    "                  'rouge2_recall': rouge_output2.recall,\n",
    "                  'rouge2_f1': rouge_output2.fmeasure}\n",
    "        rouge2['rouge1_precision'] = rouge_output1.precision\n",
    "        rouge2['rouge1_recall'] = rouge_output1.recall\n",
    "        rouge2['rouge1_f1'] = rouge_output1.fmeasure\n",
    "\n",
    "        rouge2['rougeL_precision'] = rouge_outputL.precision\n",
    "        rouge2['rougeL_recall'] = rouge_outputL.recall\n",
    "        rouge2['rougeL_f1'] = rouge_outputL.fmeasure\n",
    "        return rouge2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f37752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset scientific_papers (/home/jovyan/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9584162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 119924\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'abstract', 'section_names'],\n",
       "        num_rows: 6658\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a392c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFrecords\n",
    "# tfrecord_dir = tempfile.mkdtemp()\n",
    "tfrecord_dir = '/tmp/roberta2robera_tfrecordsxsum/'\n",
    "\n",
    "# # Train Tfrecords\n",
    "# write_tfrecord(dataset['train'], \n",
    "#                train_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"train\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "# # # Eval Tfrecords\n",
    "# write_tfrecord(dataset['validation'], \n",
    "#                eval_batch_size,\n",
    "#                tokenizer, \n",
    "#                encoder_max_seq_length, \n",
    "#                decoder_max_seq_length, \n",
    "#                \"eval\", \n",
    "#                tfrecord_dir, \n",
    "#                take_sample)\n",
    "\n",
    "train_dataset, total_train_examples = read_tfrecord(tfrecord_dir + 'train', train_batch_size, shuffle=False, drop_remainder=False)\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', eval_batch_size,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "# original_summaries = [item['summary'] for item in dataset['validation']]\n",
    "# callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "#                          tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8bb5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf47961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer fn\n",
    "\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, learning_rate_type=\"polynomial\"):\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    \n",
    "    def optimizer_fn():\n",
    "        optimizer, learning_rate_fn = create_optimizer(learning_rate,\n",
    "                                                   num_train_steps,\n",
    "                                                   num_train_steps, \n",
    "                                                      learning_rate_type=learning_rate_type)\n",
    "        return optimizer\n",
    "    return optimizer_fn\n",
    "\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs, learning_rate_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c13c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fn\n",
    "\n",
    "def get_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss_label_smoothing(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915ec83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4761ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "from tf_transformers.core import GPUTrainer\n",
    "trainer = GPUTrainer(distribution_strategy=strategy, \n",
    "                    num_gpus=num_gpus, \n",
    "                    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164db72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cff6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = get_model(is_training=True, use_dropout=True)\n",
    "train_loss_fn = get_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335868b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c0e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f74d8391670>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/roberta-base/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 204 variables from encoder to decoder .\n",
      "ENncoder variables 204 and Decoder variables 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No checkpoint found in /tmp/roberta2robera_pubmed/\n",
      "Epoch 1/3 --- Step 100/3700 --- total examples 0:   0%|          | 0/37 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model variables 525\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0', 'tf_transformers/roberta/mlm/transform/dense/kernel:0', 'tf_transformers/roberta/mlm/transform/dense/bias:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/roberta/mlm/transform/LayerNorm/beta:0', 'tf_transformers/roberta/mlm/transform/bias:0', 'tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 515 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/3 --- Step 3700/3700 --- total examples 115200: 100%|██████████| 37/37 [1:00:42<00:00, 98.44s/batch , learning_rate=1.91e-6, loss=3.3]\n",
      "INFO:absl:Model saved at epoch 1\n",
      "Epoch 2/3 --- Step 3700/3700 --- total examples 233600: 100%|██████████| 37/37 [59:02<00:00, 95.74s/batch , learning_rate=3.84e-6, loss=2.83]\n",
      "INFO:absl:Model saved at epoch 2\n",
      "Epoch 3/3 --- Step 3700/3700 --- total examples 352000: 100%|██████████| 37/37 [59:03<00:00, 95.78s/batch , learning_rate=5.78e-6, loss=2.6] \n",
      "INFO:absl:Model saved at epoch 3\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/tmp/roberta2robera_pubmed/\"\n",
    "history = trainer.run(\n",
    "    model_fn = model_fn,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = train_loss_fn,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = 3700,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=False,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b2513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c0967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f74d84986d0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f74cc050f40>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f74d84986d0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f74cc050f40>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/roberta2robera_pubmed/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "model_ar.load_checkpoint(model_checkpoint_dir)\n",
    "decoder = TextDecoder(model=model_ar, \n",
    "                     decoder_start_token_id=tokenizer.cls_token_id, \n",
    "                     input_type_ids=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620b9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:12, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "import tqdm\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', 16,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "predicted_summaries = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "    del batch_inputs['decoder_input_ids']\n",
    "    decoder_outputs = decoder.decode(batch_inputs, mode='beam', \n",
    "                                     num_beams=4,\n",
    "                                                                      alpha=2.0,\n",
    "\n",
    "                                     do_sample=False,\n",
    "                                     max_iterations=64, eos_id=tokenizer.sep_token_id)\n",
    "    predicted_ids = [item[0] for item in decoder_outputs['predicted_ids'].numpy().tolist()]\n",
    "    \n",
    "    predicted_ids_sliced = []\n",
    "    for p_id in predicted_ids:\n",
    "        if tokenizer.sep_token_id in p_id:\n",
    "            index = p_id.index(tokenizer.sep_token_id)\n",
    "            p_id = p_id[:index]\n",
    "        predicted_ids_sliced.append(p_id)\n",
    "        predicted_summaries.append(tokenizer.decode(p_id))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dcf4283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The UK government has said he is to be \"a\" in a \"un\" of a \"un\" in a \"un-year-old man has been jailed for the UK.',\n",
       " 'The first-year-old man who died in the first-year-old man in the first time.',\n",
       " 'A man who died in the first-year-year-old man has been jailed for the first time.',\n",
       " 'A man has been arrested on the first time in the first time.',\n",
       " 'The UK government has said it is to the UK to the first time.',\n",
       " \"Wales has said it is to the UK's first time in the first time in the UK.\",\n",
       " 'A man has been charged with a man in a man who died at a man who died after a man was found.',\n",
       " 'The first time in the first time in the first time, the first time in the first time.',\n",
       " 'A man has been charged with the first-year-year-year-year-old man has been jailed for a new contract.',\n",
       " \"The UK government has said he will not be the UK to the UK's first time.\",\n",
       " \"The government has said it is to the UK's first time in the UK's first time.\",\n",
       " 'A man has been charged with the season-year contract with the season-year contract.',\n",
       " 'The UK government has said he is to be a new new \"un\" in the UK,000.',\n",
       " 'A man has been charged with a man who died after a man who was found in a man.',\n",
       " 'A man has been charged with a man who died after a man who was found in a new contract.',\n",
       " 'A man has been charged in the first time in the first time in the first time.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47f76c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f030abb01c0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f02903eb130>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f030abb01c0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f02903eb130>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/roberta2robera_xsum2/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "model_ar.load_checkpoint(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c5490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce1f8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "with open(\"{}/history.json\".format(model_checkpoint_dir), \"w\") as f:\n",
    "    json.dump(str(history),f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "732aeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "model_checkpoint_dir = \"/tmp/roberta2robera_pubmed_short/\"\n",
    "shutil.rmtree(model_checkpoint_dir)\n",
    "# shutil.rmtree(tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87c1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d982b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
