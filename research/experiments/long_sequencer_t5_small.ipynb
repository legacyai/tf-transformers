{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b6e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daba6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/home/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801234ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647d34cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 26 09:31:32 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    43W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac5085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4c578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1d0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_transformers.models import T5Model, EncoderDecoder\n",
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1be145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da7a7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.core import LegacyModel, LegacyLayer\n",
    "class Long_Model(LegacyLayer):\n",
    "    def __init__(\n",
    "        self, model_layer, num_splits,\n",
    "        gru_units,\n",
    "        activation=None, is_training=False, use_dropout=False, **kwargs\n",
    "    ):\n",
    "        super(Long_Model, self).__init__(\n",
    "            is_training=is_training, use_dropout=use_dropout, name=model_layer.name, **kwargs\n",
    "        )\n",
    "        self.model_layer = model_layer\n",
    "        self.num_splits = num_splits\n",
    "        self.gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units, return_sequences=True,\n",
    "                                                                           name='gru_for_logits', trainable=True))\n",
    "        # self.gru_layer_token_embeddings = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units, return_sequences=True))\n",
    "        \n",
    "        self._config_dict = model_layer._config_dict\n",
    "        self._mask_mode   = model_layer._mask_mode\n",
    "        self._sequence_length = model_layer._sequence_length\n",
    "        self.model_inputs, self.model_outputs = self.get_model(initialize_only=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        all_outputs_token_embeddings = []\n",
    "        inputs_splitted = {}\n",
    "        input_names = []\n",
    "        for k, v in inputs.items():\n",
    "            inputs_splitted[k] = tf.split(v, self.num_splits, axis=1)\n",
    "            input_names.append(k)\n",
    "            \n",
    "        for i in range(self.num_splits):\n",
    "            inputs_main = {}\n",
    "            for name in input_names:\n",
    "                inputs_main[name] = inputs_splitted[name][i]\n",
    "            model_outputs = self.model_layer(inputs_main)\n",
    "            # all_outputs_token_logits.append(model_outputs[\"token_logits\"])\n",
    "            all_outputs_token_embeddings.append(model_outputs['token_embeddings'])\n",
    "            \n",
    "        # token_logits_concatanted = tf.concat(all_outputs_token_logits, axis=1) # over sequence length\n",
    "\n",
    "        token_embeddings_concatanted = tf.concat(all_outputs_token_embeddings, axis=1) # over sequence length\n",
    "        token_embeddings_concatanted = self.gru_layer(token_embeddings_concatanted)\n",
    "        return {'token_embeddings': token_embeddings_concatanted}\n",
    "    \n",
    " \n",
    "    def get_model(self, initialize_only=False):\n",
    "        inputs = {}\n",
    "        for k, v in self.model_layer.model_inputs.items():\n",
    "            shape = v.shape\n",
    "            inputs[k] = tf.keras.layers.Input(\n",
    "                shape[1:], batch_size=shape[0], name= k, dtype=v.dtype\n",
    "            )\n",
    "        layer_output = self(inputs)\n",
    "        if initialize_only:\n",
    "            return inputs, layer_output\n",
    "        model = LegacyModel(inputs=inputs, outputs=layer_output, name=\"long_span_selection\")\n",
    "        model.model_config = self.model_layer._config_dict\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef51ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_splits, gru_units):\n",
    "    \n",
    "    def model_fn():\n",
    "        model = T5Model.from_pretrained(\"t5-small\", \n",
    "                                               return_layer=True)\n",
    "        encoder = model._encoder\n",
    "        decoder = model._decoder\n",
    "        del model\n",
    "        long_model = Long_Model(encoder, num_splits=num_splits, gru_units=gru_units)\n",
    "\n",
    "        # long_model._layers[0]._embedding_layer = decoder._embedding_layer\n",
    "        #long_model._layers[0]._type_embeddings_layer = albert_decoder._type_embeddings_layer\n",
    "        #long_model._layers[0]._positional_embedding_layer = albert_decoder._positional_embedding_layer\n",
    "\n",
    "        model_encoder = EncoderDecoder(encoder=encoder, decoder=decoder) \n",
    "\n",
    "        model_encoder = model_encoder.get_model()\n",
    "        return model_encoder\n",
    "    return model_fn\n",
    "    \n",
    "    \n",
    "def get_model_auto_regressive():\n",
    "    gru_units = 384 # half of hidden dimension\n",
    "    model = T5Model.from_pretrained(\"t5-small\", \n",
    "                                           return_layer=True, \n",
    "                                           decoder_kwargs={'use_auto_regressive': True})\n",
    "    encoder = model._encoder\n",
    "    decoder = model._decoder\n",
    "    del model\n",
    "    long_model = Long_Model(encoder, num_splits=8, gru_units=gru_units)\n",
    "    \n",
    "    # long_model._layers[0]._embedding_layer = decoder._embedding_layer\n",
    "    #long_model._layers[0]._type_embeddings_layer = albert_decoder._type_embeddings_layer\n",
    "    #long_model._layers[0]._positional_embedding_layer = albert_decoder._positional_embedding_layer\n",
    "    \n",
    "    model_encoder = EncoderDecoder(encoder=encoder, decoder=decoder) \n",
    "    \n",
    "    model_encoder = model_encoder.get_model()\n",
    "    return model_encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd1deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2432ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de64defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def write_tfrecord(data, \n",
    "                    batch_size, \n",
    "                    tokenizer, \n",
    "                    encoder_max_length, \n",
    "                    decoder_max_length, \n",
    "                    mode, \n",
    "                    tfrecord_dir, \n",
    "                    take_sample=False, \n",
    "                    verbose=10000):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        for f in data:            \n",
    "            inputs_hf = tokenizer('long summarize: ' + f['article'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=encoder_max_length)\n",
    "\n",
    "            input_ids  = inputs_hf['input_ids'][:-1] # skip sep\n",
    "            input_mask = inputs_hf['attention_mask'][:-1] # skip sep\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            decoder_input_ids = tokenizer(f['abstract'], \n",
    "                                  truncation=True, \n",
    "                                  max_length=decoder_max_seq_length)['input_ids']\n",
    "            \n",
    "            decoder_input_ids = [tokenizer.pad_token_id] + decoder_input_ids\n",
    "            # decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "            result = {}\n",
    "            result['encoder_input_ids'] = input_ids\n",
    "            result['encoder_input_mask'] = input_mask\n",
    "            #result['encoder_input_type_ids'] = input_type_ids\n",
    "            result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "            #result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "\n",
    "            result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "            result['labels_mask'] = [1] * len(result['labels'])\n",
    "\n",
    "                # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "            yield result\n",
    "\n",
    "    schema = {\n",
    "        \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "        \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "        \"labels_mask\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                     )\n",
    "        data_train = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train))\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'pubmed'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='eval',\n",
    "                            overwrite=True,\n",
    "                            verbose_counter=verbose\n",
    "                            )\n",
    "        data_eval = data\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval))\n",
    "        \n",
    "def read_tfrecord(tfrecord_dir, max_seq_length, batch_size, shuffle=False, drop_remainder=False):\n",
    "    \n",
    "        padded_shapes = {'encoder_input_ids': [max_seq_length,], \n",
    "                        'encoder_input_mask':[max_seq_length,],\n",
    "                        'decoder_input_ids': [None,],\n",
    "                        'labels': [None,], \n",
    "                        'labels_mask': [None,]\n",
    "                }\n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(tfrecord_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(tfrecord_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['encoder_input_ids', 'encoder_input_mask', 'decoder_input_ids']\n",
    "        y_keys = ['labels', 'labels_mask']\n",
    "        dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           padded_shapes=padded_shapes,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=shuffle, \n",
    "                                           drop_remainder=drop_remainder\n",
    "                                          )\n",
    "        return dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e287e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "encoder_max_seq_length = 256\n",
    "decoder_max_seq_length = 256\n",
    "\n",
    "take_sample = False\n",
    "train_batch_size = 8\n",
    "eval_batch_size  = 8\n",
    "\n",
    "# Trainer specifics\n",
    "device = \"gpu\"\n",
    "num_gpus = 2\n",
    "tpu_address = None\n",
    "dtype = \"fp32\"\n",
    "epochs = 3\n",
    "strategy = \"mirrored\"\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 3e-5\n",
    "loss_type = None\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = \"scientific_papers\"\n",
    "#num_classes = cfg.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = False\n",
    "use_dropout = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265988e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from tf_transformers.data import TFWriter, TFReader\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2465ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905f51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa3417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scientific_papers (/home/jovyan/.cache/huggingface/datasets/scientific_papers/pubmed/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"scientific_papers\", \"pubmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ad69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFrecords\n",
    "# tfrecord_dir = tempfile.mkdtemp()\n",
    "tfrecord_dir = '/tmp/tfrecord_t5_pubmed_long/'\n",
    "\n",
    "# # Train Tfrecords\n",
    "write_tfrecord(dataset['train'], \n",
    "               train_batch_size,\n",
    "               tokenizer, \n",
    "               encoder_max_seq_length, \n",
    "               decoder_max_seq_length, \n",
    "               \"train\", \n",
    "               tfrecord_dir, \n",
    "               take_sample, \n",
    "               verbose=1000)\n",
    "\n",
    "# # Eval Tfrecords\n",
    "write_tfrecord(dataset['validation'], \n",
    "               eval_batch_size,\n",
    "               tokenizer, \n",
    "               encoder_max_seq_length, \n",
    "               decoder_max_seq_length, \n",
    "               \"eval\", \n",
    "               tfrecord_dir, \n",
    "               take_sample, \n",
    "              verbose=1000)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, total_train_examples = read_tfrecord(tfrecord_dir + 'train', encoder_max_seq_length, train_batch_size, shuffle=True, drop_remainder=True)\n",
    "eval_dataset, total_eval_examples   = read_tfrecord(tfrecord_dir + 'eval', encoder_max_seq_length,  eval_batch_size,  shuffle=False, drop_remainder=False)\n",
    "\n",
    "# original_summaries = [item['summary'] for item in dataset['validation']]\n",
    "# callback = RougeCallback( model_ar, eval_dataset, original_summaries,\n",
    "#                          tokenizer, tokenizer.sep_token_id, tokenizer.cls_token_id, decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02333a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af1385d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer fn\n",
    "\n",
    "from tf_transformers.optimization.adam_weighted import AdamWeightDecay\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, learning_rate_type=\"polynomial\"):\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "    \n",
    "    def optimizer_fn():\n",
    "        optimizer = AdamWeightDecay(learning_rate = learning_rate)\n",
    "        return optimizer\n",
    "    return optimizer_fn\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs, learning_rate_type=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24f215ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.losses import cross_entropy_loss, cross_entropy_loss_label_smoothing\n",
    "\n",
    "# Loss fn\n",
    "def get_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss_label_smoothing(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5df063ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Load trainer\n",
    "from tf_transformers.core import GPUTrainer\n",
    "strategy = 'mirrored'\n",
    "num_gpus = 2\n",
    "dtype = 'fp32'\n",
    "trainer = GPUTrainer(distribution_strategy=strategy, \n",
    "                    num_gpus=num_gpus, \n",
    "                    dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4868e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = total_train_examples//train_batch_size\n",
    "model_fn = get_model(num_splits=8, gru_units=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09659cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493135a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "207bd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = \"/tmp/model_t5_long/\"\n",
    "history = trainer.run(\n",
    "    model_fn = model_fn,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = get_loss,\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=10,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=False,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a16fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2534bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "042fa0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logs',\n",
       " 'ckpt-2.index',\n",
       " 'ckpt-1.index',\n",
       " 'ckpt-1.data-00000-of-00001',\n",
       " 'ckpt-2.data-00000-of-00001',\n",
       " 'checkpoint']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ed1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = \"/tmp/model_t5_long/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ad8e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/model_t5_long/ckpt-2'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc62f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48b6eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f62307e7d00> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f62307cee80>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f62307e7d00> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f62307cee80>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/t5-small/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f65b7d82fa0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f62307cee80>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7f65b7d82fa0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f62307cee80>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/model_t5_long/ckpt-2\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/tmp/model_t5_long/ckpt-1'\n",
    "with tf.device('/device:GPU:1'):\n",
    "\n",
    "    model_ar = get_model_auto_regressive()\n",
    "    model_ar.load_checkpoint(model_checkpoint_dir, checkpoint_path=checkpoint_path)\n",
    "\n",
    "    # Save as serialized module\n",
    "    model_ar.save_as_serialize_module(\"/tmp/model_t5_long_serialized_v1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369246c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model_pb = tf.saved_model.load(\"/tmp/model_t5_long_serialized/\")\n",
    "decoder = TextDecoder(model_pb, decoder_start_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a6cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoder\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model_pb = tf.saved_model.load(\"/tmp/model_t5_long_serialized/\")\n",
    "decoder = TextDecoder(model_pb, decoder_start_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67017106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e6b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "947it [4:07:00, 14.91s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "predicted_summaries = []\n",
    "predicted_summaries_256 = []\n",
    "original_summaries = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "    \n",
    "    del batch_inputs['decoder_input_ids'] # We do not need to pass decoder_input_ids , as we provide while initiating\n",
    "                                          # TextDecoder\n",
    "    decoder_outputs = decoder.decode(batch_inputs, \n",
    "                                 max_iterations=512, \n",
    "                                 mode='beam',\n",
    "                                 num_beams=5, \n",
    "                                 alpha=0.8,\n",
    "                                 eos_id=1)\n",
    "    \n",
    "    predicted_batch_summaries = tokenizer.batch_decode(decoder_outputs['predicted_ids'][:,0,:].numpy(),\n",
    "                                                       skip_special_tokens=True)\n",
    "    predicted_summaries.extend(predicted_batch_summaries)\n",
    "    \n",
    "    predicted_batch_summaries_256 = tokenizer.batch_decode(decoder_outputs['predicted_ids'][:,0,:].numpy()[:, :256],\n",
    "                                                       skip_special_tokens=True)\n",
    "    predicted_summaries_256.extend(predicted_batch_summaries_256)\n",
    "    \n",
    "    original_batch_summaries = tokenizer.batch_decode(batch_labels['labels'].numpy(), skip_special_tokens=True)\n",
    "    original_summaries.extend(original_batch_summaries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083e735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "predicted_summaries = []\n",
    "predicted_summaries_256 = []\n",
    "original_summaries = []\n",
    "max_length = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(eval_dataset):\n",
    "    \n",
    "    del batch_inputs['decoder_input_ids'] # We do not need to pass decoder_input_ids , as we provide while initiating\n",
    "                                          # TextDecoder\n",
    "    decoder_outputs = decoder.decode(batch_inputs, \n",
    "                                 max_iterations=384, \n",
    "                                 mode='greedy',\n",
    "                                 eos_id=1)\n",
    "    \n",
    "    preds = decoder_outputs['predicted_ids'][:,0,:].numpy()\n",
    "    \n",
    "    max_length.append(preds.shape[1])\n",
    "    \n",
    "    predicted_batch_summaries = tokenizer.batch_decode(preds,\n",
    "                                                       skip_special_tokens=True)\n",
    "    predicted_summaries.extend(predicted_batch_summaries)\n",
    "    \n",
    "    predicted_batch_summaries_256 = tokenizer.batch_decode(preds[:, :256],\n",
    "                                                       skip_special_tokens=True)\n",
    "    predicted_summaries_256.extend(predicted_batch_summaries_256)\n",
    "    \n",
    "    original_batch_summaries = tokenizer.batch_decode(batch_labels['labels'].numpy(), skip_special_tokens=True)\n",
    "    original_summaries.extend(original_batch_summaries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee4c492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from rouge_score import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9c20d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True)\n",
    "aggregator = scoring.BootstrapAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7533f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6633it [02:22, 46.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for (p_summary, o_summary) in tqdm.tqdm(zip(predicted_summaries, original_summaries)):\n",
    "    score = scorer.score(o_summary, p_summary)\n",
    "    aggregator.add_scores(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f4965be",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = aggregator.aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "538c4c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1-R,0.450247,0.453271,0.456391\n",
      "\n",
      "rouge1-P,0.350808,0.354154,0.357254\n",
      "\n",
      "rouge1-F,0.382401,0.385470,0.388405\n",
      "\n",
      "-------------------------------------------------------\n",
      "rouge2-R,0.177629,0.180571,0.183634\n",
      "\n",
      "rouge2-P,0.138939,0.141300,0.143710\n",
      "\n",
      "rouge2-F,0.151427,0.153809,0.156428\n",
      "\n",
      "-------------------------------------------------------\n",
      "rougeLsum-R,0.283254,0.285934,0.288525\n",
      "\n",
      "rougeLsum-P,0.218282,0.220623,0.222851\n",
      "\n",
      "rougeLsum-F,0.238778,0.241002,0.243262\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(score.items()):\n",
    "    print(\"%s-R,%f,%f,%f\\n\" %\n",
    "          (k, v.low.recall, v.mid.recall, v.high.recall))\n",
    "    print(\"%s-P,%f,%f,%f\\n\" %\n",
    "          (k, v.low.precision, v.mid.precision, v.high.precision))\n",
    "    print(\"%s-F,%f,%f,%f\\n\" %\n",
    "          (k, v.low.fmeasure, v.mid.fmeasure, v.high.fmeasure))\n",
    "    print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049e828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Greedy (ckpt 2) scores\n",
    "\n",
    "rouge1-R,0.450247,0.453271,0.456391\n",
    "\n",
    "rouge1-P,0.350808,0.354154,0.357254\n",
    "\n",
    "rouge1-F,0.382401,0.385470,0.388405\n",
    "\n",
    "-------------------------------------------------------\n",
    "rouge2-R,0.177629,0.180571,0.183634\n",
    "\n",
    "rouge2-P,0.138939,0.141300,0.143710\n",
    "\n",
    "rouge2-F,0.151427,0.153809,0.156428\n",
    "\n",
    "-------------------------------------------------------\n",
    "rougeLsum-R,0.283254,0.285934,0.288525\n",
    "\n",
    "rougeLsum-P,0.218282,0.220623,0.222851\n",
    "\n",
    "rougeLsum-F,0.238778,0.241002,0.243262\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
