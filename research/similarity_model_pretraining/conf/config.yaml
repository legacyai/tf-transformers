data:
    data_directory:
    train_batch_size: 32
    eval_batch_size: 32
task:
  max_seq_len: 128
  max_predictions_per_seq: 40
trainer:
  dtype: fp32
  num_gpus: 2
  tpu_address:
  epochs: 3
  strategy: mirrored
  steps_per_epoch: 10000
  model_checkpoint_dir:
  callback_steps: 1000
optimizer:
  learning_rate: 5e-4
  warmup_rate: 0.1
  learning_rate_type: cosine
  loss_type:
  use_constant_lr: false
model:
  is_training: true
  use_dropout: true
  num_layers: 12
