{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48609eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/TF_NEW/tf-transformers/src/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa13617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f258a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import glob\n",
    "import datasets\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from tf_transformers.data import TFReader, TFWriter\n",
    "from tf_transformers.models import Classification_Model\n",
    "from tf_transformers.losses import cross_entropy_loss_for_classification\n",
    "from model import get_model, get_tokenizer, get_optimizer, get_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506894e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca963cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'train_batch_size': 32, 'eval_batch_size': 64, 'take_sample': True, 'max_seq_length': 128}, 'trainer': {'type': 'gpu', 'dtype': 'fp32', 'num_gpus': 2, 'tpu_address': None, 'epochs': 3, 'strategy': 'mirrored'}, 'optimizer': {'learning_rate': 2e-05, 'loss_type': None}, 'model': {'is_training': True, 'use_dropout': True}, 'glue': {'task': {'name': 'mrpc'}, 'data': {'name': 'mrpc', 'num_classes': 2}}}\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path=\"conf/\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[\"data.take_sample=true\", \"+glue=mrpc\"])\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffd728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e04b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps\n",
    "\n",
    "# 1. Download the data\n",
    "# 2. Prepare TFRecords\n",
    "# 3. Read TFrecords to tf.data\n",
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c165f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5dc13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_model(num_classes, return_all_layer_outputs, is_training, use_dropout):\n",
    "        \n",
    "    def model_fn():\n",
    "        model = get_model(return_all_layer_outputs, is_training, use_dropout)\n",
    "        classification_model = Classification_Model(model,\n",
    "                                                    num_classes, \n",
    "                                                    use_all_layers=return_all_layer_outputs, \n",
    "                                                    is_training=is_training, \n",
    "                                                    use_dropout=use_dropout)\n",
    "        classification_model = classification_model.get_model()\n",
    "        return classification_model\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3077591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to features using specific length\n",
    "# into a temp dir (and log it as well for monitoring)\n",
    "\n",
    "def get_dataset(data, batch_size, tokenizer, max_seq_length, mode, tfrecord_dir, take_sample=False):\n",
    "    \n",
    "    if mode not in [\"train\", \"eval\"]:\n",
    "        raise ValueError(\"Inavlid mode `{}` specified. Available mode is ['train', 'eval']\".format(mode))\n",
    "    \n",
    "    def get_tfrecord_example(data):\n",
    "        result = {}\n",
    "        for f in data:\n",
    "            input_ids_s1 = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence1'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "            input_ids_s1 = tokenizer.convert_tokens_to_ids(input_ids_s1)\n",
    "            input_type_ids_s1 = [0] * len(input_ids_s1) # 0 for s1\n",
    "\n",
    "            input_ids_s2 = tokenizer.tokenize(f['sentence2'])[: max_seq_length-1] + [tokenizer.sep_token] # -1 to add SEP\n",
    "            input_ids_s2 = tokenizer.convert_tokens_to_ids(input_ids_s2)\n",
    "            input_type_ids_s2 = [1] * len(input_ids_s2)\n",
    "            \n",
    "            # concatanate two sentences\n",
    "            input_ids =  input_ids_s1 + input_ids_s2\n",
    "            input_type_ids = input_type_ids_s1 + input_type_ids_s2\n",
    "            input_mask = [1] * len(input_ids) # 1 for s2\n",
    "            \n",
    "            result = {}\n",
    "            result['input_ids'] = input_ids\n",
    "            result['input_mask'] = input_mask\n",
    "            result['input_type_ids'] = input_type_ids\n",
    "\n",
    "            result['labels'] = f['label']\n",
    "            yield result\n",
    "            \n",
    "    schema = {\n",
    "        \"input_ids\": (\"var_len\", \"int\"),\n",
    "        \"input_mask\": (\"var_len\", \"int\"),\n",
    "        \"input_type_ids\": (\"var_len\", \"int\"),\n",
    "        \"labels\": (\"var_len\", \"int\"),\n",
    "    }\n",
    "    \n",
    "    # Create a temp dir\n",
    "    if mode == \"train\":\n",
    "        # Write tf records\n",
    "        train_data_dir = os.path.join(tfrecord_dir,\"train\")        \n",
    "        tfrecord_filename = 'mrpc'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=train_data_dir,\n",
    "                            tag='train',\n",
    "                            overwrite=False\n",
    "                     )\n",
    "        data_train = data['train']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_train = data_train.select(range(500))\n",
    "            \n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_train))\n",
    "        \n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(train_data_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(train_data_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(train_data_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "        y_keys = ['labels']\n",
    "        train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=True, \n",
    "                                           drop_remainder=True\n",
    "                                          )\n",
    "        return train_dataset, stats['total_records']\n",
    "    if mode == \"eval\":\n",
    "        # Write tfrecords\n",
    "        eval_data_dir = os.path.join(tfrecord_dir,\"eval\")\n",
    "        tfrecord_filename = 'mrpc'\n",
    "        tfwriter = TFWriter(schema=schema, \n",
    "                            file_name=tfrecord_filename, \n",
    "                            model_dir=eval_data_dir,\n",
    "                            tag='dev',\n",
    "                            overwrite=False\n",
    "                            )\n",
    "        data_eval = data['validation']\n",
    "        # Take sample\n",
    "        if take_sample:\n",
    "            data_eval = data_eval.select(range(500))\n",
    "        tfwriter.process(parse_fn=get_tfrecord_example(data_eval))\n",
    "        \n",
    "        \n",
    "        # Read tfrecord to dataset\n",
    "        schema = json.load(open(\"{}/schema.json\".format(eval_data_dir)))\n",
    "        stats  = json.load(open('{}/stats.json'.format(eval_data_dir)))\n",
    "        all_files = glob.glob(\"{}/*.tfrecord\".format(eval_data_dir))\n",
    "        tf_reader = TFReader(schema=schema, \n",
    "                            tfrecord_files=all_files)\n",
    "\n",
    "        x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "        y_keys = ['labels']\n",
    "        eval_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                           keys=x_keys,\n",
    "                                           batch_size=batch_size, \n",
    "                                           x_keys = x_keys, \n",
    "                                           y_keys = y_keys,\n",
    "                                           shuffle=False, \n",
    "                                           drop_remainder=False\n",
    "                                          )\n",
    "        return eval_dataset, stats['total_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925bdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss_type):\n",
    "\n",
    "    if loss_type and loss_type == 'joint':\n",
    "\n",
    "        def loss_fn(y_true_dict, y_pred_dict):\n",
    "            \"\"\"Joint loss over all layers\"\"\"\n",
    "            loss_dict = {}\n",
    "            loss_holder = []\n",
    "            for layer_count, per_layer_output in enumerate(y_pred_dict['class_logits']):\n",
    "\n",
    "                loss = cross_entropy_loss_for_classification(\n",
    "                    labels=tf.squeeze(y_true_dict['labels'], axis=1),\n",
    "                    logits=per_layer_output\n",
    "                )\n",
    "                loss_dict['loss_{}'.format(layer_count + 1)] = loss\n",
    "                loss_holder.append(loss)\n",
    "            # Mean over batch\n",
    "            loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "            return loss_dict\n",
    "\n",
    "    else:\n",
    "\n",
    "        def loss_fn(y_true_dict, y_pred_dict):\n",
    "            \"\"\"last layer loss\"\"\"\n",
    "            loss_dict = {}\n",
    "            loss = cross_entropy_loss_for_classification(\n",
    "                labels=tf.squeeze(y_true_dict['labels'], axis=1),\n",
    "                logits=y_pred_dict['class_logits']\n",
    "            )\n",
    "            loss_dict['loss'] = loss\n",
    "            return loss_dict\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f213e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f83593c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'train_batch_size': 32, 'eval_batch_size': 64, 'take_sample': True, 'max_seq_length': 128}, 'trainer': {'type': 'gpu', 'dtype': 'fp32', 'num_gpus': 2, 'tpu_address': None, 'epochs': 3, 'strategy': 'mirrored'}, 'optimizer': {'learning_rate': 2e-05, 'loss_type': None}, 'model': {'is_training': True, 'use_dropout': True}, 'glue': {'task': {'name': 'mrpc'}, 'data': {'name': 'mrpc', 'num_classes': 2}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10419221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd6ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909c9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data specific configuration\n",
    "max_seq_len = cfg.data.max_seq_length\n",
    "take_sample = cfg.data.take_sample\n",
    "max_seq_length = cfg.data.max_seq_length\n",
    "train_batch_size = cfg.data.train_batch_size\n",
    "eval_batch_size  = cfg.data.eval_batch_size\n",
    "\n",
    "# Trainer specifics\n",
    "device = cfg.trainer.type\n",
    "num_gpus = cfg.trainer.num_gpus\n",
    "tpu_address = cfg.trainer.tpu_address\n",
    "dtype = cfg.trainer.dtype\n",
    "epochs = cfg.trainer.epochs\n",
    "strategy = cfg.trainer.strategy\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = cfg.optimizer.learning_rate\n",
    "loss_type = cfg.optimizer.loss_type\n",
    "return_all_layer_outputs = False\n",
    "if loss_type and loss_type == 'joint':\n",
    "    return_all_layer_outputs = True\n",
    "\n",
    "# Core data specifics\n",
    "data_name = cfg_task.glue.data.name\n",
    "num_classes = cfg_task.glue.data.num_classes\n",
    "\n",
    "# Model specific\n",
    "is_training = cfg.model.is_training\n",
    "use_dropout = cfg.model.use_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3728a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c6f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jovyan/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "INFO:absl:Total individual observations/examples written is 500 in 0.33753085136413574 seconds\n",
      "INFO:absl:All writer objects closed\n",
      "INFO:absl:Total individual observations/examples written is 500 in 0.3376038074493408 seconds\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Load data\n",
    "data = datasets.load_dataset(\"glue\", data_name)\n",
    "tfrecord_dir = tempfile.mkdtemp()\n",
    "\n",
    "train_dataset, total_train_examples = get_dataset(data, train_batch_size,tokenizer, max_seq_length, \"train\", tfrecord_dir, take_sample)\n",
    "eval_dataset, total_eval_examples  = get_dataset(data, eval_batch_size,tokenizer, max_seq_len, \"eval\", tfrecord_dir, take_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791c58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimizer\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, train_batch_size, epochs)\n",
    "\n",
    "# Load trainer\n",
    "# trainer = get_trainer(device, dtype, strategy, num_gpus, tpu_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8e7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0e86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model function\n",
    "model_fn = get_classification_model(num_classes, \n",
    "                                 return_all_layer_outputs, is_training, use_dropout)\n",
    "# Load loss function \n",
    "train_loss_fn = get_loss(loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a024a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1419cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 95) (32, 1)\n"
     ]
    }
   ],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs['input_ids'].shape, batch_labels['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "161dce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def call(trainer_kwargs):\n",
    "        \n",
    "        for k, v in trainer_kwargs.items():\n",
    "            print(k, '-->', v)\n",
    "callback = Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f25af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9e113b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 TF-Transformers Authors.\n",
    "# All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from absl import logging\n",
    "\n",
    "from tf_transformers.core import keras_utils\n",
    "from tf_transformers.core.distribute_utils import get_distribution_strategy\n",
    "from tf_transformers.core.performance_utils import (\n",
    "    configure_optimizer,\n",
    "    get_tf_dtype,\n",
    "    is_float16,\n",
    "    set_mixed_precision_policy,\n",
    ")\n",
    "\n",
    "\n",
    "def flat_metric_dict(metric_dict):\n",
    "    \"\"\"Flatten the dict\"\"\"\n",
    "    dict_flatten = {}\n",
    "    dict_flatten['steps'] = list(metric_dict.keys())\n",
    "    for _key, value in metric_dict.items():\n",
    "        for sub_key, sub_value in value.items():\n",
    "            if sub_key not in dict_flatten:\n",
    "                dict_flatten[sub_key] = [sub_value]\n",
    "            else:\n",
    "                dict_flatten[sub_key].append(sub_value)\n",
    "    return dict_flatten\n",
    "\n",
    "\n",
    "def save_model_checkpoints(model, overwrite_checkpoint_dir, model_checkpoint_dir, max_number_of_models):\n",
    "    # Model checkpoint\n",
    "    if not overwrite_checkpoint_dir:\n",
    "        import os\n",
    "\n",
    "        if os.path.exists(model_checkpoint_dir):\n",
    "            raise FileExistsError(\"Model directory exists\")\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory=model_checkpoint_dir, max_to_keep=max_number_of_models)\n",
    "    return manager\n",
    "\n",
    "\n",
    "def get_loss_metric_dict(training_loss_names, validation_loss_names):\n",
    "\n",
    "    training_loss_dict_metric = {name: tf.keras.metrics.Mean(name, dtype=tf.float32) for name in training_loss_names}\n",
    "    training_loss_dict_metric[\"learning_rate\"] = tf.keras.metrics.Mean(\n",
    "        \"learning_rate\", dtype=tf.float32\n",
    "    )  # We store learning rate here and reset after every global steps\n",
    "\n",
    "    validation_loss_dict_metric = {}\n",
    "    if validation_loss_names:\n",
    "        validation_loss_dict_metric = {\n",
    "            name: tf.keras.metrics.Mean(name, dtype=tf.float32) for name in validation_loss_names\n",
    "        }\n",
    "\n",
    "    return training_loss_dict_metric, validation_loss_dict_metric\n",
    "\n",
    "\n",
    "def get_and_reset_metric_from_dict(metric_dict):\n",
    "    if not metric_dict:\n",
    "        return {}\n",
    "    metric_result = {name: metric.result().numpy() for name, metric in metric_dict.items()}\n",
    "    for _name, metric in metric_dict.items():\n",
    "        metric.reset_states()\n",
    "    return metric_result\n",
    "\n",
    "\n",
    "def get_tensorboard_writers(model_checkpoint_dir):\n",
    "    train_log_dir = model_checkpoint_dir + \"/logs/train\"\n",
    "    test_log_dir = model_checkpoint_dir + \"/logs/dev\"\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    return train_summary_writer, test_summary_writer\n",
    "\n",
    "\n",
    "def train_and_eval(\n",
    "    model,\n",
    "    optimizer,\n",
    "    strategy,\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    steps_per_call,\n",
    "    train_dataset_iter,\n",
    "    train_loss_fn,\n",
    "    GLOBAL_BATCH_SIZE,\n",
    "    training_loss_dict_metric,\n",
    "    validation_dataset_distributed,\n",
    "    validation_loss_fn,\n",
    "    validation_loss_dict_metric,\n",
    "    validation_interval_steps,\n",
    "    mixed_precision,\n",
    "    callbacks,\n",
    "    callbacks_interval_steps,\n",
    "    trainer_kwargs,\n",
    "    checkpoint_manager,\n",
    "    model_checkpoint_dir,\n",
    "    model_save_interval_steps,\n",
    "):\n",
    "    def save_model(epoch_end=False):\n",
    "        if not epoch_end:\n",
    "            if model_save_interval_steps:\n",
    "                if global_step % model_save_interval_steps == 0:\n",
    "                    checkpoint_manager.save()\n",
    "                    logging.info(\"Model saved at step {}\".format(global_step))\n",
    "        else:\n",
    "            checkpoint_manager.save()\n",
    "            logging.info(\"Model saved at epoch {}\".format(epoch))\n",
    "\n",
    "    # @tf.function(experimental_relax_shapes=True)\n",
    "    def write_metrics(metric_dict, writer, step):\n",
    "        # @tf.function\n",
    "        def _write(step):\n",
    "            # other model code would go here\n",
    "            with writer.as_default():\n",
    "                for name, result in metric_dict.items():\n",
    "                    tf.summary.scalar(name, result, step=step)\n",
    "\n",
    "        _write(step)\n",
    "        writer.flush()\n",
    "\n",
    "    def compute_loss(batch_labels, model_outputs):\n",
    "        \"\"\"Loss computation which takes care of loss reduction based on GLOBAL_BATCH_SIZE\"\"\"\n",
    "        per_example_loss = train_loss_fn(batch_labels, model_outputs)\n",
    "        per_example_loss_averaged = {}\n",
    "        # Inplace update\n",
    "        # Avergae loss per global batch size , recommended\n",
    "        for name, loss in per_example_loss.items():\n",
    "            per_example_loss_averaged[name] = tf.nn.compute_average_loss(loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        return per_example_loss_averaged\n",
    "\n",
    "    def compute_loss_valid(batch_labels, model_outputs):\n",
    "        \"\"\"Validation Loss computation which takes care of loss reduction based on GLOBAL_BATCH_SIZE\"\"\"\n",
    "        per_example_loss = validation_loss_fn(batch_labels, model_outputs)\n",
    "        per_example_loss_averaged = {}\n",
    "        # Inplace update\n",
    "        # Avergae loss per global batch size , recommended\n",
    "        for name, loss in per_example_loss.items():\n",
    "            per_example_loss_averaged[name] = tf.nn.compute_average_loss(loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        return per_example_loss_averaged\n",
    "\n",
    "    # Train Functions\n",
    "    @tf.function\n",
    "    def do_train(iterator):\n",
    "        \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "        def train_step(dist_inputs):\n",
    "            \"\"\"The computation to run on each device.\"\"\"\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                model_outputs = model(batch_inputs)\n",
    "                loss = compute_loss(batch_labels, model_outputs)\n",
    "                tf.debugging.check_numerics(loss['loss'], message='Loss value is either NaN or inf')\n",
    "                if isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                    loss_scaled = {name: optimizer.get_scaled_loss(loss_value) for name, loss_value in loss.items()}\n",
    "                # TODO\n",
    "                # Scales down the loss for gradients to be invariant from replicas.\n",
    "                # loss = loss / strategy.num_replicas_in_sync\n",
    "            if mixed_precision:\n",
    "                scaled_gradients = tape.gradient(loss_scaled[\"loss\"], model.trainable_variables)\n",
    "                grads = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "            else:\n",
    "                grads = tape.gradient(loss[\"loss\"], model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "            return loss\n",
    "\n",
    "        for _ in tf.range(tf.convert_to_tensor(steps_per_call)):\n",
    "            dist_inputs = next(iterator)\n",
    "            loss = strategy.run(train_step, args=(dist_inputs,))\n",
    "            # strategy reduce\n",
    "            loss = {\n",
    "                name: strategy.reduce(tf.distribute.ReduceOp.MEAN, loss_value, axis=None)\n",
    "                for name, loss_value in loss.items()\n",
    "            }\n",
    "            for name, loss_value in loss.items():\n",
    "                training_loss = training_loss_dict_metric[name]\n",
    "                training_loss.update_state(loss_value)\n",
    "            # Get current learning rate\n",
    "            if isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                current_lr = optimizer._optimizer._decayed_lr(tf.float32)\n",
    "            else:\n",
    "                current_lr = optimizer._decayed_lr(tf.float32)\n",
    "            training_loss_dict_metric[\"learning_rate\"].update_state(current_lr)\n",
    "            # training_result = get_and_reset_metric_from_dict(training_loss_dict_metric)\n",
    "\n",
    "    # do validation\n",
    "    def do_validation(validation_dataset_distributed):\n",
    "        \"\"\"Validation step\"\"\"\n",
    "\n",
    "        @tf.function\n",
    "        def _validate_step(dist_inputs):\n",
    "\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            model_outputs = model(batch_inputs)\n",
    "            loss = compute_loss_valid(batch_labels, model_outputs)\n",
    "            return loss\n",
    "\n",
    "        if not epoch_end:\n",
    "            if (\n",
    "                validation_dataset_distributed\n",
    "                and validation_loss_fn\n",
    "                and validation_interval_steps\n",
    "                and (global_step % validation_interval_steps == 0)\n",
    "            ):\n",
    "                logging.info(\"Validation in progress at step {} . . . .\".format(global_step))\n",
    "                with tqdm.tqdm(validation_dataset_distributed, unit=\" Val batch \") as val_batches:\n",
    "                    for dist_inputs in val_batches:\n",
    "                        loss = strategy.run(_validate_step, args=(dist_inputs,))\n",
    "                        for name, loss_value in loss.items():\n",
    "                            loss_value = strategy.reduce(tf.distribute.ReduceOp.SUM, loss_value, axis=None)\n",
    "                            validation_loss = validation_loss_dict_metric[name]\n",
    "                            validation_loss.update_state(loss_value)\n",
    "\n",
    "                validation_result = get_and_reset_metric_from_dict(validation_loss_dict_metric)\n",
    "                validation_history[global_step] = validation_result\n",
    "                write_metrics(validation_result, val_summary_writer, global_step)\n",
    "                logging.info(\"Validation result at step {}\".format(validation_result))\n",
    "                print(\"\\n\")\n",
    "        else:\n",
    "            if validation_dataset_distributed and validation_loss_fn:\n",
    "                logging.info(\"Validation in progress at epoch end {} . . . .\".format(epoch))\n",
    "                with tqdm.tqdm(validation_dataset_distributed, unit=\" Val batch \") as val_batches:\n",
    "                    for dist_inputs in val_batches:\n",
    "                        loss = strategy.run(_validate_step, args=(dist_inputs,))\n",
    "                        for name, loss_value in loss.items():\n",
    "                            loss_value = strategy.reduce(tf.distribute.ReduceOp.SUM, loss_value, axis=None)\n",
    "                            validation_loss = validation_loss_dict_metric[name]\n",
    "                            validation_loss.update_state(loss_value)\n",
    "\n",
    "                validation_result = get_and_reset_metric_from_dict(validation_loss_dict_metric)\n",
    "                write_metrics(validation_result, val_summary_writer, global_step)\n",
    "                # validation_history[global_step] = validation_result\n",
    "                logging.info(\"Validation result at epoch {} is {}\".format(epoch, validation_result))\n",
    "                print(\"\\n\")\n",
    "\n",
    "    def do_callbacks(callbacks):\n",
    "        \"\"\"Call callbacks\"\"\"\n",
    "        if not epoch_end:\n",
    "            callback_scores = None\n",
    "            if callbacks and callbacks_interval_steps:\n",
    "                logging.info(\"Callbacks in progress at step {} . . . .\".format(global_step))\n",
    "                callback_scores = []\n",
    "                for callback, callback_steps in zip(callbacks, callbacks_interval_steps):\n",
    "                    if callback_steps and (global_step % callback_steps == 0):\n",
    "                        score = callback(trainer_kwargs)\n",
    "                        callback_scores.append(score)\n",
    "                    else:\n",
    "                        callback_scores.append(None)\n",
    "            return callback_scores\n",
    "        else:\n",
    "            callback_scores = None\n",
    "            if callbacks:\n",
    "                logging.info(\"Callbacks in progress at epoch end {} . . . .\".format(epoch))\n",
    "                callback_scores = []\n",
    "                for callback in callbacks:\n",
    "                    score = callback(trainer_kwargs)\n",
    "                    callback_scores.append(score)\n",
    "\n",
    "                    # Try to write a callback scores (only on epoch end)\n",
    "                    # If we are returning a dict like {'exact_match': 81} or\n",
    "                    # {'rougue-1': 30} etc . . . .\n",
    "                    if score and isinstance(score, dict):\n",
    "                        write_metrics(score, val_summary_writer, epoch)\n",
    "            return callback_scores\n",
    "\n",
    "    # Loop starts here\n",
    "    # Get Tensorboard writers\n",
    "    train_summary_writer, val_summary_writer = get_tensorboard_writers(model_checkpoint_dir)\n",
    "    validation_history = {}\n",
    "    training_history = {}\n",
    "    global_step = 0\n",
    "    epoch_end = False\n",
    "    STEPS = steps_per_epoch // steps_per_call\n",
    "    print(\"STEPS\", STEPS)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # start_epoch_time = time.time()\n",
    "        with tqdm.trange(STEPS, unit=\"batch \") as tepoch:\n",
    "            for step in tepoch:\n",
    "                steps_covered = (step + 1) * steps_per_call\n",
    "                global_step += steps_per_call\n",
    "                print(\"Started epoch {} and step {}\".format(epoch, global_step))\n",
    "                tepoch.set_description(\n",
    "                    \"Epoch {}/{} --- Step {}/{} --- \".format(epoch, epochs, steps_covered, steps_per_epoch)\n",
    "                )\n",
    "                # Call Train\n",
    "                do_train(train_dataset_iter)\n",
    "                print(\"Train done\")\n",
    "                # Call Validation\n",
    "                do_validation(validation_dataset_distributed)\n",
    "                print(\"Val done\")\n",
    "                # Call Callbacks\n",
    "                callback_scores = do_callbacks(callbacks)\n",
    "\n",
    "                # Train Metrics\n",
    "                training_result = get_and_reset_metric_from_dict(training_loss_dict_metric)\n",
    "                training_history[global_step] = training_result\n",
    "                write_metrics(training_result, train_summary_writer, global_step)\n",
    "                # training_result[\"learning_rate\"] = learning_rate_holder.result().numpy()\n",
    "                # learning_rate_holder.reset_states()\n",
    "                tepoch.set_postfix(**training_result)\n",
    "\n",
    "                # Save model\n",
    "                save_model()\n",
    "\n",
    "        # Do after every epoch\n",
    "        epoch_end = True\n",
    "        save_model(epoch_end)\n",
    "        #do_validation(validation_dataset_distributed)\n",
    "        #callback_scores = do_callbacks(callbacks)\n",
    "        epoch_end = False\n",
    "\n",
    "    # Flatten the results\n",
    "    training_history = flat_metric_dict(training_history)\n",
    "    validation_history = flat_metric_dict(validation_history)\n",
    "    return training_history, validation_history, callback_scores\n",
    "\n",
    "\n",
    "class GPUTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        distribution_strategy,\n",
    "        num_gpus=0,\n",
    "        all_reduce_alg=None,\n",
    "        num_packs=1,\n",
    "        tpu_address=None,\n",
    "        dtype='fp32',\n",
    "        loss_scale='dynamic',\n",
    "    ):\n",
    "\n",
    "        self.distribution_strategy = get_distribution_strategy(\n",
    "            distribution_strategy=distribution_strategy,\n",
    "            num_gpus=num_gpus,\n",
    "            all_reduce_alg=all_reduce_alg,\n",
    "            num_packs=num_packs,\n",
    "            tpu_address=tpu_address,\n",
    "        )\n",
    "\n",
    "        self.num_replicas = self.distribution_strategy.num_replicas_in_sync\n",
    "        self._dtype = get_tf_dtype(dtype)\n",
    "\n",
    "        # Setting dtype policy\n",
    "        set_mixed_precision_policy(self._dtype)\n",
    "        self.use_float16 = is_float16(self._dtype)\n",
    "        self.loss_scale = loss_scale\n",
    "\n",
    "        # # TODO\n",
    "        # if self.use_tpu:\n",
    "        # params[\"num_replicas\"] = self.distribution_strategy.num_replicas_in_sync\n",
    "        # else:\n",
    "        # logging.info(\"Running transformer with num_gpus = %d\", num_gpus)\n",
    "\n",
    "        # Add keras utils threads\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        model_fn,\n",
    "        optimizer_fn,\n",
    "        train_dataset,\n",
    "        train_loss_fn,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        model_checkpoint_dir,\n",
    "        batch_size,\n",
    "        training_loss_names=None,\n",
    "        validation_loss_names=None,\n",
    "        validation_dataset=None,\n",
    "        validation_loss_fn=None,\n",
    "        validation_interval_steps=None,\n",
    "        steps_per_call=100,\n",
    "        enable_xla=True,\n",
    "        callbacks=None,\n",
    "        callbacks_interval_steps=None,\n",
    "        overwrite_checkpoint_dir=False,\n",
    "        max_number_of_models=10,\n",
    "        model_save_interval_steps=None,\n",
    "        repeat_dataset=True,\n",
    "        latest_checkpoint=None,\n",
    "    ):\n",
    "\n",
    "        if steps_per_epoch:\n",
    "            logging.info(\"Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\")\n",
    "        if callbacks:\n",
    "            if callbacks_interval_steps is None:\n",
    "                callbacks_interval_steps = [None for callback in callbacks]\n",
    "            assert len(callbacks) == len(callbacks_interval_steps)\n",
    "\n",
    "        # Enable XLA\n",
    "        keras_utils.set_session_config(enable_xla=enable_xla)\n",
    "        logging.info(\"Policy: ----> {}\".format(keras_utils.get_policy_name()))\n",
    "        logging.info(\"Strategy: ---> {}\".format(self.distribution_strategy))\n",
    "        logging.info(\"Num GPU Devices: ---> {}\".format(self.distribution_strategy.num_replicas_in_sync))\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # Under Strategy Scope\n",
    "        with self.distribution_strategy.scope():\n",
    "            # Model\n",
    "            model = model_fn()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = optimizer_fn()\n",
    "\n",
    "            optimizer = configure_optimizer(optimizer, use_float16=self.use_float16, loss_scale=self.loss_scale)\n",
    "\n",
    "        # We use this to avoid inferring names from loss functions\n",
    "        _training_loss_names = ['loss']\n",
    "        _validation_loss_names = ['loss']\n",
    "        if training_loss_names:\n",
    "            _training_loss_names += training_loss_names\n",
    "        if validation_loss_names:\n",
    "            _validation_loss_names += validation_loss_names\n",
    "        # Make unique names\n",
    "        training_loss_names = list(set(_training_loss_names))\n",
    "        validation_loss_names = list(set(_validation_loss_names))\n",
    "        # Checkpoint manager\n",
    "        checkpoint_manager = save_model_checkpoints(\n",
    "            model, overwrite_checkpoint_dir, model_checkpoint_dir, max_number_of_models\n",
    "        )\n",
    "\n",
    "        # Try to load latest checkpoint\n",
    "        model.load_checkpoint(checkpoint_dir=model_checkpoint_dir, checkpoint_path=latest_checkpoint, opt=optimizer)\n",
    "\n",
    "        # Get metric dicts before distributing the dataset\n",
    "        # ddistributed datasets has no attribute .take\n",
    "        training_loss_dict_metric, validation_loss_dict_metric = get_loss_metric_dict(\n",
    "            training_loss_names, validation_loss_names\n",
    "        )\n",
    "        # Distribute dataset\n",
    "        if not repeat_dataset:\n",
    "            train_dataset_distributed = self.distribution_strategy.experimental_distribute_dataset(\n",
    "                train_dataset.repeat(epochs + 1)\n",
    "            )\n",
    "        else:\n",
    "            train_dataset_distributed = self.distribution_strategy.experimental_distribute_dataset(\n",
    "                train_dataset.repeat()\n",
    "            )\n",
    "        validation_dataset_distributed = None\n",
    "        if validation_dataset:\n",
    "            validation_dataset_distributed = self.distribution_strategy.experimental_distribute_dataset(\n",
    "                validation_dataset\n",
    "            )\n",
    "\n",
    "        # Make train dataset iterator\n",
    "        train_dataset_distributed = iter(train_dataset_distributed)\n",
    "\n",
    "        history = {}\n",
    "        training_history, validation_history, callback_scores = train_and_eval(\n",
    "            model,\n",
    "            optimizer,\n",
    "            self.distribution_strategy,\n",
    "            epochs,\n",
    "            steps_per_epoch,\n",
    "            steps_per_call,\n",
    "            train_dataset_distributed,\n",
    "            train_loss_fn,\n",
    "            batch_size,\n",
    "            training_loss_dict_metric,\n",
    "            validation_dataset_distributed,\n",
    "            validation_loss_fn,\n",
    "            validation_loss_dict_metric,\n",
    "            validation_interval_steps,\n",
    "            self.use_float16,\n",
    "            callbacks,\n",
    "            callbacks_interval_steps,\n",
    "            locals(),\n",
    "            checkpoint_manager,\n",
    "            model_checkpoint_dir,\n",
    "            model_save_interval_steps,\n",
    "        )\n",
    "        history['training_history'] = training_history\n",
    "        history['validation_hsitory'] = validation_history\n",
    "        history['callbacks'] = callback_scores\n",
    "\n",
    "        # Save json\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55fe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca05c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_strategy = \"mirrored\"\n",
    "num_gpus = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "224b1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "trainer = GPUTrainer(\n",
    "        distribution_strategy,\n",
    "        num_gpus=num_gpus,\n",
    "        all_reduce_alg=None,\n",
    "        num_packs=1,\n",
    "        tpu_address=None,\n",
    "        dtype='fp32',\n",
    "        loss_scale='dynamic',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a3053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f1ea9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7ff612223790>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type albert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/albert-base-v2/ckpt-1\n",
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No checkpoint found in /tmp/model_ckpt/\n",
      "Epoch 1/2 --- Step 1/100 --- :   0%|          | 0/100 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS 100\n",
      "Started epoch 1 and step 1\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:absl:Callbacks in progress at step 1 . . . .\n",
      "Epoch 1/2 --- Step 2/100 --- :   1%|          | 1/100 [00:22<37:38, 22.81s/batch , learning_rate=4.44e-7, loss=0.426]INFO:absl:Callbacks in progress at step 2 . . . .\n",
      "Epoch 1/2 --- Step 3/100 --- :   2%|         | 2/100 [00:22<26:08, 16.01s/batch , learning_rate=8.89e-7, loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 2\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 3 . . . .\n",
      "Epoch 1/2 --- Step 4/100 --- :   3%|         | 3/100 [00:23<18:10, 11.24s/batch , learning_rate=1.33e-6, loss=0.413]INFO:absl:Callbacks in progress at step 4 . . . .\n",
      "Epoch 1/2 --- Step 5/100 --- :   4%|         | 4/100 [00:23<12:38,  7.90s/batch , learning_rate=1.78e-6, loss=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 4\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 5 . . . .\n",
      "Epoch 1/2 --- Step 6/100 --- :   5%|         | 5/100 [00:23<08:49,  5.57s/batch , learning_rate=2.22e-6, loss=0.364]INFO:absl:Callbacks in progress at step 6 . . . .\n",
      "Epoch 1/2 --- Step 7/100 --- :   6%|         | 6/100 [00:23<06:09,  3.93s/batch , learning_rate=2.67e-6, loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 6\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 7 . . . .\n",
      "Epoch 1/2 --- Step 8/100 --- :   7%|         | 7/100 [00:23<04:19,  2.79s/batch , learning_rate=3.11e-6, loss=0.379]INFO:absl:Callbacks in progress at step 8 . . . .\n",
      "Epoch 1/2 --- Step 9/100 --- :   8%|         | 8/100 [00:23<03:03,  1.99s/batch , learning_rate=3.56e-6, loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 8\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 9 . . . .\n",
      "Epoch 1/2 --- Step 10/100 --- :   9%|         | 9/100 [00:23<02:10,  1.43s/batch , learning_rate=4e-6, loss=0.345]  INFO:absl:Callbacks in progress at step 10 . . . .\n",
      "Epoch 1/2 --- Step 11/100 --- :  10%|         | 10/100 [00:23<01:33,  1.04s/batch , learning_rate=4.44e-6, loss=0.357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 10\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 11 . . . .\n",
      "Epoch 1/2 --- Step 12/100 --- :  11%|         | 11/100 [00:24<01:08,  1.31batch /s, learning_rate=4.89e-6, loss=0.341]INFO:absl:Callbacks in progress at step 12 . . . .\n",
      "Epoch 1/2 --- Step 13/100 --- :  12%|        | 12/100 [00:24<00:50,  1.75batch /s, learning_rate=5.33e-6, loss=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 12\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 13 . . . .\n",
      "Epoch 1/2 --- Step 14/100 --- :  13%|        | 13/100 [00:24<00:37,  2.29batch /s, learning_rate=5.78e-6, loss=0.375]INFO:absl:Callbacks in progress at step 14 . . . .\n",
      "Epoch 1/2 --- Step 15/100 --- :  14%|        | 14/100 [00:24<00:29,  2.93batch /s, learning_rate=6.22e-6, loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 14\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 15 . . . .\n",
      "Epoch 1/2 --- Step 16/100 --- :  15%|        | 15/100 [00:24<00:23,  3.63batch /s, learning_rate=6.67e-6, loss=0.318]INFO:absl:Callbacks in progress at step 16 . . . .\n",
      "Epoch 1/2 --- Step 17/100 --- :  16%|        | 16/100 [00:24<00:19,  4.37batch /s, learning_rate=7.11e-6, loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 16\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 17 . . . .\n",
      "Epoch 1/2 --- Step 18/100 --- :  17%|        | 17/100 [00:24<00:16,  4.99batch /s, learning_rate=7.56e-6, loss=0.322]INFO:absl:Callbacks in progress at step 18 . . . .\n",
      "Epoch 1/2 --- Step 19/100 --- :  18%|        | 18/100 [00:24<00:14,  5.66batch /s, learning_rate=8e-6, loss=0.264]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 18\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 19 . . . .\n",
      "Epoch 1/2 --- Step 20/100 --- :  19%|        | 19/100 [00:25<00:12,  6.24batch /s, learning_rate=8.44e-6, loss=0.351]INFO:absl:Callbacks in progress at step 20 . . . .\n",
      "Epoch 1/2 --- Step 21/100 --- :  20%|        | 20/100 [00:25<00:11,  6.74batch /s, learning_rate=8.89e-6, loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 20\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 21 . . . .\n",
      "Epoch 1/2 --- Step 22/100 --- :  21%|        | 21/100 [00:25<00:10,  7.28batch /s, learning_rate=9.33e-6, loss=0.323]INFO:absl:Callbacks in progress at step 22 . . . .\n",
      "Epoch 1/2 --- Step 23/100 --- :  22%|       | 22/100 [00:25<00:10,  7.63batch /s, learning_rate=9.78e-6, loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 22\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 23 . . . .\n",
      "Epoch 1/2 --- Step 24/100 --- :  23%|       | 23/100 [00:25<00:09,  7.77batch /s, learning_rate=1.02e-5, loss=0.31] INFO:absl:Callbacks in progress at step 24 . . . .\n",
      "Epoch 1/2 --- Step 25/100 --- :  24%|       | 24/100 [00:25<00:09,  8.01batch /s, learning_rate=1.07e-5, loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 24\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 25 . . . .\n",
      "Epoch 1/2 --- Step 26/100 --- :  25%|       | 25/100 [00:25<00:09,  8.10batch /s, learning_rate=1.11e-5, loss=0.302]INFO:absl:Callbacks in progress at step 26 . . . .\n",
      "Epoch 1/2 --- Step 27/100 --- :  26%|       | 26/100 [00:25<00:09,  8.11batch /s, learning_rate=1.16e-5, loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 26\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 27 . . . .\n",
      "Epoch 1/2 --- Step 28/100 --- :  27%|       | 27/100 [00:25<00:08,  8.13batch /s, learning_rate=1.2e-5, loss=0.304] INFO:absl:Callbacks in progress at step 28 . . . .\n",
      "Epoch 1/2 --- Step 29/100 --- :  28%|       | 28/100 [00:26<00:08,  8.14batch /s, learning_rate=1.24e-5, loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 28\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 29 . . . .\n",
      "Epoch 1/2 --- Step 30/100 --- :  29%|       | 29/100 [00:26<00:08,  8.27batch /s, learning_rate=1.29e-5, loss=0.329]INFO:absl:Callbacks in progress at step 30 . . . .\n",
      "Epoch 1/2 --- Step 31/100 --- :  30%|       | 30/100 [00:26<00:08,  8.16batch /s, learning_rate=1.33e-5, loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 30\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 31 . . . .\n",
      "Epoch 1/2 --- Step 32/100 --- :  31%|       | 31/100 [00:26<00:08,  8.17batch /s, learning_rate=1.38e-5, loss=0.279]INFO:absl:Callbacks in progress at step 32 . . . .\n",
      "Epoch 1/2 --- Step 33/100 --- :  32%|      | 32/100 [00:26<00:08,  8.17batch /s, learning_rate=1.42e-5, loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 32\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 33 . . . .\n",
      "Epoch 1/2 --- Step 34/100 --- :  33%|      | 33/100 [00:26<00:08,  8.37batch /s, learning_rate=1.47e-5, loss=0.328]INFO:absl:Callbacks in progress at step 34 . . . .\n",
      "Epoch 1/2 --- Step 35/100 --- :  34%|      | 34/100 [00:26<00:07,  8.29batch /s, learning_rate=1.51e-5, loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 34\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 35 . . . .\n",
      "Epoch 1/2 --- Step 36/100 --- :  35%|      | 35/100 [00:26<00:07,  8.25batch /s, learning_rate=1.56e-5, loss=0.319]INFO:absl:Callbacks in progress at step 36 . . . .\n",
      "Epoch 1/2 --- Step 37/100 --- :  36%|      | 36/100 [00:27<00:07,  8.35batch /s, learning_rate=1.6e-5, loss=0.28]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 36\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 37 . . . .\n",
      "Epoch 1/2 --- Step 38/100 --- :  37%|      | 37/100 [00:27<00:07,  8.41batch /s, learning_rate=1.64e-5, loss=0.303]INFO:absl:Callbacks in progress at step 38 . . . .\n",
      "Epoch 1/2 --- Step 39/100 --- :  38%|      | 38/100 [00:27<00:07,  8.48batch /s, learning_rate=1.69e-5, loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 38\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 39 . . . .\n",
      "Epoch 1/2 --- Step 40/100 --- :  39%|      | 39/100 [00:27<00:07,  8.42batch /s, learning_rate=1.73e-5, loss=0.277]INFO:absl:Callbacks in progress at step 40 . . . .\n",
      "Epoch 1/2 --- Step 41/100 --- :  40%|      | 40/100 [00:27<00:07,  8.38batch /s, learning_rate=1.78e-5, loss=0.269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 40\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 41 . . . .\n",
      "Epoch 1/2 --- Step 42/100 --- :  41%|      | 41/100 [00:27<00:07,  8.30batch /s, learning_rate=1.82e-5, loss=0.322]INFO:absl:Callbacks in progress at step 42 . . . .\n",
      "Epoch 1/2 --- Step 43/100 --- :  42%|     | 42/100 [00:27<00:07,  8.04batch /s, learning_rate=1.87e-5, loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 42\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 43 . . . .\n",
      "Epoch 1/2 --- Step 44/100 --- :  43%|     | 43/100 [00:27<00:07,  8.06batch /s, learning_rate=1.91e-5, loss=0.321]INFO:absl:Callbacks in progress at step 44 . . . .\n",
      "Epoch 1/2 --- Step 45/100 --- :  44%|     | 44/100 [00:28<00:06,  8.11batch /s, learning_rate=1.96e-5, loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 44\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 45 . . . .\n",
      "Epoch 1/2 --- Step 46/100 --- :  45%|     | 45/100 [00:28<00:06,  8.11batch /s, learning_rate=0, loss=0.24]       INFO:absl:Callbacks in progress at step 46 . . . .\n",
      "Epoch 1/2 --- Step 47/100 --- :  46%|     | 46/100 [00:28<00:06,  8.15batch /s, learning_rate=0, loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 46\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 47 . . . .\n",
      "Epoch 1/2 --- Step 48/100 --- :  47%|     | 47/100 [00:28<00:06,  7.91batch /s, learning_rate=0, loss=0.256]INFO:absl:Callbacks in progress at step 48 . . . .\n",
      "Epoch 1/2 --- Step 49/100 --- :  48%|     | 48/100 [00:28<00:06,  8.19batch /s, learning_rate=0, loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 48\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 49 . . . .\n",
      "Epoch 1/2 --- Step 50/100 --- :  49%|     | 49/100 [00:28<00:06,  8.26batch /s, learning_rate=0, loss=0.3]  INFO:absl:Callbacks in progress at step 50 . . . .\n",
      "Epoch 1/2 --- Step 51/100 --- :  50%|     | 50/100 [00:28<00:06,  8.19batch /s, learning_rate=0, loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 50\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 51 . . . .\n",
      "Epoch 1/2 --- Step 52/100 --- :  51%|     | 51/100 [00:28<00:05,  8.28batch /s, learning_rate=0, loss=0.297]INFO:absl:Callbacks in progress at step 52 . . . .\n",
      "Epoch 1/2 --- Step 53/100 --- :  52%|    | 52/100 [00:29<00:05,  8.23batch /s, learning_rate=0, loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 52\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 53 . . . .\n",
      "Epoch 1/2 --- Step 54/100 --- :  53%|    | 53/100 [00:29<00:05,  8.21batch /s, learning_rate=0, loss=0.264]INFO:absl:Callbacks in progress at step 54 . . . .\n",
      "Epoch 1/2 --- Step 55/100 --- :  54%|    | 54/100 [00:29<00:05,  8.29batch /s, learning_rate=0, loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 54\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 55 . . . .\n",
      "Epoch 1/2 --- Step 56/100 --- :  55%|    | 55/100 [00:29<00:05,  8.29batch /s, learning_rate=0, loss=0.254]INFO:absl:Callbacks in progress at step 56 . . . .\n",
      "Epoch 1/2 --- Step 57/100 --- :  56%|    | 56/100 [00:29<00:05,  8.29batch /s, learning_rate=0, loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 56\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 57 . . . .\n",
      "Epoch 1/2 --- Step 58/100 --- :  57%|    | 57/100 [00:29<00:05,  8.26batch /s, learning_rate=0, loss=0.322]INFO:absl:Callbacks in progress at step 58 . . . .\n",
      "Epoch 1/2 --- Step 59/100 --- :  58%|    | 58/100 [00:29<00:05,  8.28batch /s, learning_rate=0, loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 58\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 59 . . . .\n",
      "Epoch 1/2 --- Step 60/100 --- :  59%|    | 59/100 [00:29<00:04,  8.24batch /s, learning_rate=0, loss=0.289]INFO:absl:Callbacks in progress at step 60 . . . .\n",
      "Epoch 1/2 --- Step 61/100 --- :  60%|    | 60/100 [00:29<00:04,  8.22batch /s, learning_rate=0, loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 60\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 61 . . . .\n",
      "Epoch 1/2 --- Step 62/100 --- :  61%|    | 61/100 [00:30<00:04,  8.21batch /s, learning_rate=0, loss=0.293]INFO:absl:Callbacks in progress at step 62 . . . .\n",
      "Epoch 1/2 --- Step 63/100 --- :  62%|   | 62/100 [00:30<00:04,  8.19batch /s, learning_rate=0, loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 62\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 63 . . . .\n",
      "Epoch 1/2 --- Step 64/100 --- :  63%|   | 63/100 [00:30<00:04,  7.98batch /s, learning_rate=0, loss=0.261]INFO:absl:Callbacks in progress at step 64 . . . .\n",
      "Epoch 1/2 --- Step 65/100 --- :  64%|   | 64/100 [00:30<00:04,  8.07batch /s, learning_rate=0, loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 64\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 65 . . . .\n",
      "Epoch 1/2 --- Step 66/100 --- :  65%|   | 65/100 [00:30<00:04,  8.12batch /s, learning_rate=0, loss=0.34] INFO:absl:Callbacks in progress at step 66 . . . .\n",
      "Epoch 1/2 --- Step 67/100 --- :  66%|   | 66/100 [00:30<00:04,  8.13batch /s, learning_rate=0, loss=0.302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 66\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 67 . . . .\n",
      "Epoch 1/2 --- Step 68/100 --- :  67%|   | 67/100 [00:30<00:03,  8.27batch /s, learning_rate=0, loss=0.271]INFO:absl:Callbacks in progress at step 68 . . . .\n",
      "Epoch 1/2 --- Step 69/100 --- :  68%|   | 68/100 [00:30<00:03,  8.25batch /s, learning_rate=0, loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 68\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 69 . . . .\n",
      "Epoch 1/2 --- Step 70/100 --- :  69%|   | 69/100 [00:31<00:03,  8.28batch /s, learning_rate=0, loss=0.296]INFO:absl:Callbacks in progress at step 70 . . . .\n",
      "Epoch 1/2 --- Step 71/100 --- :  70%|   | 70/100 [00:31<00:03,  8.36batch /s, learning_rate=0, loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 70\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 71 . . . .\n",
      "Epoch 1/2 --- Step 72/100 --- :  71%|   | 71/100 [00:31<00:03,  8.54batch /s, learning_rate=0, loss=0.318]INFO:absl:Callbacks in progress at step 72 . . . .\n",
      "Epoch 1/2 --- Step 73/100 --- :  72%|  | 72/100 [00:31<00:03,  8.45batch /s, learning_rate=0, loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 72\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 73 . . . .\n",
      "Epoch 1/2 --- Step 74/100 --- :  73%|  | 73/100 [00:31<00:03,  8.35batch /s, learning_rate=0, loss=0.288]INFO:absl:Callbacks in progress at step 74 . . . .\n",
      "Epoch 1/2 --- Step 75/100 --- :  74%|  | 74/100 [00:31<00:03,  8.42batch /s, learning_rate=0, loss=0.352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 74\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 75 . . . .\n",
      "Epoch 1/2 --- Step 76/100 --- :  75%|  | 75/100 [00:31<00:03,  8.33batch /s, learning_rate=0, loss=0.237]INFO:absl:Callbacks in progress at step 76 . . . .\n",
      "Epoch 1/2 --- Step 77/100 --- :  76%|  | 76/100 [00:31<00:02,  8.40batch /s, learning_rate=0, loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 76\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 77 . . . .\n",
      "Epoch 1/2 --- Step 78/100 --- :  77%|  | 77/100 [00:32<00:02,  8.47batch /s, learning_rate=0, loss=0.323]INFO:absl:Callbacks in progress at step 78 . . . .\n",
      "Epoch 1/2 --- Step 79/100 --- :  78%|  | 78/100 [00:32<00:02,  8.32batch /s, learning_rate=0, loss=0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 78\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 79 . . . .\n",
      "Epoch 1/2 --- Step 80/100 --- :  79%|  | 79/100 [00:32<00:02,  8.26batch /s, learning_rate=0, loss=0.262]INFO:absl:Callbacks in progress at step 80 . . . .\n",
      "Epoch 1/2 --- Step 81/100 --- :  80%|  | 80/100 [00:32<00:02,  8.01batch /s, learning_rate=0, loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 80\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 81 . . . .\n",
      "Epoch 1/2 --- Step 82/100 --- :  81%|  | 81/100 [00:32<00:02,  8.05batch /s, learning_rate=0, loss=0.262]INFO:absl:Callbacks in progress at step 82 . . . .\n",
      "Epoch 1/2 --- Step 83/100 --- :  82%| | 82/100 [00:32<00:02,  8.12batch /s, learning_rate=0, loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 82\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 83 . . . .\n",
      "Epoch 1/2 --- Step 84/100 --- :  83%| | 83/100 [00:32<00:02,  8.34batch /s, learning_rate=0, loss=0.316]INFO:absl:Callbacks in progress at step 84 . . . .\n",
      "Epoch 1/2 --- Step 85/100 --- :  84%| | 84/100 [00:32<00:01,  8.41batch /s, learning_rate=0, loss=0.29] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 84\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 85 . . . .\n",
      "Epoch 1/2 --- Step 86/100 --- :  85%| | 85/100 [00:33<00:01,  8.33batch /s, learning_rate=0, loss=0.255]INFO:absl:Callbacks in progress at step 86 . . . .\n",
      "Epoch 1/2 --- Step 87/100 --- :  86%| | 86/100 [00:33<00:01,  8.32batch /s, learning_rate=0, loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 86\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 87 . . . .\n",
      "Epoch 1/2 --- Step 88/100 --- :  87%| | 87/100 [00:33<00:01,  8.27batch /s, learning_rate=0, loss=0.307]INFO:absl:Callbacks in progress at step 88 . . . .\n",
      "Epoch 1/2 --- Step 89/100 --- :  88%| | 88/100 [00:33<00:01,  8.24batch /s, learning_rate=0, loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 88\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 89 . . . .\n",
      "Epoch 1/2 --- Step 90/100 --- :  89%| | 89/100 [00:33<00:01,  8.25batch /s, learning_rate=0, loss=0.25] INFO:absl:Callbacks in progress at step 90 . . . .\n",
      "Epoch 1/2 --- Step 91/100 --- :  90%| | 90/100 [00:33<00:01,  8.24batch /s, learning_rate=0, loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 90\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 91 . . . .\n",
      "Epoch 1/2 --- Step 92/100 --- :  91%| | 91/100 [00:33<00:01,  8.22batch /s, learning_rate=0, loss=0.299]INFO:absl:Callbacks in progress at step 92 . . . .\n",
      "Epoch 1/2 --- Step 93/100 --- :  92%|| 92/100 [00:33<00:00,  8.31batch /s, learning_rate=0, loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 92\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 93 . . . .\n",
      "Epoch 1/2 --- Step 94/100 --- :  93%|| 93/100 [00:33<00:00,  8.31batch /s, learning_rate=0, loss=0.27] INFO:absl:Callbacks in progress at step 94 . . . .\n",
      "Epoch 1/2 --- Step 95/100 --- :  94%|| 94/100 [00:34<00:00,  8.27batch /s, learning_rate=0, loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 94\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 95 . . . .\n",
      "Epoch 1/2 --- Step 96/100 --- :  95%|| 95/100 [00:34<00:00,  8.37batch /s, learning_rate=0, loss=0.308]INFO:absl:Callbacks in progress at step 96 . . . .\n",
      "Epoch 1/2 --- Step 97/100 --- :  96%|| 96/100 [00:34<00:00,  8.30batch /s, learning_rate=0, loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 96\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 97 . . . .\n",
      "Epoch 1/2 --- Step 98/100 --- :  97%|| 97/100 [00:34<00:00,  8.32batch /s, learning_rate=0, loss=0.308]INFO:absl:Callbacks in progress at step 98 . . . .\n",
      "Epoch 1/2 --- Step 99/100 --- :  98%|| 98/100 [00:34<00:00,  8.26batch /s, learning_rate=0, loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 98\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 99 . . . .\n",
      "Epoch 1/2 --- Step 100/100 --- :  99%|| 99/100 [00:34<00:00,  8.22batch /s, learning_rate=0, loss=0.252]INFO:absl:Callbacks in progress at step 100 . . . .\n",
      "Epoch 1/2 --- Step 100/100 --- : 100%|| 100/100 [00:34<00:00,  2.87batch /s, learning_rate=0, loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 1 and step 100\n",
      "Train done\n",
      "Val done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:absl:Model saved at epoch 1\n",
      "Epoch 2/2 --- Step 1/100 --- :   0%|          | 0/100 [00:00<?, ?batch /s]INFO:absl:Callbacks in progress at step 101 . . . .\n",
      "Epoch 2/2 --- Step 2/100 --- :   1%|          | 1/100 [00:00<00:12,  8.15batch /s, learning_rate=0, loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started epoch 2 and step 101\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 102 . . . .\n",
      "Epoch 2/2 --- Step 3/100 --- :   2%|         | 2/100 [00:00<00:12,  7.92batch /s, learning_rate=0, loss=0.277]INFO:absl:Callbacks in progress at step 103 . . . .\n",
      "Epoch 2/2 --- Step 4/100 --- :   3%|         | 3/100 [00:00<00:12,  8.00batch /s, learning_rate=0, loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 103\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 104 . . . .\n",
      "Epoch 2/2 --- Step 5/100 --- :   4%|         | 4/100 [00:00<00:11,  8.26batch /s, learning_rate=0, loss=0.292]INFO:absl:Callbacks in progress at step 105 . . . .\n",
      "Epoch 2/2 --- Step 6/100 --- :   5%|         | 5/100 [00:00<00:11,  8.26batch /s, learning_rate=0, loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 105\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 106 . . . .\n",
      "Epoch 2/2 --- Step 7/100 --- :   6%|         | 6/100 [00:00<00:11,  8.20batch /s, learning_rate=0, loss=0.266]INFO:absl:Callbacks in progress at step 107 . . . .\n",
      "Epoch 2/2 --- Step 8/100 --- :   7%|         | 7/100 [00:00<00:11,  8.21batch /s, learning_rate=0, loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 107\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 108 . . . .\n",
      "Epoch 2/2 --- Step 9/100 --- :   8%|         | 8/100 [00:00<00:11,  8.20batch /s, learning_rate=0, loss=0.306]INFO:absl:Callbacks in progress at step 109 . . . .\n",
      "Epoch 2/2 --- Step 10/100 --- :   9%|         | 9/100 [00:01<00:11,  8.22batch /s, learning_rate=0, loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 109\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 110 . . . .\n",
      "Epoch 2/2 --- Step 11/100 --- :  10%|         | 10/100 [00:01<00:10,  8.20batch /s, learning_rate=0, loss=0.293]INFO:absl:Callbacks in progress at step 111 . . . .\n",
      "Epoch 2/2 --- Step 12/100 --- :  11%|         | 11/100 [00:01<00:10,  8.15batch /s, learning_rate=0, loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 111\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 112 . . . .\n",
      "Epoch 2/2 --- Step 13/100 --- :  12%|        | 12/100 [00:01<00:10,  8.24batch /s, learning_rate=0, loss=0.32] INFO:absl:Callbacks in progress at step 113 . . . .\n",
      "Epoch 2/2 --- Step 14/100 --- :  13%|        | 13/100 [00:01<00:10,  8.34batch /s, learning_rate=0, loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 113\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 114 . . . .\n",
      "Epoch 2/2 --- Step 15/100 --- :  14%|        | 14/100 [00:01<00:10,  8.52batch /s, learning_rate=0, loss=0.374]INFO:absl:Callbacks in progress at step 115 . . . .\n",
      "Epoch 2/2 --- Step 16/100 --- :  15%|        | 15/100 [00:01<00:10,  8.46batch /s, learning_rate=0, loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 115\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 116 . . . .\n",
      "Epoch 2/2 --- Step 17/100 --- :  16%|        | 16/100 [00:01<00:09,  8.49batch /s, learning_rate=0, loss=0.268]INFO:absl:Callbacks in progress at step 117 . . . .\n",
      "Epoch 2/2 --- Step 18/100 --- :  17%|        | 17/100 [00:02<00:09,  8.37batch /s, learning_rate=0, loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 117\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 118 . . . .\n",
      "Epoch 2/2 --- Step 19/100 --- :  18%|        | 18/100 [00:02<00:09,  8.35batch /s, learning_rate=0, loss=0.244]INFO:absl:Callbacks in progress at step 119 . . . .\n",
      "Epoch 2/2 --- Step 20/100 --- :  19%|        | 19/100 [00:02<00:10,  8.09batch /s, learning_rate=0, loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 119\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 120 . . . .\n",
      "Epoch 2/2 --- Step 21/100 --- :  20%|        | 20/100 [00:02<00:09,  8.14batch /s, learning_rate=0, loss=0.276]INFO:absl:Callbacks in progress at step 121 . . . .\n",
      "Epoch 2/2 --- Step 22/100 --- :  21%|        | 21/100 [00:02<00:09,  7.93batch /s, learning_rate=0, loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 121\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 122 . . . .\n",
      "Epoch 2/2 --- Step 23/100 --- :  22%|       | 22/100 [00:02<00:09,  7.93batch /s, learning_rate=0, loss=0.26] INFO:absl:Callbacks in progress at step 123 . . . .\n",
      "Epoch 2/2 --- Step 24/100 --- :  23%|       | 23/100 [00:02<00:09,  7.99batch /s, learning_rate=0, loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 123\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 124 . . . .\n",
      "Epoch 2/2 --- Step 25/100 --- :  24%|       | 24/100 [00:02<00:09,  7.80batch /s, learning_rate=0, loss=0.286]INFO:absl:Callbacks in progress at step 125 . . . .\n",
      "Epoch 2/2 --- Step 26/100 --- :  25%|       | 25/100 [00:03<00:09,  8.00batch /s, learning_rate=0, loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 125\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 126 . . . .\n",
      "Epoch 2/2 --- Step 27/100 --- :  26%|       | 26/100 [00:03<00:08,  8.26batch /s, learning_rate=0, loss=0.36] INFO:absl:Callbacks in progress at step 127 . . . .\n",
      "Epoch 2/2 --- Step 28/100 --- :  27%|       | 27/100 [00:03<00:08,  8.24batch /s, learning_rate=0, loss=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 127\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 128 . . . .\n",
      "Epoch 2/2 --- Step 29/100 --- :  28%|       | 28/100 [00:03<00:08,  8.19batch /s, learning_rate=0, loss=0.253]INFO:absl:Callbacks in progress at step 129 . . . .\n",
      "Epoch 2/2 --- Step 30/100 --- :  29%|       | 29/100 [00:03<00:08,  8.14batch /s, learning_rate=0, loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 129\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 130 . . . .\n",
      "Epoch 2/2 --- Step 31/100 --- :  30%|       | 30/100 [00:03<00:08,  8.16batch /s, learning_rate=0, loss=0.302]INFO:absl:Callbacks in progress at step 131 . . . .\n",
      "Epoch 2/2 --- Step 32/100 --- :  31%|       | 31/100 [00:03<00:08,  8.18batch /s, learning_rate=0, loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 131\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 132 . . . .\n",
      "Epoch 2/2 --- Step 33/100 --- :  32%|      | 32/100 [00:03<00:08,  8.21batch /s, learning_rate=0, loss=0.232]INFO:absl:Callbacks in progress at step 133 . . . .\n",
      "Epoch 2/2 --- Step 34/100 --- :  33%|      | 33/100 [00:04<00:08,  8.27batch /s, learning_rate=0, loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 133\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 134 . . . .\n",
      "Epoch 2/2 --- Step 35/100 --- :  34%|      | 34/100 [00:04<00:07,  8.36batch /s, learning_rate=0, loss=0.312]INFO:absl:Callbacks in progress at step 135 . . . .\n",
      "Epoch 2/2 --- Step 36/100 --- :  35%|      | 35/100 [00:04<00:07,  8.34batch /s, learning_rate=0, loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 135\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 136 . . . .\n",
      "Epoch 2/2 --- Step 37/100 --- :  36%|      | 36/100 [00:04<00:07,  8.28batch /s, learning_rate=0, loss=0.347]INFO:absl:Callbacks in progress at step 137 . . . .\n",
      "Epoch 2/2 --- Step 38/100 --- :  37%|      | 37/100 [00:04<00:07,  8.35batch /s, learning_rate=0, loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 137\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 138 . . . .\n",
      "Epoch 2/2 --- Step 39/100 --- :  38%|      | 38/100 [00:04<00:07,  8.34batch /s, learning_rate=0, loss=0.269]INFO:absl:Callbacks in progress at step 139 . . . .\n",
      "Epoch 2/2 --- Step 40/100 --- :  39%|      | 39/100 [00:04<00:07,  8.27batch /s, learning_rate=0, loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 139\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 140 . . . .\n",
      "Epoch 2/2 --- Step 41/100 --- :  40%|      | 40/100 [00:04<00:07,  8.22batch /s, learning_rate=0, loss=0.253]INFO:absl:Callbacks in progress at step 141 . . . .\n",
      "Epoch 2/2 --- Step 42/100 --- :  41%|      | 41/100 [00:04<00:07,  8.42batch /s, learning_rate=0, loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 141\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 142 . . . .\n",
      "Epoch 2/2 --- Step 43/100 --- :  42%|     | 42/100 [00:05<00:06,  8.37batch /s, learning_rate=0, loss=0.317]INFO:absl:Callbacks in progress at step 143 . . . .\n",
      "Epoch 2/2 --- Step 44/100 --- :  43%|     | 43/100 [00:05<00:07,  8.08batch /s, learning_rate=0, loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 143\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 144 . . . .\n",
      "Epoch 2/2 --- Step 45/100 --- :  44%|     | 44/100 [00:05<00:06,  8.23batch /s, learning_rate=0, loss=0.307]INFO:absl:Callbacks in progress at step 145 . . . .\n",
      "Epoch 2/2 --- Step 46/100 --- :  45%|     | 45/100 [00:05<00:06,  8.24batch /s, learning_rate=0, loss=0.277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 145\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 146 . . . .\n",
      "Epoch 2/2 --- Step 47/100 --- :  46%|     | 46/100 [00:05<00:06,  8.23batch /s, learning_rate=0, loss=0.339]INFO:absl:Callbacks in progress at step 147 . . . .\n",
      "Epoch 2/2 --- Step 48/100 --- :  47%|     | 47/100 [00:05<00:06,  8.26batch /s, learning_rate=0, loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 147\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 148 . . . .\n",
      "Epoch 2/2 --- Step 49/100 --- :  48%|     | 48/100 [00:05<00:06,  8.22batch /s, learning_rate=0, loss=0.285]INFO:absl:Callbacks in progress at step 149 . . . .\n",
      "Epoch 2/2 --- Step 50/100 --- :  49%|     | 49/100 [00:05<00:06,  8.25batch /s, learning_rate=0, loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 149\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 150 . . . .\n",
      "Epoch 2/2 --- Step 51/100 --- :  50%|     | 50/100 [00:06<00:06,  8.24batch /s, learning_rate=0, loss=0.297]INFO:absl:Callbacks in progress at step 151 . . . .\n",
      "Epoch 2/2 --- Step 52/100 --- :  51%|     | 51/100 [00:06<00:06,  8.02batch /s, learning_rate=0, loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 151\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 152 . . . .\n",
      "Epoch 2/2 --- Step 53/100 --- :  52%|    | 52/100 [00:06<00:05,  8.04batch /s, learning_rate=0, loss=0.284]INFO:absl:Callbacks in progress at step 153 . . . .\n",
      "Epoch 2/2 --- Step 54/100 --- :  53%|    | 53/100 [00:06<00:05,  8.03batch /s, learning_rate=0, loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 153\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 154 . . . .\n",
      "Epoch 2/2 --- Step 55/100 --- :  54%|    | 54/100 [00:06<00:05,  8.10batch /s, learning_rate=0, loss=0.293]INFO:absl:Callbacks in progress at step 155 . . . .\n",
      "Epoch 2/2 --- Step 56/100 --- :  55%|    | 55/100 [00:06<00:05,  8.14batch /s, learning_rate=0, loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 155\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 156 . . . .\n",
      "Epoch 2/2 --- Step 57/100 --- :  56%|    | 56/100 [00:06<00:05,  8.38batch /s, learning_rate=0, loss=0.369]INFO:absl:Callbacks in progress at step 157 . . . .\n",
      "Epoch 2/2 --- Step 58/100 --- :  57%|    | 57/100 [00:06<00:05,  8.32batch /s, learning_rate=0, loss=0.29] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 157\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 158 . . . .\n",
      "Epoch 2/2 --- Step 59/100 --- :  58%|    | 58/100 [00:07<00:05,  8.20batch /s, learning_rate=0, loss=0.322]INFO:absl:Callbacks in progress at step 159 . . . .\n",
      "Epoch 2/2 --- Step 60/100 --- :  59%|    | 59/100 [00:07<00:04,  8.29batch /s, learning_rate=0, loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 159\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 160 . . . .\n",
      "Epoch 2/2 --- Step 61/100 --- :  60%|    | 60/100 [00:07<00:04,  8.36batch /s, learning_rate=0, loss=0.297]INFO:absl:Callbacks in progress at step 161 . . . .\n",
      "Epoch 2/2 --- Step 62/100 --- :  61%|    | 61/100 [00:07<00:04,  8.34batch /s, learning_rate=0, loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 161\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 162 . . . .\n",
      "Epoch 2/2 --- Step 63/100 --- :  62%|   | 62/100 [00:07<00:04,  8.34batch /s, learning_rate=0, loss=0.258]INFO:absl:Callbacks in progress at step 163 . . . .\n",
      "Epoch 2/2 --- Step 64/100 --- :  63%|   | 63/100 [00:07<00:04,  8.40batch /s, learning_rate=0, loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 163\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 164 . . . .\n",
      "Epoch 2/2 --- Step 65/100 --- :  64%|   | 64/100 [00:07<00:04,  8.37batch /s, learning_rate=0, loss=0.243]INFO:absl:Callbacks in progress at step 165 . . . .\n",
      "Epoch 2/2 --- Step 66/100 --- :  65%|   | 65/100 [00:07<00:04,  8.30batch /s, learning_rate=0, loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 165\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 166 . . . .\n",
      "Epoch 2/2 --- Step 67/100 --- :  66%|   | 66/100 [00:08<00:04,  8.26batch /s, learning_rate=0, loss=0.344]INFO:absl:Callbacks in progress at step 167 . . . .\n",
      "Epoch 2/2 --- Step 68/100 --- :  67%|   | 67/100 [00:08<00:03,  8.33batch /s, learning_rate=0, loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 167\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 168 . . . .\n",
      "Epoch 2/2 --- Step 69/100 --- :  68%|   | 68/100 [00:08<00:03,  8.03batch /s, learning_rate=0, loss=0.249]INFO:absl:Callbacks in progress at step 169 . . . .\n",
      "Epoch 2/2 --- Step 70/100 --- :  69%|   | 69/100 [00:08<00:03,  8.10batch /s, learning_rate=0, loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 169\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 170 . . . .\n",
      "Epoch 2/2 --- Step 71/100 --- :  70%|   | 70/100 [00:08<00:03,  8.33batch /s, learning_rate=0, loss=0.33] INFO:absl:Callbacks in progress at step 171 . . . .\n",
      "Epoch 2/2 --- Step 72/100 --- :  71%|   | 71/100 [00:08<00:03,  8.27batch /s, learning_rate=0, loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 171\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 172 . . . .\n",
      "Epoch 2/2 --- Step 73/100 --- :  72%|  | 72/100 [00:08<00:03,  8.35batch /s, learning_rate=0, loss=0.294]INFO:absl:Callbacks in progress at step 173 . . . .\n",
      "Epoch 2/2 --- Step 74/100 --- :  73%|  | 73/100 [00:08<00:03,  8.32batch /s, learning_rate=0, loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 173\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 174 . . . .\n",
      "Epoch 2/2 --- Step 75/100 --- :  74%|  | 74/100 [00:08<00:03,  8.25batch /s, learning_rate=0, loss=0.257]INFO:absl:Callbacks in progress at step 175 . . . .\n",
      "Epoch 2/2 --- Step 76/100 --- :  75%|  | 75/100 [00:09<00:03,  8.21batch /s, learning_rate=0, loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 175\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 176 . . . .\n",
      "Epoch 2/2 --- Step 77/100 --- :  76%|  | 76/100 [00:09<00:02,  8.19batch /s, learning_rate=0, loss=0.294]INFO:absl:Callbacks in progress at step 177 . . . .\n",
      "Epoch 2/2 --- Step 78/100 --- :  77%|  | 77/100 [00:09<00:02,  8.23batch /s, learning_rate=0, loss=0.298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 177\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 178 . . . .\n",
      "Epoch 2/2 --- Step 79/100 --- :  78%|  | 78/100 [00:09<00:02,  8.21batch /s, learning_rate=0, loss=0.264]INFO:absl:Callbacks in progress at step 179 . . . .\n",
      "Epoch 2/2 --- Step 80/100 --- :  79%|  | 79/100 [00:09<00:02,  8.29batch /s, learning_rate=0, loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 179\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 180 . . . .\n",
      "Epoch 2/2 --- Step 81/100 --- :  80%|  | 80/100 [00:09<00:02,  8.31batch /s, learning_rate=0, loss=0.246]INFO:absl:Callbacks in progress at step 181 . . . .\n",
      "Epoch 2/2 --- Step 82/100 --- :  81%|  | 81/100 [00:09<00:02,  8.32batch /s, learning_rate=0, loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 181\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 182 . . . .\n",
      "Epoch 2/2 --- Step 83/100 --- :  82%| | 82/100 [00:09<00:02,  8.32batch /s, learning_rate=0, loss=0.275]INFO:absl:Callbacks in progress at step 183 . . . .\n",
      "Epoch 2/2 --- Step 84/100 --- :  83%| | 83/100 [00:10<00:02,  8.29batch /s, learning_rate=0, loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 183\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 184 . . . .\n",
      "Epoch 2/2 --- Step 85/100 --- :  84%| | 84/100 [00:10<00:01,  8.28batch /s, learning_rate=0, loss=0.318]INFO:absl:Callbacks in progress at step 185 . . . .\n",
      "Epoch 2/2 --- Step 86/100 --- :  85%| | 85/100 [00:10<00:01,  8.23batch /s, learning_rate=0, loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 185\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 186 . . . .\n",
      "Epoch 2/2 --- Step 87/100 --- :  86%| | 86/100 [00:10<00:01,  7.99batch /s, learning_rate=0, loss=0.282]INFO:absl:Callbacks in progress at step 187 . . . .\n",
      "Epoch 2/2 --- Step 88/100 --- :  87%| | 87/100 [00:10<00:01,  8.04batch /s, learning_rate=0, loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 187\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 188 . . . .\n",
      "Epoch 2/2 --- Step 89/100 --- :  88%| | 88/100 [00:10<00:01,  8.08batch /s, learning_rate=0, loss=0.35] INFO:absl:Callbacks in progress at step 189 . . . .\n",
      "Epoch 2/2 --- Step 90/100 --- :  89%| | 89/100 [00:10<00:01,  8.23batch /s, learning_rate=0, loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 189\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 190 . . . .\n",
      "Epoch 2/2 --- Step 91/100 --- :  90%| | 90/100 [00:10<00:01,  8.25batch /s, learning_rate=0, loss=0.286]INFO:absl:Callbacks in progress at step 191 . . . .\n",
      "Epoch 2/2 --- Step 92/100 --- :  91%| | 91/100 [00:11<00:01,  8.26batch /s, learning_rate=0, loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 191\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 192 . . . .\n",
      "Epoch 2/2 --- Step 93/100 --- :  92%|| 92/100 [00:11<00:00,  8.36batch /s, learning_rate=0, loss=0.283]INFO:absl:Callbacks in progress at step 193 . . . .\n",
      "Epoch 2/2 --- Step 94/100 --- :  93%|| 93/100 [00:11<00:00,  8.53batch /s, learning_rate=0, loss=0.349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 193\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 194 . . . .\n",
      "Epoch 2/2 --- Step 95/100 --- :  94%|| 94/100 [00:11<00:00,  8.56batch /s, learning_rate=0, loss=0.314]INFO:absl:Callbacks in progress at step 195 . . . .\n",
      "Epoch 2/2 --- Step 96/100 --- :  95%|| 95/100 [00:11<00:00,  8.42batch /s, learning_rate=0, loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 195\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 196 . . . .\n",
      "Epoch 2/2 --- Step 97/100 --- :  96%|| 96/100 [00:11<00:00,  8.34batch /s, learning_rate=0, loss=0.303]INFO:absl:Callbacks in progress at step 197 . . . .\n",
      "Epoch 2/2 --- Step 98/100 --- :  97%|| 97/100 [00:11<00:00,  8.29batch /s, learning_rate=0, loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 197\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 198 . . . .\n",
      "Epoch 2/2 --- Step 99/100 --- :  98%|| 98/100 [00:11<00:00,  8.01batch /s, learning_rate=0, loss=0.286]INFO:absl:Callbacks in progress at step 199 . . . .\n",
      "Epoch 2/2 --- Step 100/100 --- :  99%|| 99/100 [00:12<00:00,  8.01batch /s, learning_rate=0, loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 199\n",
      "Train done\n",
      "Val done\n",
      "Started epoch 2 and step 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Callbacks in progress at step 200 . . . .\n",
      "Epoch 2/2 --- Step 100/100 --- : 100%|| 100/100 [00:12<00:00,  8.22batch /s, learning_rate=0, loss=0.278]\n",
      "INFO:absl:Model saved at epoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train done\n",
      "Val done\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint_dir = \"/tmp/model_ckpt/\"\n",
    "history = trainer.run(\n",
    "    model_fn = model_fn,\n",
    "    optimizer_fn = optimizer_fn,\n",
    "    train_dataset = train_dataset,\n",
    "    train_loss_fn = train_loss_fn,\n",
    "    epochs = 2,\n",
    "    steps_per_epoch = 100,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=train_batch_size,\n",
    "    training_loss_names=None,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=eval_dataset,\n",
    "    validation_loss_fn=train_loss_fn,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=1,\n",
    "    enable_xla=False,\n",
    "    callbacks=[callback],\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=True,\n",
    "    latest_checkpoint=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87310e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80833028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4327d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0d3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284b746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e4d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fe450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487978f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf89a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type albert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/albert-base-v2/ckpt-1\n",
      "INFO:absl:Using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "        # Under Strategy Scope\n",
    "        with trainer.distribution_strategy.scope():\n",
    "            # Model\n",
    "            model = model_fn()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = optimizer_fn()\n",
    "\n",
    "            optimizer = configure_optimizer(optimizer, use_float16=False, loss_scale=\"dynamic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db3b7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_loss(batch_labels, model_outputs):\n",
    "        \"\"\"Loss computation which takes care of loss reduction based on GLOBAL_BATCH_SIZE\"\"\"\n",
    "        per_example_loss = train_loss_fn(batch_labels, model_outputs)\n",
    "        per_example_loss_averaged = {}\n",
    "        # Inplace update\n",
    "        # Avergae loss per global batch size , recommended\n",
    "        for name, loss in per_example_loss.items():\n",
    "            per_example_loss_averaged[name] = tf.nn.compute_average_loss(loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "        return per_example_loss_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62a30704",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train Functions\n",
    "    @tf.function\n",
    "    def do_train(iterator):\n",
    "        \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "        def train_step(dist_inputs):\n",
    "            \"\"\"The computation to run on each device.\"\"\"\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                model_outputs = model(batch_inputs)\n",
    "                loss = compute_loss(batch_labels, model_outputs)\n",
    "                tf.debugging.check_numerics(loss['loss'], message='Loss value is either NaN or inf')\n",
    "                if isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                    loss_scaled = {name: optimizer.get_scaled_loss(loss_value) for name, loss_value in loss.items()}\n",
    "                # TODO\n",
    "                # Scales down the loss for gradients to be invariant from replicas.\n",
    "                # loss = loss / strategy.num_replicas_in_sync\n",
    "            if mixed_precision:\n",
    "                scaled_gradients = tape.gradient(loss_scaled[\"loss\"], model.trainable_variables)\n",
    "                grads = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "            else:\n",
    "                grads = tape.gradient(loss[\"loss\"], model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "            return loss\n",
    "\n",
    "        for _ in tf.range(tf.convert_to_tensor(steps_per_call)):\n",
    "            dist_inputs = next(iterator)\n",
    "            loss = strategy.run(train_step, args=(dist_inputs,))\n",
    "            # strategy reduce\n",
    "            loss = {\n",
    "                name: strategy.reduce(tf.distribute.ReduceOp.MEAN, loss_value, axis=None)\n",
    "                for name, loss_value in loss.items()\n",
    "            }\n",
    "            \n",
    "            t_loss.update_state(loss['loss'])\n",
    "            # training_result = get_and_reset_metric_from_dict(training_loss_dict_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e30b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_distributed = trainer.distribution_strategy.experimental_distribute_dataset(train_dataset)\n",
    "train_dataset_distributed = iter(train_dataset_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c32a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_call = 1\n",
    "GLOBAL_BATCH_SIZE = 32\n",
    "mixed_precision = False\n",
    "strategy = trainer.distribution_strategy\n",
    "t_loss = tf.keras.metrics.Mean(\"loss\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c94bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "l = do_train(train_dataset_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68aa53bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.36904955>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_loss.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65c831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56c483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b91e0e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0300fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:XLA enabled\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fbe2c4b1280>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "You are using a model of type albert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/albert-base-v2/ckpt-1\n",
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No checkpoint found in /tmp/model_ckpt/\n",
      "Epoch 1/2 --- Step 50/100 --- :   0%|          | 0/2 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEPS 2\n",
      "Started epoch 1 and step 50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/mlm/transform/dense/kernel:0', 'tf_transformers/albert/mlm/transform/dense/bias:0', 'tf_transformers/albert/mlm/transform/LayerNorm/gamma:0', 'tf_transformers/albert/mlm/transform/LayerNorm/beta:0', 'tf_transformers/albert/mlm/transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9124fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(tfrecord_dir)\n",
    "shutil.rmtree(model_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bb9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
