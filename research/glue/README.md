

```
python run_glue.py +glue=mrpc glue.data.max_seq_length=128
```

```
python run_glue.py optimizer.loss_type=joint
```

```
python run_glue.py optimizer.loss_type=joint data.take_sample=true
```

Output folder looks like this ```outputs/2021-10-17/13-54-47/```




GLUE SCORE calculated
------------------------
|    |     cola |     mnli |     mrpc |   qnli |      qqp |   rte |   sst2 |     stsb |   glue_score |
|---:|---------:|---------:|---------:|-------:|---------:|------:|-------:|---------:|-------------:|
|  0 | 0.273154 | 0.411209 | 0.774384 |   0.82 | 0.727667 | 0.642 |   0.83 | 0.863203 |     0.667702 |
[2021-10-18 08:01:13,437][absl][INFO] - GLUE evaluation is Succesful
[2021-10-18 08:01:13,438][absl][INFO] - Models and evaluation results saved in /home/jovyan/Projects/tf-transformers/research/glue/outputs/2021-10-18/07-53-14


GLUE SCORE calculated
------------------------
|          |      cola |     mnli |     mrpc |     qnli |      qqp |      rte |     sst2 |      stsb |   glue_score |
|:---------|----------:|---------:|---------:|---------:|---------:|---------:|---------:|----------:|-------------:|
| layer_1  | 0         | 0.581514 | 0.748025 | 0.612484 | 0.730661 | 0.552347 | 0.807339 | 0.0593384 |     0.511464 |
| layer_2  | 0.0181483 | 0.737464 | 0.777894 | 0.822259 | 0.834173 | 0.570397 | 0.869266 | 0.8295    |     0.682388 |
| layer_3  | 0.253889  | 0.78251  | 0.809552 | 0.859418 | 0.863381 | 0.588448 | 0.881881 | 0.862159  |     0.737655 |
| layer_4  | 0.378279  | 0.810607 | 0.845078 | 0.883397 | 0.874679 | 0.631769 | 0.905963 | 0.877547  |     0.775915 |
| layer_5  | 0.478266  | 0.82725  | 0.867434 | 0.896394 | 0.882526 | 0.642599 | 0.916284 | 0.889308  |     0.800008 |
| layer_6  | 0.518539  | 0.835905 | 0.879525 | 0.909024 | 0.886847 | 0.67509  | 0.918578 | 0.894333  |     0.81473  |
| layer_7  | 0.561713  | 0.842418 | 0.890978 | 0.913051 | 0.888815 | 0.700361 | 0.918578 | 0.898398  |     0.826789 |
| layer_8  | 0.573798  | 0.845268 | 0.892842 | 0.915431 | 0.889218 | 0.689531 | 0.917431 | 0.898625  |     0.827768 |
| layer_9  | 0.571642  | 0.846082 | 0.897044 | 0.914882 | 0.889671 | 0.700361 | 0.918578 | 0.901202  |     0.829933 |
| layer_10 | 0.566903  | 0.847608 | 0.895647 | 0.915614 | 0.889495 | 0.714801 | 0.919725 | 0.902506  |     0.831537 |
| layer_11 | 0.574227  | 0.848117 | 0.890978 | 0.916712 | 0.889708 | 0.725632 | 0.919725 | 0.901971  |     0.833384 |
| layer_12 | 0.573001  | 0.848117 | 0.887111 | 0.91598  | 0.889409 | 0.722022 | 0.920872 | 0.899414  |     0.831991 |
