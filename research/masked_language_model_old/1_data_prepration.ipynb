{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sidhu/Projects/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "440a61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "from tf_transformers.text.sentencepiece_layer import extend_sentencepicemodel\n",
    "from tf_transformers.text import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90ffb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10be28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae8d5955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init inputs ()\n",
      "init kwargs {'model_max_length': 512, 'vocab_file': '/home/sidhu/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d', 'special_tokens_map_file': None, 'tokenizer_file': '/home/sidhu/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74', 'name_or_path': 'albert-base-v2'}\n"
     ]
    }
   ],
   "source": [
    "# We will use T5 tokenizer , but we extend it with 2 more tokens\n",
    "\n",
    "# 1. [CLS]\n",
    "# 2. [MASK]\n",
    "# We will use </s> as SEP token\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "model_name = 'albert-base-v2'\n",
    "tokenizer_hf = AlbertTokenizer.from_pretrained(model_name)\n",
    "save_path = \"{}/{}\".format(temp_dir, model_name)\n",
    "tokenizer_hf.save_pretrained(save_path)\n",
    "\n",
    "in_file  = '{}/spiece.model'.format(save_path)\n",
    "# out_file = '{}/new_spiece.model'.format(save_path)\n",
    "# special_tokens = ['[CLS]', '[MASK]']\n",
    "\n",
    "# extend_sentencepicemodel(in_file, out_file, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1057408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f725932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use new sentencepiece model in T5 use like this\n",
    "t5_kwargs = {'bos_token': '[CLS]',\n",
    " 'eos_token': '</s>', \n",
    " 'unk_token': '<unk>', \n",
    " 'pad_token': '<pad>', \n",
    " 'mask_token': '[MASK]', \n",
    " 'name_or_path': '{}'.format(save_path), \n",
    " 'vocab_file': '{}/new_spiece.model'.format(save_path)}\n",
    "tokenizer_hf = T5Tokenizer(**t5_kwargs)\n",
    "tokenizer_hf.unique_no_split_tokens = tokenizer_hf.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b67c6e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab is tf.Tensor(30000, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use this one for fast processinf\n",
    "tokenizer_tf = SentencepieceTokenizer(\n",
    "        model_file_path=in_file,\n",
    "        lower_case=True,\n",
    "        special_tokens=['[CLS]', '[MASK]', '<unk>', '<pad>']\n",
    "    )\n",
    "print(\"total vocab is\", tokenizer_tf.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374cd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f6d4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is [MASK] token with [CLS] </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d025d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c82e6a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁this',\n",
       " '▁is',\n",
       " '[MASK]',\n",
       " '▁to',\n",
       " 'ken',\n",
       " '▁with',\n",
       " '[CLS]',\n",
       " '▁',\n",
       " '<',\n",
       " '/',\n",
       " 's',\n",
       " '>']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527212da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebd0449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sp = tokenizer_tf._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3440733d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([1565,  403,    4,    1, 1410,    2,    1], dtype=int32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this is [MASK] token with [CLS] <unk>'\n",
    "tokenizer_sp.string_to_id(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f6814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36b7edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.string_to_id(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4cea8899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16,), dtype=int32, numpy=\n",
       "array([   48,    25,   636, 23265,   500,    20,  2853,    29,   636,\n",
       "        5316,    18,   500,    13,     1, 17081,     1], dtype=int32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.split(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35456346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'this is [mask] token with [cls]  \\xe2\\x81\\x87 unk \\xe2\\x81\\x87 '>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.detokenize([   48,    25,   636, 23265,   500,    20,  2853,    29,   636,\n",
    "        5316,    18,   500,    13,     1, 17081,     1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d738f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thisis[MASK] ⁇ with[CLS]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.detokenize([ 3713,   159, 32001,     2,  4065, 32000]).numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7ba8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([  100,    19, 32001, 14145,    28, 32000,     1], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.string_to_id(['▁This', '▁is', '[MASK]', '▁token', '▁with', '[CLS]', '</s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e57ea431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 19, 32001, 14145, 28, 32000, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fd270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa561dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is[MASK] token with[CLS] </s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.decode([100, 19, 32001, 14145, 28, 32000, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5961ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'This is[MASK] token with[CLS]'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.detokenize([100, 19, 32001, 14145, 28, 32000, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b1fa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thisis[MASK] <unk> with[CLS]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_hf.decode([ 3713,   159, 32001,     2,  4065, 32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d82411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([100,  19], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.tokenize(\"This is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cd3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40d248bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added [CLS]...\n",
      "added [MASK]...\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.text import sentencepiece_model_pb2\n",
    "\n",
    "\n",
    "mp = sentencepiece_model_pb2.ModelProto()\n",
    "mp.ParseFromString(open(in_file, \"rb\").read())\n",
    "\n",
    "for token in special_tokens:\n",
    "    new_token = sentencepiece_model_pb2.ModelProto().SentencePiece()\n",
    "    new_token.piece = token\n",
    "    new_token.score = 0\n",
    "    new_token.type = 3\n",
    "    mp.pieces.append(new_token)\n",
    "    print(f'added {token}...')\n",
    "\n",
    "with open(out_file, 'wb') as f:\n",
    "    f.write(mp.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc726b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05a73976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01a80916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model = sp.SentencePieceProcessor()\n",
    "import tensorflow as tf\n",
    "model_proto = tf.io.gfile.GFile(in_file, \"rb\").read()\n",
    "sp_model.LoadFromSerializedProto(model_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f050234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁this',\n",
       " '▁is',\n",
       " '▁[',\n",
       " 'MASK',\n",
       " ']',\n",
       " '▁to',\n",
       " 'ken',\n",
       " '▁with',\n",
       " '▁[',\n",
       " 'CLS',\n",
       " ']',\n",
       " '▁',\n",
       " '<',\n",
       " 'unk',\n",
       " '>']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.encode_as_pieces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3848a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is [MASK] token with [CLS] </s>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8f9ada5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=int32, numpy=\n",
       "array([  100,    19,     3, 32001, 14145,    28,     3, 32000,     3,\n",
       "           2,    87,     7,  3155], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60060d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56723288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'This is [MASK] token with [CLS]  \\xe2\\x81\\x87 /s>'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_sp.detokenize([  100,    19,     3, 32001, 14145,    28,     3, 32000,     3,\n",
    "           2,    87,     7,  3155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee23b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babe039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proto = tf.io.gfile.GFile(out_file, \"rb\").read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9b0c5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d96a8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '''Wikipedia (/ˌwɪkɪˈpiːdiə/ (About this soundlisten) wik-ih-PEE-dee-ə or /ˌwɪki-/ (About this soundlisten) wik-ee-) is a free content, multilingual online encyclopedia written and maintained by a community of volunteer contributors through a model of open collaboration, using a wiki-based editing system. Wikipedia is the largest and most-read reference work in history,[3] and is consistently one of the 15 most popular websites as ranked by Alexa; as of 2021, it was ranked as the 13th most popular site.[3][4] The project carries no advertisements and is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through small donations.[5]\n",
    "\n",
    "Wikipedia was launched on January 15, 2001, by Jimmy Wales[6] and Larry Sanger; Sanger coined its name as a blending of \"wiki\" and \"encyclopedia\".[7][8] Initially available only in English, versions in other languages were quickly developed. Combined, Wikipedia's editions comprise more than 56 million articles, that attract an average of around 2 billion unique device visits and receive more than 17 million edits per month, or develop at a rate of 1.9 edits per second.[9][10]\n",
    "\n",
    "Wikipedia has received praise for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced amount of commercial bias, but criticism for exhibiting systemic bias, particularly gender bias against women.[11] Its reliability was frequently criticized in the 2000s, but has improved over time and has been generally praised in the late 2010s and 2020s.[12][3][11] Its coverage of controversial topics such as American politics and major events such as the COVID-19 pandemic has received substantial media attention. At various points, Wikipedia has been censored by world governments, ranging from the blocking of specific pages to bans on the entire site. Wikipedia has become an element of popular culture, with references in books, films and academic studies. In 2018, Facebook and YouTube announced that they would help users detect fake news by suggesting fact-checking links to related Wikipedia articles.[13][14]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52a1b2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia (/ˌwɪkɪˈpiːdiə/ (About this soundlisten) wik-ih-PEE-dee-ə or /ˌwɪki-/ (About this soundlisten) wik-ee-) is a free content, multilingual online encyclopedia written and maintained by a community of volunteer contributors through a model of open collaboration, using a wiki-based editing system.\n",
      "-----------------------------------\n",
      "Wikipedia is the largest and most-read reference work in history,[3] and is consistently one of the 15 most popular websites as ranked by Alexa; as of 2021, it was ranked as the 13th most popular site.\n",
      "-----------------------------------\n",
      "[3][4] The project carries no advertisements and is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through small donations.\n",
      "-----------------------------------\n",
      "[5]\n",
      "\n",
      "Wikipedia was launched on January 15, 2001, by Jimmy Wales[6] and Larry Sanger; Sanger coined its name as a blending of \"wiki\" and \"encyclopedia\".\n",
      "-----------------------------------\n",
      "[7][8] Initially available only in English, versions in other languages were quickly developed.\n",
      "-----------------------------------\n",
      "Combined, Wikipedia's editions comprise more than 56 million articles, that attract an average of around 2 billion unique device visits and receive more than 17 million edits per month, or develop at a rate of 1.9 edits per second.\n",
      "-----------------------------------\n",
      "[9][10]\n",
      "\n",
      "Wikipedia has received praise for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced amount of commercial bias, but criticism for exhibiting systemic bias, particularly gender bias against women.\n",
      "-----------------------------------\n",
      "[11] Its reliability was frequently criticized in the 2000s, but has improved over time and has been generally praised in the late 2010s and 2020s.\n",
      "-----------------------------------\n",
      "[12][3][11] Its coverage of controversial topics such as American politics and major events such as the COVID-19 pandemic has received substantial media attention.\n",
      "-----------------------------------\n",
      "At various points, Wikipedia has been censored by world governments, ranging from the blocking of specific pages to bans on the entire site.\n",
      "-----------------------------------\n",
      "Wikipedia has become an element of popular culture, with references in books, films and academic studies.\n",
      "-----------------------------------\n",
      "In 2018, Facebook and YouTube announced that they would help users detect fake news by suggesting fact-checking links to related Wikipedia articles.\n",
      "-----------------------------------\n",
      "[13][14]\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in nltk.tokenize.sent_tokenize(article):\n",
    "    print(i)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8ea01046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sidhu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf268332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('venv_tf2.4': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0e43aa85e0d007ca602e04b3033c86d6af4f225b69c2da0d32fa6602213775d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
