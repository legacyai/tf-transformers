{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9869f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aca8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect dataset\n",
    "\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def hf_dump_chars_to_textfile(file, dataset, data_keys, max_char=-1):\n",
    "    \"\"\"Write part of a TFDS sentence dataset to lines in a text file.\n",
    "\n",
    "  Args:\n",
    "    dataset: tf.dataset containing string-data.\n",
    "    data_keys: what keys in dataset to dump from.\n",
    "    max_char: max character to dump to text file.\n",
    "\n",
    "  Returns:\n",
    "    name of temp file with dataset bytes, exact number of characters dumped.\n",
    "  \"\"\"\n",
    "    line_count = 0\n",
    "    with open(file, \"a+\") as outfp:\n",
    "        char_count = 0\n",
    "        for example in tqdm.tqdm(dataset):\n",
    "            for k in data_keys:\n",
    "                line = example[k]\n",
    "                line = line + \"\\n\"\n",
    "                char_count += len(line)\n",
    "                line_count += 1\n",
    "                outfp.write(line)\n",
    "\n",
    "    print(\"Total lines {}, chars {}\".format(line_count, char_count))\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20200501.en\")\n",
    "OUT_FILE = '/home/sidhu/Datasets/wikipedia.txt'\n",
    "hf_dump_chars_to_textfile(OUT_FILE, \n",
    "                          dataset[\"train\"],\n",
    "                          (\"text\",))\n",
    "\n",
    "\n",
    "\n",
    "# dataset = load_dataset(\"c4\", \"realnewslike\")\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import tqdm\n",
    "INDIR = \"/home/sidhu/Datasets/c4/realnewslike/\"\n",
    "OUTDIR = \"/home/sidhu/Datasets/c4_newsalike_text\"\n",
    "all_files = glob.glob(\"{}/*.json.gz\".format(INDIR))\n",
    "\n",
    "def normalize(json_str):\n",
    "    json_str = json_str.replace(\"{'text':\", \"\").replace(\"'timestamp':\", \"\").replace(\"'url:''\", \"\")\n",
    "    json_str = json_str.replace('{\"text\":', \"\").replace('\"timestamp\":', \"\").replace('\"url:\"', \"\")\n",
    "    json_str = json_str.replace(\"{text:\", \"\").replace(\"timestamp:\", \"\").replace(\"url:\", \"\")\n",
    "    json_str = json_str.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    json_str = json_str.replace(\"\\\\n\", \"\\n\")\n",
    "    return json_str.split(\"\\n\")\n",
    "\n",
    "for jsonfilename in tqdm.tqdm(all_files):\n",
    "    with gzip.open(jsonfilename, 'r') as fin:        # 4. gzip\n",
    "        json_bytes = fin.read()                      # 3. bytes (i.e. UTF-8)\n",
    "\n",
    "    json_str = normalize(json_bytes.decode('utf-8'))            # 2. string (i.e. JSON)\n",
    "\n",
    "    file_name = jsonfilename.split(\"/\")[-1].split(\".json\")[0]\n",
    "    line_count = 0\n",
    "    with open(\"{}/{}.txt\".format(OUTDIR,file_name) , \"a+\") as outfp:\n",
    "        char_count = 0\n",
    "        for line in json_str:\n",
    "            line = line + \"\\n\"\n",
    "            char_count += len(line)\n",
    "            line_count += 1\n",
    "            outfp.write(line)\n",
    "\n",
    "    print(\"Total lines {}, chars {}\".format(line_count, char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08142154",
   "metadata": {},
   "outputs": [],
   "source": [
    "212763665\n",
    "\n",
    "3187609 # After filtering length of 5\n",
    "\n",
    "74004228 # book corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4586f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Sentencepiece tokenizer (Albert)\n",
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='DATA/wikipedia.txt',\n",
    "                               model_prefix='bert-joint',\n",
    "                               vocab_size=32000,\n",
    "                               pad_id=0,\n",
    "                               unk_id=1,\n",
    "                               bos_id=-1,\n",
    "                               user_defined_symbols=['(', ')', '\"', '-', '.', '–', '£', '€'],\n",
    "                               control_symbols=['[CLS]','[SEP]','[MASK]'],\n",
    "                               shuffle_input_sentence=True,\n",
    "                               input_sentence_size=10000000,\n",
    "                               character_coverage=0.99995,\n",
    "                               model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfb449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54566f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone tft\n",
    "\n",
    "git clone -b modification https://github.com/legacyai/tf-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571dc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd02a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import tqdm\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "\n",
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tokenizer\n",
    "model_file_path = '/home/sidhu/Datasets/PRETRAIN_DATA/vocab/bert-joint.model'\n",
    "model_file_path = 'albert-tokenizer/spiece.model'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "model_file_path = 'sample/spiece.model'\n",
    "dtype = tf.int32\n",
    "nbest_size = 0\n",
    "alpha = 1.0\n",
    "\n",
    "def _create_tokenizer(model_serialized_proto, dtype, nbest_size, alpha):\n",
    "    return tf_text.SentencepieceTokenizer(\n",
    "        model=model_serialized_proto,\n",
    "        out_type=dtype,\n",
    "        nbest_size=nbest_size,\n",
    "        alpha=alpha)\n",
    "\n",
    "model_serialized_proto = tf.io.gfile.GFile(model_file_path,\n",
    "                                                       \"rb\").read()\n",
    "\n",
    "tokenizer_sp = _create_tokenizer(model_serialized_proto, \n",
    "                             dtype,\n",
    "                             nbest_size,\n",
    "                             alpha)\n",
    "    \n",
    "def map_tokenize(text):\n",
    "    #text = text.numpy().decode().strip()\n",
    "    return tokenizer_sp.tokenize(text).merge_dims(-1, 1).to_tensor()\n",
    "\n",
    "def map_tokenize_py(text):\n",
    "    input_ids = tf.py_function(map_tokenize, [text],\n",
    "                tf.int32)\n",
    "    return [input_ids]\n",
    "\n",
    "\n",
    "# Read wikipedia data\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(['This is text 1', 'This is text2 waw wdxce', 'This is text3', 'This is text4'])\n",
    "dataset = dataset.batch(2)\n",
    "ds = dataset.map(map_tokenize_py)\n",
    "\n",
    "for item in ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wikipedia data\n",
    "\n",
    "file_paths = ['DATA/wikipedia.txt']\n",
    "\n",
    "#file_buffer = open(file_paths[0])\n",
    "\n",
    "# file_paths = ['/tmp/tmpip9jwekj']\n",
    "dataset = tf.data.TextLineDataset(file_paths)\n",
    "BATCH_SIZE = 1024\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "\n",
    "def parse_train():\n",
    "    for batch_input in tqdm.tqdm(dataset):\n",
    "        batch_tokenized = tokenizer.tokenize(batch_input).merge_dims(-1,1).to_list()\n",
    "        for input_ids in batch_tokenized:\n",
    "            \n",
    "            yield {\"input_ids\": input_ids}\n",
    "        \n",
    "            \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = 'TFRECORD'\n",
    "tfrecord_filename = 'wikipedia'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    n_files=100,\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c11701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca046e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF way\n",
    "# from transformers import AlbertTokenizer\n",
    "\n",
    "# tokenizer_hf = AlbertTokenizer(vocab_file='vocab/bert-joint.model')\n",
    "\n",
    "# def parse_train_hf():\n",
    "#     for batch_input in tqdm.tqdm(dataset):\n",
    "#         batch_input = [item.decode() for item in batch_input.numpy()]\n",
    "#         batch_tokenized = tokenizer_hf(batch_input)['input_ids']\n",
    "#         for input_ids in batch_tokenized:\n",
    "            \n",
    "#             yield {\"input_ids\": input_ids}\n",
    "        \n",
    "            \n",
    "# schema = {\n",
    "#     \"input_ids\": (\"var_len\", \"int\"),\n",
    "# }\n",
    "\n",
    "# tfrecord_train_dir = 'DUMMY'\n",
    "# tfrecord_filename = 'wikipedia'\n",
    "# tfwriter = TFWriter(schema=schema, \n",
    "#                     file_name=tfrecord_filename, \n",
    "#                     model_dir=tfrecord_train_dir,\n",
    "#                     tag='train',\n",
    "#                     n_files=100,\n",
    "#                     overwrite=True\n",
    "#                     )\n",
    "# tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1ca48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b84414",
   "metadata": {},
   "outputs": [],
   "source": [
    "_MAX_SEQ_LEN = 128\n",
    "_MAX_PREDICTIONS_PER_BATCH = 20\n",
    "_VOCAB_SIZE = 32000\n",
    "_MIN_SEN_LEN = 5\n",
    "\n",
    "_START_TOKEN = tokenizer.string_to_id('[CLS]')\n",
    "_END_TOKEN = tokenizer.string_to_id('[SEP]')\n",
    "_MASK_TOKEN = tokenizer.string_to_id('[MASK]')\n",
    "#_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = tokenizer.string_to_id('<unk>')\n",
    "_PAD_TOKEN = tokenizer.string_to_id('<pad>')\n",
    "\n",
    "\n",
    "_START_TOKEN = 3\n",
    "_END_TOKEN = 4\n",
    "_MASK_TOKEN = 5\n",
    "#_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = 1\n",
    "_PAD_TOKEN = 0\n",
    "\n",
    "# Truncate inputs to a maximum length.\n",
    "trimmer = tf_text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "# Random Selector\n",
    "random_selector = tf_text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN, _PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# Mask Value chooser (Encapsulates the BERT MLM token selection logic)\n",
    "mask_values_chooser = tf_text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa537ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbf699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mlm(item):\n",
    "    # Tokenizer (always return Ragged tensor I think)\n",
    "    #segments = tokenizer.tokenize(item).merge_dims(1, -1)\n",
    "    # Trim based on maximum Sequence Length (list is important)\n",
    "    \n",
    "    segments = item['input_ids']\n",
    "    trimmed_segments = trimmer.trim([segments])\n",
    "    \n",
    "    # We replace trimmer with slice [:_MAX_SEQ_LEN-2] operation # 2 to add CLS and SEP\n",
    "    # input_ids = item['input_ids'][:_MAX_SEQ_LEN-2]\n",
    "    \n",
    "    # Combine segments, get segment ids and add special tokens.\n",
    "    segments_combined, segment_ids = tf_text.combine_segments(\n",
    "          trimmed_segments,\n",
    "          start_of_sequence_id=_START_TOKEN,\n",
    "          end_of_segment_id=_END_TOKEN)\n",
    "    \n",
    "    # We replace segment with concat\n",
    "    # input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "\n",
    "    # Apply dynamic masking\n",
    "    masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "      segments_combined,\n",
    "      item_selector=random_selector,\n",
    "      mask_values_chooser=mask_values_chooser)\n",
    "\n",
    "    # Prepare and pad combined segment inputs\n",
    "    input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "        masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "        segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    # Masked lm weights will mask the weights\n",
    "    masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "      masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "      masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    \n",
    "\n",
    "#     model_inputs = {\n",
    "#       \"input_ids\": input_word_ids,\n",
    "#       \"input_mask\": input_mask,\n",
    "#       \"input_type_ids\": input_type_ids,\n",
    "#       \"masked_lm_ids\": masked_lm_ids,\n",
    "#       \"masked_lm_positions\": masked_lm_positions,\n",
    "#       \"masked_lm_weights\": masked_lm_weights,\n",
    "#     }\n",
    "    \n",
    "    inputs = {}\n",
    "    inputs['input_ids'] = input_word_ids\n",
    "    inputs['input_type_ids'] = input_type_ids\n",
    "    inputs['input_mask'] = input_mask\n",
    "    inputs['masked_lm_positions'] = masked_lm_positions\n",
    "    \n",
    "    labels = {}\n",
    "    labels['masked_lm_labels'] = masked_lm_ids\n",
    "    labels['masked_lm_weights']   = masked_lm_weights # Mask\n",
    "    \n",
    "    return (inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c899ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and check timings\n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "tfrecord_train_dir = '/home/sidhu/Datasets/PRETRAIN_DATA/TFRECORD'\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids']\n",
    "\n",
    "MAX_LEN = 128\n",
    "batch_size = 512\n",
    "# padded_shapes = {'input_ids': [MAX_LEN], \n",
    "#                  'labels': [MAX_LEN], \n",
    "#                  'labels_mask': [MAX_LEN]}\n",
    "train_dataset = tf_reader.read_record(auto_batch=False, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   shuffle=True,\n",
    "                                   drop_remainder=True\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_length(x):\n",
    "    return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']) ,tf.constant(_MIN_SEN_LEN)), axis=0)\n",
    "train_dataset = train_dataset.filter(filter_by_length)\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\n",
    "train_dataset = train_dataset.map(map_mlm)\n",
    "\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(train_dataset):\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854740c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce37c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With AlbertTokenizer batch tokenization\n",
    "\n",
    "_MAX_SEQ_LEN = 128\n",
    "_MAX_PREDICTIONS_PER_BATCH = 20\n",
    "_VOCAB_SIZE = 32000\n",
    "_MIN_SEN_LEN = 5\n",
    "\n",
    "_START_TOKEN = 3\n",
    "_END_TOKEN = 4\n",
    "_MASK_TOKEN = 5\n",
    "#_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = 1\n",
    "_PAD_TOKEN = 0\n",
    "\n",
    "# Truncate inputs to a maximum length.\n",
    "trimmer = tf_text.RoundRobinTrimmer(max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "# Random Selector\n",
    "random_selector = tf_text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN, _PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# Mask Value chooser (Encapsulates the BERT MLM token selection logic)\n",
    "mask_values_chooser = tf_text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n",
    "\n",
    "#tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "def map_mlm(item):\n",
    "        \n",
    "    trimmed_segments = trimmer.trim([item])\n",
    "\n",
    "    # Combine segments, get segment ids and add special tokens.\n",
    "    segments_combined, segment_ids = tf_text.combine_segments(\n",
    "          trimmed_segments,\n",
    "          start_of_sequence_id=_START_TOKEN,\n",
    "          end_of_segment_id=_END_TOKEN)\n",
    "    \n",
    "\n",
    "\n",
    "    # Apply dynamic masking\n",
    "    masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "      segments_combined,\n",
    "      item_selector=random_selector,\n",
    "      mask_values_chooser=mask_values_chooser)\n",
    "\n",
    "    # Prepare and pad combined segment inputs\n",
    "    input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "        masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "        segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    # Masked lm weights will mask the weights\n",
    "    masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "      masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "      masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    \n",
    "\n",
    "    inputs = {}\n",
    "    inputs['input_ids'] = input_word_ids\n",
    "    inputs['input_type_ids'] = input_type_ids\n",
    "    inputs['input_mask'] = input_mask\n",
    "    inputs['masked_lm_positions'] = masked_lm_positions\n",
    "    \n",
    "    labels = {}\n",
    "    labels['masked_lm_labels'] = masked_lm_ids\n",
    "    labels['masked_lm_weights']   = masked_lm_weights # Mask\n",
    "    \n",
    "    return (inputs, labels)\n",
    "\n",
    "def encode(item):\n",
    "    \n",
    "    item = [text.decode() for text in item.numpy()]\n",
    "    input_ids = tokenizer(item,\n",
    "                          max_length=_MAX_SEQ_LEN,\n",
    "                          add_special_tokens=False, \n",
    "                          padding=True, \n",
    "                          truncation=True)['input_ids']\n",
    "    return [input_ids]\n",
    "\n",
    "def tokenize_hf(item):\n",
    "    \n",
    "    input_ids = tf.py_function(encode, [item],\n",
    "                tf.int32)\n",
    "    return input_ids\n",
    "\n",
    "def partition_dense_to_ragged_by_pad_length(item, pad_id):\n",
    "    \n",
    "    tf.assert_equal(pad_id, 0)\n",
    "    # We are assuming pad id is 0, otherwise logic breaks\n",
    "    # Flatten the data (2D tensor to 1D tensor)\n",
    "    \n",
    "    # Bloody reshape wont work\n",
    "    # data = tf.reshape(item, [])\n",
    "    \n",
    "    n_elems = tf.reduce_prod(tf.shape(item))\n",
    "    data    = tf.reshape(item, (n_elems,))\n",
    "    \n",
    "    # Giving row numbers for each row we assigne (1, 2, 3)\n",
    "    num_batches = tf.shape(item)[0]+1\n",
    "    row_numbers = tf.range(1, num_batches, delta=1,name='range')\n",
    "    row_numbers = tf.expand_dims(row_numbers, 1)\n",
    "    partitions  = row_numbers + tf.zeros_like(item)\n",
    "    partitions = tf.reshape(partitions, (n_elems,))\n",
    "    \n",
    "    # Flattent partitions\n",
    "    partitions_masked = tf.cast(tf.not_equal(data, pad_id), tf.int32)\n",
    "    partitions= partitions * partitions_masked\n",
    "    \n",
    "    ragged_tensors = tf.ragged.stack_dynamic_partitions(data, partitions, num_batches)\n",
    "    ragged_tensors = ragged_tensors[1:].merge_dims(-1,1) # First element is padded , we dont want that\n",
    "    ragged_tensors = tf.RaggedTensor.with_row_splits_dtype(ragged_tensors, dtype=tf.int64)\n",
    "    return ragged_tensors\n",
    "\n",
    "\n",
    "def filter_empty_string(line):\n",
    "    return tf.not_equal(tf.strings.length(line),0)\n",
    "\n",
    "def normalize_string(line):\n",
    "    return tf.strings.strip(line)\n",
    "\n",
    "def filter_by_batch(x, y, batch_size):\n",
    "    x_batch = tf.shape(x['input_ids'])[0]\n",
    "    return tf.equal(x_batch, tf.constant(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd66572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/home/sidhu/Datasets/imdb.txt']\n",
    "train_dataset = tf.data.TextLineDataset(file_paths)\n",
    "train_dataset = train_dataset.map(normalize_string,\n",
    "                                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.filter(filter_empty_string)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "#train_dataset  = train_dataset.map(tokenize_hf, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset  = train_dataset.map(map_tokenize_py, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_dataset  = train_dataset.map(lambda x: partition_dense_to_ragged_by_pad_length(x,0), \n",
    "             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset  = train_dataset.map(map_mlm, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(100)\n",
    "train_dataset  = train_dataset.filter(lambda x, y: filter_by_batch(x, y, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552afd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now Check with HF tokenizer\n",
    "\n",
    "# from transformers import AlbertTokenizer\n",
    "# tokenizer_hf = AlbertTokenizer(vocab_file='/home/sidhu/Datasets/PRETRAIN_DATA/vocab/bert-joint.model')\n",
    "\n",
    "# file_paths = ['/home/sidhu/Datasets/wikipedia.txt']\n",
    "# dataset = tf.data.TextLineDataset(file_paths)\n",
    "\n",
    "\n",
    "# def filter_empty_string(line):\n",
    "#     return tf.not_equal(tf.strings.length(line),0)\n",
    "\n",
    "# def normalize_string(line):\n",
    "#     return tf.strings.strip(line)\n",
    "\n",
    "# def hf_tokenize(item):\n",
    "#     item = item.numpy().decode()\n",
    "#     input_ids = tokenizer_hf(item, add_special_tokens=False)['input_ids']\n",
    "#     return [input_ids]\n",
    "\n",
    "# def map_mlm_hf(item):\n",
    "#     input_ids = tf.py_function(hf_tokenize, [item],\n",
    "#                    tf.int32)\n",
    "#     # MAX_SEQ_LEN - 2 # 2 for CLS and SEP\n",
    "#     input_ids = input_ids[:_MAX_SEQ_LEN-2]\n",
    "#     input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "#     segments_combined = tf.RaggedTensor.from_tensor([input_ids])\n",
    "#     segment_ids = tf.zeros_like(segments_combined)\n",
    "#     # Apply dynamic masking\n",
    "#     masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "#       segments_combined,\n",
    "#       item_selector=random_selector,\n",
    "#       mask_values_chooser=mask_values_chooser)\n",
    "    \n",
    "#     masked_token_ids = tf.squeeze(masked_token_ids.to_tensor(), axis=0)\n",
    "#     masked_pos = tf.squeeze(masked_pos.to_tensor(), axis=0)\n",
    "#     masked_lm_ids = tf.squeeze(masked_lm_ids.to_tensor(), axis=0)\n",
    "#     # Prepare and pad combined segment inputs\n",
    "#     # input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "#         # masked_token_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "#     # input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "#         # segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "#     # Prepare and pad masking task inputs\n",
    "#     # Masked lm weights will mask the weights\n",
    "# #     masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "# #       masked_pos, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "# #     masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "# #       masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    \n",
    "\n",
    "#     model_inputs = {\n",
    "#       \"input_ids\": masked_token_ids,\n",
    "#       \"input_mask\": tf.ones_like(masked_token_ids),\n",
    "#       \"input_type_ids\": tf.zeros_like(masked_token_ids),\n",
    "#       \"masked_lm_labels\": masked_lm_ids,\n",
    "#       \"masked_lm_positions\": masked_pos,\n",
    "#       \"masked_lm_weights\": tf.ones_like(masked_pos),\n",
    "#     }\n",
    "    \n",
    "#     return model_inputs\n",
    "\n",
    "# dataset = dataset.map(normalize_string)\n",
    "# dataset = dataset.filter(filter_empty_string)\n",
    "\n",
    "# train_dataset = dataset.map(map_mlm_hf)    \n",
    "# batch_size = 1024\n",
    "# padded_shapes = {\n",
    "#       \"input_ids\": [_MAX_SEQ_LEN],\n",
    "#       \"input_mask\": [_MAX_SEQ_LEN],\n",
    "#       \"input_type_ids\": [_MAX_SEQ_LEN],\n",
    "#       \"masked_lm_labels\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "#       \"masked_lm_positions\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "#       \"masked_lm_weights\": [_MAX_PREDICTIONS_PER_BATCH],\n",
    "#     }\n",
    "# train_dataset = auto_batch(\n",
    "#     train_dataset,\n",
    "#     batch_size,\n",
    "#     padded_values=None,\n",
    "#     padded_shapes=padded_shapes,\n",
    "#     x_keys=['input_ids', 'input_type_ids', 'input_mask', 'masked_lm_positions'],\n",
    "#     y_keys=['masked_lm_labels', 'masked_lm_weights'],\n",
    "#     shuffle=False,\n",
    "#     drop_remainder=False,\n",
    "#     shuffle_buffer_size=100,\n",
    "#     prefetch_buffer_size=100,\n",
    "# )\n",
    "\n",
    "# for (batch_inputs, batch_labels) in train_dataset:\n",
    "#     print(batch_inputs, batch_labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4e504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71c681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94abfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build a dummy dataset and see whether it works\n",
    "\n",
    "# input_ids = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 128), dtype=tf.int32)\n",
    "# masked_lm_positions = tf.random.uniform(minval=0, maxval=128, shape=(4000, 20), dtype=tf.int32)\n",
    "# masked_lm_labels = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 20), dtype=tf.int32)\n",
    "# masked_lm_weights = tf.random.uniform(minval=0, maxval=2, shape=(4000, 20), dtype=tf.int32)\n",
    "# dummy_batch_inputs = {\"input_ids\": input_ids, \n",
    "#                       \"input_mask\": tf.ones_like(input_ids), \n",
    "#                       \"input_type_ids\": tf.zeros_like(input_ids), \n",
    "#                       \"masked_lm_positions\": masked_lm_positions}\n",
    "\n",
    "# dummy_batch_labels = {\"masked_lm_labels\": masked_lm_labels, \n",
    "#                      \"masked_lm_weights\": masked_lm_weights}\n",
    "\n",
    "\n",
    "# batch_size = 1024\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((dummy_batch_inputs, dummy_batch_labels)).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_ids = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 128), dtype=tf.int32)\n",
    "#masked_lm_positions = tf.random.uniform(minval=0, maxval=128, shape=(4000, 20), dtype=tf.int32)\n",
    "masked_lm_labels = tf.random.uniform(minval=0, maxval=32000, shape=(4000, 128), dtype=tf.int32)\n",
    "masked_lm_weights = tf.random.uniform(minval=0, maxval=2, shape=(4000, 128), dtype=tf.int32)\n",
    "dummy_batch_inputs = {\"input_ids\": input_ids, \n",
    "                      \"input_mask\": tf.ones_like(input_ids), \n",
    "                      \"input_type_ids\": tf.zeros_like(input_ids)\n",
    "                      }\n",
    "\n",
    "dummy_batch_labels = {\"masked_lm_labels\": masked_lm_labels, \n",
    "                     \"masked_lm_weights\": masked_lm_weights}\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dummy_batch_inputs, dummy_batch_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da9658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modeling\n",
    "\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.core import Trainer, TPUTrainer\n",
    "\n",
    "def get_model():\n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 32000,\n",
    "        \"layer_norm_epsilon\": 1e-12\n",
    "    }\n",
    "    \n",
    "    from tf_transformers.models import BertModel\n",
    "    model = BertModel.from_config(config,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                 use_masked_lm_positions=True, # Add batch_size to avoid dynamic shapes\n",
    "                                return_all_layer_outputs=True) \n",
    "\n",
    "#     model = BertModel.from_config(config,\n",
    "#                                   batch_size=batch_size,\n",
    "#                                  use_masked_lm_positions=False, # Add batch_size to avoid dynamic shapes\n",
    "#                                 return_all_layer_outputs=False) \n",
    "    return model\n",
    "def get_optimizer():\n",
    "    init_lr = 2e-05\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=init_lr, \n",
    "                                                 num_train_steps=100000,\n",
    "                                                 num_warmup_steps=10000)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                            logits=y_pred_dict['token_logits'], \n",
    "                            label_weights=y_true_dict['masked_lm_weights'])\n",
    "    return {\"loss\": loss}\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "#     token_logits = y_pred_dict['all_layer_token_logits'][-1]\n",
    "#     loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "#                             logits=token_logits, \n",
    "#                             label_weights=y_true_dict['labels_mask'])\n",
    "#     return {\"loss\": loss}\n",
    "    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        \n",
    "        loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                logits=per_layer_output, \n",
    "                                label_weights=y_true_dict['masked_lm_weights'])\n",
    "        loss_dict['loss_{}'.format(layer_count+1)] = loss\n",
    "        loss_holder.append(loss)\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293754d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_address = 'local'\n",
    "trainer =  TPUTrainer(\n",
    "    tpu_address=tpu_address,\n",
    "    dtype='fp32'\n",
    ")\n",
    "\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE\n",
    "MODEL_DIR  ='bert_wiki_joint'\n",
    "\n",
    "training_loss_names = ['loss_1',\n",
    " 'loss_2',\n",
    " 'loss_3',\n",
    " 'loss_4',\n",
    " 'loss_5',\n",
    " 'loss_6',\n",
    " 'loss_7',\n",
    " 'loss_8',\n",
    " 'loss_9',\n",
    " 'loss_10',\n",
    " 'loss_11',\n",
    " 'loss_12']\n",
    "\n",
    "#training_loss_names = None\n",
    "trainer.run(\n",
    "    model_fn=get_model,\n",
    "    optimizer_fn=get_optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    train_loss_fn=lm_loss,\n",
    "    epochs=2,\n",
    "    steps_per_epoch=200,\n",
    "    model_checkpoint_dir=MODEL_DIR, # gs://tft_free/\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    "    training_loss_names=training_loss_names,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=True\n",
    ")\n",
    "\n",
    "import os\n",
    "os.system(\"gsutil -m cp -R {} gs://tft_free/{}\".format(MODEL_DIR, MODEL_DIR))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
