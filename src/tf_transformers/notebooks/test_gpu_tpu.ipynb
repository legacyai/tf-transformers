{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_text as tf_text\n",
    "import tqdm\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.core import  TPUTrainer\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.losses import cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFRecord or read from gs bucket\n",
    "\n",
    "# Read TFrecords\n",
    "\n",
    "import json\n",
    "import glob\n",
    "\n",
    "tfrecord_train_dir = '/home/sidhu/Datasets/tfrecord_cnn_train'\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids']\n",
    "y_keys = ['labels', 'labels_mask']\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "padded_shapes = {'input_ids': [MAX_LEN], \n",
    "                 'labels': [MAX_LEN], \n",
    "                 'labels_mask': [MAX_LEN]}\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True,\n",
    "                                   drop_remainder=True,\n",
    "                                   padded_shapes=padded_shapes\n",
    "                                  )\n",
    "\n",
    "\n",
    "# When mask_mode == 'user-defined' use this map\n",
    "\n",
    "def map_add_input_mask(x, y):\n",
    "    \n",
    "    x['input_mask'] = tf.ones_like(x['input_ids'])\n",
    "    return x, y\n",
    "\n",
    "train_dataset = train_dataset.map(map_add_input_mask)\n",
    "\n",
    "\n",
    "def map_add_input_mask_bert(x, y):\n",
    "    \n",
    "    x['input_mask'] = tf.ones_like(x['input_ids'])\n",
    "    x['input_type_ids'] = tf.zeros_like(x['input_ids'])\n",
    "    return x, y\n",
    "\n",
    "train_dataset = train_dataset.map(map_add_input_mask_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd1b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "\n",
    "    from tf_transformers.models import GPT2Model\n",
    "    # model = GPT2Model.from_pretrained(\"gpt2\", save_checkpoint_cache=False)\n",
    "    config = {\n",
    "      \"attention_probs_dropout_prob\": 0.1,\n",
    "      \"hidden_act\": \"gelu\",\n",
    "      \"intermediate_act\": \"gelu\",\n",
    "      \"hidden_dropout_prob\": 0.1,\n",
    "      \"embedding_size\": 768,\n",
    "      \"initializer_range\": 0.02,\n",
    "      \"intermediate_size\": 3072,\n",
    "      \"max_position_embeddings\": 1024,\n",
    "      \"num_attention_heads\": 12,\n",
    "      \"attention_head_size\": 64,\n",
    "      \"num_hidden_layers\": 12,\n",
    "      \"type_vocab_size\": -1,\n",
    "      \"vocab_size\": 50257,\n",
    "      \"layer_norm_epsilon\": 1e-05\n",
    "    }\n",
    "    model = GPT2Model.from_config(config, mask_mode=\"user_defined\")\n",
    "    print(\"Model inputs --->\", model.input)\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    from tf_transformers.models import BertModel\n",
    "    # Change vocab size to 50257 as in GPT2\n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"layer_norm_epsilon\": 1e-12\n",
    "        }\n",
    "    model = BertModel.from_config(config)\n",
    "    print(\"Model inputs --->\", model.input)\n",
    "    print(\"Model variables --->\", len(model.variables))\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    from tf_transformers.models import BertModel\n",
    "    # Change vocab size to 50257 as in GPT2\n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"layer_norm_epsilon\": 1e-12\n",
    "        }\n",
    "    model = BertModel.from_config(config, return_all_layer_outputs=True)\n",
    "    print(\"Model inputs --->\", model.input)\n",
    "    print(\"Model variables --->\", len(model.variables))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer():\n",
    "\n",
    "    init_lr = 2e-05\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=init_lr, \n",
    "                                                 num_train_steps=100000,\n",
    "                                                 num_warmup_steps=10000)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                            logits=y_pred_dict['token_logits'], \n",
    "                            label_weights=y_true_dict['labels_mask'])\n",
    "    return {\"loss\": loss}\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "#     token_logits = y_pred_dict['all_layer_token_logits'][-1]\n",
    "#     loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "#                             logits=token_logits, \n",
    "#                             label_weights=y_true_dict['labels_mask'])\n",
    "#     return {\"loss\": loss}\n",
    "    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        \n",
    "        loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                                logits=per_layer_output, \n",
    "                                label_weights=y_true_dict['labels_mask'])\n",
    "        loss_dict['loss_{}'.format(layer_count+1)] = loss\n",
    "        loss_holder.append(loss)\n",
    "#         if layer_count == 0:\n",
    "#             l = loss\n",
    "#         else:\n",
    "#             l += loss\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "    #loss_dict['loss'] = l/(layer_count+1)\n",
    "    return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c1d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a704dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_address = 'local'\n",
    "trainer =  TPUTrainer(\n",
    "    tpu_address=tpu_address,\n",
    "    dtype='bf16'\n",
    ")\n",
    "\n",
    "GLOBAL_BATCH_SIZE = batch_size\n",
    "\n",
    "training_loss_names = ['loss_1',\n",
    " 'loss_2',\n",
    " 'loss_3',\n",
    " 'loss_4',\n",
    " 'loss_5',\n",
    " 'loss_6',\n",
    " 'loss_7',\n",
    " 'loss_8',\n",
    " 'loss_9',\n",
    " 'loss_10',\n",
    " 'loss_11',\n",
    " 'loss_12']\n",
    "\n",
    "#training_loss_names = None\n",
    "trainer.run(\n",
    "    model_fn=get_model,\n",
    "    optimizer_fn=get_optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    train_loss_fn=lm_loss,\n",
    "    epochs=2,\n",
    "    steps_per_epoch=300,\n",
    "    model_checkpoint_dir=\"temp_dir\", # gs://tft_free/\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    "    training_loss_names=training_loss_names,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=None,\n",
    "    callbacks_interval_steps=None,\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=True\n",
    ")\n",
    "\n",
    "\n",
    "#!gsutil -cp -r temp_dir/logs gs:tft_free/logs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c35e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd06046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset and check timings\n",
    "tfrecord_train_dir = '/home/sidhu/Datasets/PRETRAIN_DATA/TFRECORD'\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "\n",
    "\n",
    "index = 1\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "\n",
    "#all_files = [all_files[index]]\n",
    "\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids']\n",
    "\n",
    "MAX_LEN = 128\n",
    "batch_size = 128\n",
    "# padded_shapes = {'input_ids': [MAX_LEN], \n",
    "#                  'labels': [MAX_LEN], \n",
    "#                  'labels_mask': [MAX_LEN]}\n",
    "train_dataset = tf_reader.read_record(auto_batch=False, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   shuffle=True,\n",
    "                                   drop_remainder=True\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "def filter_by_length(x):\n",
    "    return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']) ,tf.constant(_MIN_SEN_LEN)), axis=0)\n",
    "\n",
    "def filter_by_batch(x, y):\n",
    "    x_batch = tf.shape(x['input_ids'])[0]\n",
    "    return tf.equal(x_batch, tf.constant(batch_size))\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_by_length)\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\n",
    "train_dataset = train_dataset.map(map_mlm)\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_by_batch)\n",
    "all_batches = []\n",
    "for (batch_inputs, batch_labels) in tqdm.tqdm(train_dataset):\n",
    "    all_batches.append(batch_inputs['input_ids'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48019a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e8633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_distributed = iter(trainer.distribution_strategy.experimental_distribute_dataset(\n",
    "                train_dataset.repeat(1)\n",
    "            ))\n",
    "\n",
    "dist_inputs = next(train_dataset_distributed)\n",
    "batch_inputs = trainer.distribution_strategy.experimental_local_results(dist_inputs[0])\n",
    "self.input_ids_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
