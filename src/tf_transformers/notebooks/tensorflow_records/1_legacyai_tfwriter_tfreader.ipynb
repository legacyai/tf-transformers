{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/home/tf_transformers2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest versions have name changed from nlp -> datasets\n",
    "# pip install datasets\n",
    "from nlp import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from tf_transformers.utils import TFWriter, TFReader\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_squad(dataset, tokenizer, context_limit=64, max_counter=10000):\n",
    "    for counter,entry in enumerate(dataset):\n",
    "        answer = entry['answers']['text'][0]\n",
    "        context_ids = tokenizer.encode(entry['context'])\n",
    "        if len(context_ids) < context_limit:\n",
    "            diff = context_limit - len(context_ids)\n",
    "            context_ids = context_ids + [0] * diff\n",
    "        id_ = entry['id']\n",
    "        question_ids = tokenizer.encode(entry['question'])\n",
    "        title_tokens = entry['title'].split('_')\n",
    "        \n",
    "        result = {}\n",
    "        result['answer'] = answer\n",
    "        result['context_ids'] = context_ids[:context_limit]\n",
    "        result['id'] = id_\n",
    "        result['question_ids'] = question_ids\n",
    "        result['title_tokens'] = title_tokens\n",
    "        result['dummy_classification_label']  = np.random.randint(0, 10)\n",
    "        result['dummy_float_id'] = np.random.randint(0,10000) * 1.0 # make it float\n",
    "        \n",
    "        if counter < max_counter:\n",
    "            yield result\n",
    "        else:\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is TFWriter ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TFWriter is an easy to use utility for creating TFRecords . \n",
    "* TFWriter should accept a schema (dict), which has a specific pattern .\n",
    "* Schemas are easy to generate .\n",
    "\n",
    "  There are 2 types of schema:\n",
    "        * VariableFeature schema (var_len)\n",
    "            This means, we can have variable length entries in TFRecords and we can pad later:\n",
    "            The keyword for this schema is varlen\n",
    "            \n",
    "        * FixedFeature schema (fixed_len)\n",
    "            This means, we can have only fixed length entries in TFRecords and we have to determine\n",
    "            the fixed length while creating the schema itself\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a schema for processing squad data\n",
    "\n",
    "# A schema is a dictionary with keys and values\n",
    "\n",
    "# bytes - for text\n",
    "# int   - for integer\n",
    "# float - for float\n",
    "\n",
    "# var_len - represents variable length feature\n",
    "# fixed_len - represents fixed length feature\n",
    "\n",
    "# answer -> (text) text\n",
    "# context_ids -> (list of numbers) fixed_len (we have to specify the limit, we keep it 64 for demo)\n",
    "# id -> (text) text\n",
    "# question_ids -> (list of numbers) int . variable_len (Unlike fixed_len, we do not need to specify the limit)\n",
    "# title_tokens -> (list of tokens)\n",
    "# dummy_classification_label -> (scalar) int\n",
    "# dummy float_id -> (scalar) float\n",
    "\n",
    "squad_schema = {\n",
    "    \"answer\": (\"var_len\", \"bytes\"),\n",
    "    \"context_ids\": (\"fixed_len\", \"int\", [64]),\n",
    "    \"id\": (\"var_len\", \"bytes\"),\n",
    "    \"question_ids\": (\"var_len\", \"int\"),\n",
    "    \"title_tokens\": (\"var_len\", \"bytes\"),\n",
    "    \"dummy_classification_label\": (\"var_len\", \"int\"),\n",
    "    \"dummy_float_id\": (\"var_len\", \"float\")\n",
    "}\n",
    "\n",
    "\n",
    "# schema (above defined)\n",
    "# file_name -> name of the records generated\n",
    "# model_dir -> name of the model directory\n",
    "# tag -> ['dev' or 'train'] ('dev' will not shuffle any data whatsoever) .If `train`, we can shuffle or not\n",
    "# n_files -> No of records files (Large dataset normally split into multiple tfrecords)\n",
    "# shuffle -> True/False (only for tag = `train`)\n",
    "# max_files_per_record -> Total number of entries (individual examples) write per record\n",
    "# overwrite: True/False . If True, if model_dir exists, we will overwrite it\n",
    "# verbose_counter: How often you want to print the logs\n",
    "\n",
    "model_dir = 'tfrecords_tmp'\n",
    "tfwriter = TFWriter(schema=squad_schema,\n",
    "                 file_name = 'squad',\n",
    "                 model_dir = model_dir,\n",
    "                 tag='train',\n",
    "                 n_files=10,\n",
    "                 shuffle=False,\n",
    "                 max_files_per_record = 2000,\n",
    "                 overwrite=False, \n",
    "                 verbose_counter = 5000)\n",
    "\n",
    "# One way\n",
    "\n",
    "# Iterate over your entry and write it\n",
    "# entry -> a dict with above mentioned keys as in schema\n",
    "# make sure values support the type as in your schema\n",
    "\n",
    "# for entry in train_parser:\n",
    "#     tfwriter.write_record(entry)\n",
    "    \n",
    "# Recommended way ( We will take only 10000 examples from Squad)\n",
    "train_parser = parse_squad(dataset['train'], tokenizer)\n",
    "tfwriter.process(train_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['schema.json',\n",
       " 'squad_train_0.tfrecord',\n",
       " 'squad_train_1.tfrecord',\n",
       " 'squad_train_2.tfrecord',\n",
       " 'squad_train_3.tfrecord',\n",
       " 'squad_train_4.tfrecord']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets have a look at the generated files\n",
    "import os\n",
    "os.listdir('/mnt/home/PRE_MODELS/HuggingFace_models/tfrecords_tmp')\n",
    "\n",
    "# Or you can do !ls -lr\n",
    "\n",
    "# to see the number of examples per record\n",
    "# This will show, how many examples each record has written\n",
    "print(tfwriter.examples_per_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable shuffle = True\n",
    "model_dir = 'tfrecords_tmp'\n",
    "dataset = load_dataset('squad')\n",
    "tfwriter = TFWriter(schema=squad_schema,\n",
    "                 file_name = 'squad',\n",
    "                 model_dir = model_dir,\n",
    "                 tag='train',\n",
    "                 n_files=10,\n",
    "                 shuffle=True,\n",
    "                 max_files_per_record = 2000,\n",
    "                 overwrite=False, \n",
    "                 verbose_counter = 5000)\n",
    "\n",
    "# One way\n",
    "\n",
    "# Iterate over your entry and write it\n",
    "# entry -> a dict with above mentioned keys as in schema\n",
    "# make sure values support the type as in your schema\n",
    "\n",
    "# for entry in train_parser:\n",
    "#     tfwriter.write_record(entry)\n",
    "    \n",
    "# Recommended way ( We will take only 10000 examples from Squad)\n",
    "train_parser = parse_squad(dataset['train'], tokenizer)\n",
    "tfwriter.process(train_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can see 10 files with different counts\n",
    "# Reason being, by default n_files = 10 .\n",
    "# We create n_files, and then keep choosing the files randomly\n",
    "# this will add a lot of shuffleness to the data, while \n",
    "# writing itself\n",
    "print(tfwriter.examples_per_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tfrecords\n",
    "\n",
    "# Now lets see how we can use TFReeader + schema to read the records\n",
    "\n",
    "# schema --> You can load schema using json.load(open(model_dir + 'schema.json'))\n",
    "# Schema will be saved in the same directory where tfrecords are saved\n",
    "\n",
    "# tfrecord_files --> list of tfrecord files . (Make sure only tfrecord files have to be provided). Any other files\n",
    "# will cause error\n",
    "\n",
    "# shuffle_files --> True/False (whether input reading has to be shuffled or not)\n",
    "# keys --> an array of keys, we have in schema. If it is empty, we will read record with all the keys as in schema\n",
    "# If keys = ['answer'] (example), we will read only `answer` from the tfrecords\n",
    "\n",
    "# select files .tfrecord extension \n",
    "import glob\n",
    "all_files = glob.glob('tfrecords_tmp/*.tfrecord')\n",
    "all_files = [all_files[0], all_files[1]]\n",
    "keys = ['answer', \n",
    "        'context_ids', \n",
    "        'id',\n",
    "        'question_ids',\n",
    "        'title_tokens', \n",
    "        'dummy_classification_label',\n",
    "        ] # excluding 'dummy_float_id'\n",
    "tf_reader = TFReader(schema = squad_schema,\n",
    "                     tfrecord_files = all_files, \n",
    "                     shuffle_files=False,\n",
    "                     keys=keys)\n",
    "\n",
    "\n",
    "    \n",
    "# item is one example\n",
    "# If you look at item.keys() , it is same as keys we have provided\n",
    "\n",
    "# Lets do a simple batch and pad\n",
    "dataset_batch = dataset.padded_batch(batch_size=5, \n",
    "                           padding_values={'answer': tf.constant('', tf.string), \n",
    "                                               'context_ids': tf.constant(0,  tf.int64), \n",
    "                                               'id': tf.constant('',  tf.string),\n",
    "                                               'question_ids': tf.constant(1,  tf.int32), \n",
    "                                               'title_tokens': tf.constant('',  tf.string), \n",
    "                                               'dummy_classification_label': tf.constant(0,  tf.int32)\n",
    "                                               })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
