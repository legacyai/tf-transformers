{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7cb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/home/TF_NEW/tf-transformers/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656baaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdb0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/Crypto/Random/Fortuna/FortunaGenerator.py:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if sys.version_info[0] is 2 and  sys.version_info[1] is 1:\n",
      "/usr/lib/python3/dist-packages/Crypto/Random/Fortuna/FortunaGenerator.py:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if sys.version_info[0] is 2 and  sys.version_info[1] is 1:\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import collections\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50a562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define needed arguments .\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/data/create_pretraining_data.py\n",
    "do_lower_case = True\n",
    "do_whole_word_mask = False\n",
    "max_ngram_size = None # \"Mask contiguous whole words (n-grams) of up to `max_ngram_size` using a \"\n",
    "                      # \"weighting scheme to favor shorter n-grams. \"\n",
    "                      # \"Note: `--do_whole_word_mask=True` must also be set when n-gram masking.\")\n",
    "    \n",
    "max_seq_length = 128\n",
    "max_predictions_per_seq = 20\n",
    "random_seed = 12345\n",
    "dupe_factor = 1\n",
    "masked_lm_prob = 0.15\n",
    "short_seq_prob = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(random_seed)\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "# A _Gram is a [half-open) interval of token indices which form a word.\n",
    "# E.g.,\n",
    "#   words:  [\"The\", \"doghouse\"]\n",
    "#   tokens: [\"The\", \"dog\", \"##house\"]\n",
    "#   grams:  [(0,1), (1,3)]\n",
    "_Gram = collections.namedtuple(\"_Gram\", [\"begin\", \"end\"])\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer_path = \"../../PRE_MODELS/HuggingFace_models/tokenizers/bert_base_cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "vocab_words = list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf28667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _window(iterable, size):\n",
    "  \"\"\"Helper to create a sliding window iterator with a given size.\n",
    "  E.g.,\n",
    "    input = [1, 2, 3, 4]\n",
    "    _window(input, 1) => [1], [2], [3], [4]\n",
    "    _window(input, 2) => [1, 2], [2, 3], [3, 4]\n",
    "    _window(input, 3) => [1, 2, 3], [2, 3, 4]\n",
    "    _window(input, 4) => [1, 2, 3, 4]\n",
    "    _window(input, 5) => None\n",
    "  Args:\n",
    "    iterable: elements to iterate over.\n",
    "    size: size of the window.\n",
    "  Yields:\n",
    "    Elements of `iterable` batched into a sliding window of length `size`.\n",
    "  \"\"\"\n",
    "  i = iter(iterable)\n",
    "  window = []\n",
    "  try:\n",
    "    for e in range(0, size):\n",
    "      window.append(next(i))\n",
    "    yield window\n",
    "  except StopIteration:\n",
    "    # handle the case where iterable's length is less than the window size.\n",
    "    return\n",
    "  for e in i:\n",
    "    window = window[1:] + [e]\n",
    "    yield window\n",
    "\n",
    "\n",
    "def _contiguous(sorted_grams):\n",
    "  \"\"\"Test whether a sequence of grams is contiguous.\n",
    "  Args:\n",
    "    sorted_grams: _Grams which are sorted in increasing order.\n",
    "  Returns:\n",
    "    True if `sorted_grams` are touching each other.\n",
    "  E.g.,\n",
    "    _contiguous([(1, 4), (4, 5), (5, 10)]) == True\n",
    "    _contiguous([(1, 2), (4, 5)]) == False\n",
    "  \"\"\"\n",
    "  for a, b in _window(sorted_grams, 2):\n",
    "    if a.end != b.begin:\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "def _masking_ngrams(grams, max_ngram_size, max_masked_tokens, rng):\n",
    "  \"\"\"Create a list of masking {1, ..., n}-grams from a list of one-grams.\n",
    "  This is an extention of 'whole word masking' to mask multiple, contiguous\n",
    "  words such as (e.g., \"the red boat\").\n",
    "  Each input gram represents the token indices of a single word,\n",
    "     words:  [\"the\", \"red\", \"boat\"]\n",
    "     tokens: [\"the\", \"red\", \"boa\", \"##t\"]\n",
    "     grams:  [(0,1), (1,2), (2,4)]\n",
    "  For a `max_ngram_size` of three, possible outputs masks include:\n",
    "    1-grams: (0,1), (1,2), (2,4)\n",
    "    2-grams: (0,2), (1,4)\n",
    "    3-grams; (0,4)\n",
    "  Output masks will not overlap and contain less than `max_masked_tokens` total\n",
    "  tokens.  E.g., for the example above with `max_masked_tokens` as three,\n",
    "  valid outputs are,\n",
    "       [(0,1), (1,2)]  # \"the\", \"red\" covering two tokens\n",
    "       [(1,2), (2,4)]  # \"red\", \"boa\", \"##t\" covering three tokens\n",
    "  The length of the selected n-gram follows a zipf weighting to\n",
    "  favor shorter n-gram sizes (weight(1)=1, weight(2)=1/2, weight(3)=1/3, ...).\n",
    "  Args:\n",
    "    grams: List of one-grams.\n",
    "    max_ngram_size: Maximum number of contiguous one-grams combined to create\n",
    "      an n-gram.\n",
    "    max_masked_tokens: Maximum total number of tokens to be masked.\n",
    "    rng: `random.Random` generator.\n",
    "  Returns:\n",
    "    A list of n-grams to be used as masks.\n",
    "  \"\"\"\n",
    "  if not grams:\n",
    "    return None\n",
    "\n",
    "  grams = sorted(grams)\n",
    "  num_tokens = grams[-1].end\n",
    "\n",
    "  # Ensure our grams are valid (i.e., they don't overlap).\n",
    "  for a, b in _window(grams, 2):\n",
    "    if a.end > b.begin:\n",
    "      raise ValueError(\"overlapping grams: {}\".format(grams))\n",
    "\n",
    "  # Build map from n-gram length to list of n-grams.\n",
    "  ngrams = {i: [] for i in range(1, max_ngram_size+1)}\n",
    "  for gram_size in range(1, max_ngram_size+1):\n",
    "    for g in _window(grams, gram_size):\n",
    "      if _contiguous(g):\n",
    "        # Add an n-gram which spans these one-grams.\n",
    "        ngrams[gram_size].append(_Gram(g[0].begin, g[-1].end))\n",
    "\n",
    "  # Shuffle each list of n-grams.\n",
    "  for v in ngrams.values():\n",
    "    rng.shuffle(v)\n",
    "\n",
    "  # Create the weighting for n-gram length selection.\n",
    "  # Stored cummulatively for `random.choices` below.\n",
    "  cummulative_weights = list(\n",
    "      itertools.accumulate([1./n for n in range(1, max_ngram_size+1)]))\n",
    "\n",
    "  output_ngrams = []\n",
    "  # Keep a bitmask of which tokens have been masked.\n",
    "  masked_tokens = [False] * num_tokens\n",
    "  # Loop until we have enough masked tokens or there are no more candidate\n",
    "  # n-grams of any length.\n",
    "  # Each code path should ensure one or more elements from `ngrams` are removed\n",
    "  # to guarentee this loop terminates.\n",
    "  while (sum(masked_tokens) < max_masked_tokens and\n",
    "         sum(len(s) for s in ngrams.values())):\n",
    "    # Pick an n-gram size based on our weights.\n",
    "    sz = random.choices(range(1, max_ngram_size+1),\n",
    "                        cum_weights=cummulative_weights)[0]\n",
    "\n",
    "    # Ensure this size doesn't result in too many masked tokens.\n",
    "    # E.g., a two-gram contains _at least_ two tokens.\n",
    "    if sum(masked_tokens) + sz > max_masked_tokens:\n",
    "      # All n-grams of this length are too long and can be removed from\n",
    "      # consideration.\n",
    "      ngrams[sz].clear()\n",
    "      continue\n",
    "\n",
    "    # All of the n-grams of this size have been used.\n",
    "    if not ngrams[sz]:\n",
    "      continue\n",
    "\n",
    "    # Choose a random n-gram of the given size.\n",
    "    gram = ngrams[sz].pop()\n",
    "    num_gram_tokens = gram.end-gram.begin\n",
    "\n",
    "    # Check if this would add too many tokens.\n",
    "    if num_gram_tokens + sum(masked_tokens) > max_masked_tokens:\n",
    "      continue\n",
    "\n",
    "    # Check if any of the tokens in this gram have already been masked.\n",
    "    if sum(masked_tokens[gram.begin:gram.end]):\n",
    "      continue\n",
    "\n",
    "    # Found a usable n-gram!  Mark its tokens as masked and add it to return.\n",
    "    masked_tokens[gram.begin:gram.end] = [True] * (gram.end-gram.begin)\n",
    "    output_ngrams.append(gram)\n",
    "  return output_ngrams\n",
    "\n",
    "\n",
    "def _wordpieces_to_grams(tokens):\n",
    "  \"\"\"Reconstitue grams (words) from `tokens`.\n",
    "  E.g.,\n",
    "     tokens: ['[CLS]', 'That', 'lit', '##tle', 'blue', 'tru', '##ck', '[SEP]']\n",
    "      grams: [          [1,2), [2,         4),  [4,5) , [5,       6)]\n",
    "  Args:\n",
    "    tokens: list of wordpieces\n",
    "  Returns:\n",
    "    List of _Grams representing spans of whole words\n",
    "    (without \"[CLS]\" and \"[SEP]\").\n",
    "  \"\"\"\n",
    "  grams = []\n",
    "  gram_start_pos = None\n",
    "  for i, token in enumerate(tokens):\n",
    "    if gram_start_pos is not None and token.startswith(\"##\"):\n",
    "      continue\n",
    "    if gram_start_pos is not None:\n",
    "      grams.append(_Gram(gram_start_pos, i))\n",
    "    if token not in [\"[CLS]\", \"[SEP]\"]:\n",
    "      gram_start_pos = i\n",
    "    else:\n",
    "      gram_start_pos = None\n",
    "  if gram_start_pos is not None:\n",
    "    grams.append(_Gram(gram_start_pos, len(tokens)))\n",
    "  return grams\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng,\n",
    "                                 do_whole_word_mask,\n",
    "                                 max_ngram_size=None):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "  if do_whole_word_mask:\n",
    "    grams = _wordpieces_to_grams(tokens)\n",
    "  else:\n",
    "    # Here we consider each token to be a word to allow for sub-word masking.\n",
    "    if max_ngram_size:\n",
    "      raise ValueError(\"cannot use ngram masking without whole word masking\")\n",
    "    grams = [_Gram(i, i+1) for i in range(0, len(tokens))\n",
    "             if tokens[i] not in [\"[CLS]\", \"[SEP]\"]]\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "  # Generate masks.  If `max_ngram_size` in [0, None] it means we're doing\n",
    "  # whole word masking or token level masking.  Both of these can be treated\n",
    "  # as the `max_ngram_size=1` case.\n",
    "  masked_grams = _masking_ngrams(grams, max_ngram_size or 1,\n",
    "                                 num_to_predict, rng)\n",
    "  masked_lms = []\n",
    "  output_tokens = list(tokens)\n",
    "  for gram in masked_grams:\n",
    "    # 80% of the time, replace all n-gram tokens with [MASK]\n",
    "    if rng.random() < 0.8:\n",
    "      replacement_action = lambda idx: \"[MASK]\"\n",
    "    else:\n",
    "      # 10% of the time, keep all the original n-gram tokens.\n",
    "      if rng.random() < 0.5:\n",
    "        replacement_action = lambda idx: tokens[idx]\n",
    "      # 10% of the time, replace each n-gram token with a random word.\n",
    "      else:\n",
    "        replacement_action = lambda idx: rng.choice(vocab_words)\n",
    "\n",
    "    for idx in range(gram.begin, gram.end):\n",
    "      output_tokens[idx] = replacement_action(idx)\n",
    "      masked_lms.append(MaskedLmInstance(index=idx, label=tokens[idx]))\n",
    "\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150b393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87aba851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e0e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9534a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Wrote 9000 tfrecods\n",
      "INFO:absl:Wrote 10000 tfrecods\n",
      "INFO:absl:Wrote 11000 tfrecods\n",
      "INFO:absl:Wrote 12000 tfrecods\n",
      "INFO:absl:Wrote 13000 tfrecods\n",
      "INFO:absl:Wrote 14000 tfrecods\n",
      "INFO:absl:Wrote 15000 tfrecods\n",
      "INFO:absl:Wrote 16000 tfrecods\n",
      "INFO:absl:Wrote 17000 tfrecods\n",
      "INFO:absl:Wrote 18000 tfrecods\n",
      "INFO:absl:Wrote 19000 tfrecods\n",
      "INFO:absl:Wrote 20000 tfrecods\n",
      "INFO:absl:Wrote 21000 tfrecods\n",
      "INFO:absl:Wrote 22000 tfrecods\n",
      "INFO:absl:Wrote 23000 tfrecods\n",
      "INFO:absl:Wrote 24000 tfrecods\n",
      "INFO:absl:Wrote 25000 tfrecods\n",
      "INFO:absl:Wrote 26000 tfrecods\n",
      "INFO:absl:Wrote 27000 tfrecods\n",
      "INFO:absl:Wrote 28000 tfrecods\n",
      "INFO:absl:Wrote 29000 tfrecods\n",
      "INFO:absl:Wrote 30000 tfrecods\n",
      "INFO:absl:Wrote 31000 tfrecods\n",
      "INFO:absl:Wrote 32000 tfrecods\n",
      "INFO:absl:Wrote 33000 tfrecods\n",
      "INFO:absl:Wrote 34000 tfrecods\n",
      "INFO:absl:Wrote 35000 tfrecods\n",
      "INFO:absl:Wrote 36000 tfrecods\n",
      "INFO:absl:Wrote 37000 tfrecods\n",
      "INFO:absl:Wrote 38000 tfrecods\n",
      "INFO:absl:Wrote 39000 tfrecods\n",
      "INFO:absl:Wrote 40000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 40005\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "MIN_WORDS_IN_SENTENCE = 5\n",
    "def create_mlm_tfrecord():\n",
    "    counter = 0\n",
    "    for index, row in all_data.iterrows():\n",
    "        paragraph = row['review']\n",
    "        for text in paragraph.split('. '):\n",
    "            if text == '' or text == ' ' or text == None:\n",
    "                continue\n",
    "                \n",
    "            if len(text.split()) < MIN_WORDS_IN_SENTENCE:\n",
    "                continue\n",
    "\n",
    "            # slice off if exceeds tha max_seq_length\n",
    "            tokens = tokenizer.tokenize(text)[:max_seq_length-2] \n",
    "            (tokens, masked_lm_positions,\n",
    "                     masked_lm_labels) = create_masked_lm_predictions(\n",
    "                         tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng,\n",
    "                         do_whole_word_mask, max_ngram_size)\n",
    "\n",
    "            tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n",
    "            input_ids =  tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "            # pos + 1 for accounting shifting by CLS token\n",
    "            masked_lm_positions = [pos+1 for pos in masked_lm_positions]\n",
    "            masked_lm_labels = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "            result = {}\n",
    "            result['input_ids'] = input_ids\n",
    "            result['input_type_ids'] = input_type_ids\n",
    "            result['input_mask'] = input_mask\n",
    "\n",
    "            result['masked_lm_positions'] = masked_lm_positions\n",
    "            result['masked_lm_labels'] = masked_lm_labels\n",
    "            \n",
    "            counter += 1\n",
    "            yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {'input_ids': (\"var_len\", \"int\"), \n",
    "         'input_mask': (\"var_len\", \"int\"), \n",
    "         'input_type_ids': (\"var_len\", \"int\"), \n",
    "         'masked_lm_positions': (\"var_len\", \"int\"),\n",
    "         'masked_lm_labels': (\"var_len\", \"int\") \n",
    "         }\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/squad/bert_mlm/train'\n",
    "tfrecord_filename = 'imdb'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=create_mlm_tfrecord())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e95e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_mask', 'input_type_ids', 'masked_lm_positions']\n",
    "y_keys = ['masked_lm_labels']\n",
    "batch_size = 64\n",
    "\n",
    "padded_values = {'masked_lm_psitions': tf.constant(tokenizer.pad_token_id), \n",
    "                'masked_lm_labels': tf.constant(tokenizer.pad_token_id)}\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size,\n",
    "                                    padded_values=padded_values,\n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bc3af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 120), dtype=int32, numpy=\n",
      "array([[  101,  6853,   103, ...,     0,     0,     0],\n",
      "       [  101,  4978,  2499, ...,     0,     0,     0],\n",
      "       [  101,  1337,  1123, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  2119,  5819, ...,     0,     0,     0],\n",
      "       [  101,  1327,   170, ...,     0,     0,     0],\n",
      "       [  101, 16752, 21089, ...,   119,   119,   102]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(32, 120), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(32, 120), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'masked_lm_positions': <tf.Tensor: shape=(32, 18), dtype=int32, numpy=\n",
      "array([[  2,  15,  17,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  6,  10,  11,  15,  22,  26,  29,  33,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  9,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  2,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [ 11,  14,  16,  18,  25,  26,  34,  54,  56,  59,  61,  72,  89,\n",
      "         96, 101, 105,   0,   0],\n",
      "       [  4,   7,  15,  33,  34,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  9,  12,  14,  31,  34,  38,  40,  42,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  1,  10,  12,  13,  14,  27,  44,  52,  53,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  9,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [ 11,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  1,   2,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  5,   9,  19,  27,  35,  37,  43,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  9,  10,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [ 10,  12,  17,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  6,   9,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  1,  18,  20,  29,  30,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  1,   5,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  3,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  2,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  2,  13,  22,  25,  32,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [ 11,  24,  28,  32,  35,  39,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  1,   4,   7,  13,  20,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  3,  10,  12,  21,  33,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  3,   7,  13,  27,  31,  34,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  3,   4,  13,  18,  25,  26,  41,  47,  62,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  7,  19,  23,  24,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  7,   8,   9,  18,  31,  34,  35,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  8,  13,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  5,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0],\n",
      "       [  5,  10,  16,  21,  22,  24,  27,  31,  35,  36,  52,  54,  55,\n",
      "         70,  78,  89, 109, 115]], dtype=int32)>} {'masked_lm_labels': <tf.Tensor: shape=(32, 18), dtype=int32, numpy=\n",
      "array([[ 1240,  1136,  1106,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  117,  1114,   170, 20975,  1106,  1117,  2368, 13559,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 2641,  5750,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1122,  5367,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1114, 25470,  2349, 10401,   170,  1992,  1105,  2570,  8702,\n",
      "        23878,  1113,  7831,  1402,  1105,  9052,   117,     0,     0],\n",
      "       [ 2606,  1141,  1132,  1107,  1115,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 2627, 16387,   132,  1641, 17294,  2168,  4390,  3703,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1135,  1129,  1361,  1183,   117,   135,  1103,  1150,  1108,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 7068,   119,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 4452,   119,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 4081,  4207,  5148,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  159,  1105,  1103,   119,  7549, 12118,  1178,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 2741,  1120,  1155,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1106,  1105,  1254,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1108,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  117,  3319,  1180,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1409,  5996,  1267,  1103, 10893,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 2809,   112,   170,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1486,  1164,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [11713,  1122,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1110,   117,  1134,  1842,   107,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  133, 21359,  4954,  1374,  1122, 23895,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 8223,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  146,  8868,  2432, 11612,   133,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  188, 23665, 12266, 22766,  1157,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  131,  1103,  1594,   135, 26465,  2463,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1191,  1128,   112,  1103,  2111,  1272, 15486, 16879,   119,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1184, 13084,  1152,  1321,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [  107,  1150,  1125,   112,  1116,  1354,   146,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1642,   117,  1103,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 1111,   118,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       [ 5528,  1105, 10795,   117,  1195,  1140,  3271,   112,  1525,\n",
      "          170,  2093,  2520,  3676,   188,  1106,  1105,  1243,  1111]],\n",
      "      dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for batch_inputs, batch_labels in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c8fa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 18), dtype=int32, numpy=\n",
       "array([[  2,  15,  17,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  6,  10,  11,  15,  22,  26,  29,  33,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  9,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  2,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [ 11,  14,  16,  18,  25,  26,  34,  54,  56,  59,  61,  72,  89,\n",
       "         96, 101, 105,   0,   0],\n",
       "       [  4,   7,  15,  33,  34,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  9,  12,  14,  31,  34,  38,  40,  42,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  1,  10,  12,  13,  14,  27,  44,  52,  53,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  9,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [ 11,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  1,   2,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  5,   9,  19,  27,  35,  37,  43,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  9,  10,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [ 10,  12,  17,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  6,   9,  10,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  1,  18,  20,  29,  30,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  1,   5,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  3,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  2,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  2,  13,  22,  25,  32,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [ 11,  24,  28,  32,  35,  39,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  1,   4,   7,  13,  20,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  3,  10,  12,  21,  33,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  3,   7,  13,  27,  31,  34,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  3,   4,  13,  18,  25,  26,  41,  47,  62,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  7,  19,  23,  24,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  7,   8,   9,  18,  31,  34,  35,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  8,  13,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  5,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0],\n",
       "       [  5,  10,  16,  21,  22,  24,  27,  31,  35,  36,  52,  54,  55,\n",
       "         70,  78,  89, 109, 115]], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['masked_lm_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78728f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 18), dtype=int32, numpy=\n",
       "array([[ 1240,  1136,  1106,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  117,  1114,   170, 20975,  1106,  1117,  2368, 13559,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 2641,  5750,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1122,  5367,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1114, 25470,  2349, 10401,   170,  1992,  1105,  2570,  8702,\n",
       "        23878,  1113,  7831,  1402,  1105,  9052,   117,     0,     0],\n",
       "       [ 2606,  1141,  1132,  1107,  1115,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 2627, 16387,   132,  1641, 17294,  2168,  4390,  3703,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1135,  1129,  1361,  1183,   117,   135,  1103,  1150,  1108,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 7068,   119,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 4452,   119,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 4081,  4207,  5148,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  159,  1105,  1103,   119,  7549, 12118,  1178,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 2741,  1120,  1155,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1106,  1105,  1254,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1108,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  117,  3319,  1180,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1409,  5996,  1267,  1103, 10893,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 2809,   112,   170,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1486,  1164,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [11713,  1122,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1110,   117,  1134,  1842,   107,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  133, 21359,  4954,  1374,  1122, 23895,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 8223,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  146,  8868,  2432, 11612,   133,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  188, 23665, 12266, 22766,  1157,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  131,  1103,  1594,   135, 26465,  2463,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1191,  1128,   112,  1103,  2111,  1272, 15486, 16879,   119,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1184, 13084,  1152,  1321,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  107,  1150,  1125,   112,  1116,  1354,   146,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1642,   117,  1103,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 1111,   118,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5528,  1105, 10795,   117,  1195,  1140,  3271,   112,  1525,\n",
       "          170,  2093,  2520,  3676,   188,  1106,  1105,  1243,  1111]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels['masked_lm_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baa4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3932c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 11 14:31:25 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    58W / 300W |  30587MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b30b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66f479aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.models import BERTEncoder\n",
    "\n",
    "config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"intermediate_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"embedding_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"attention_head_size\": 64,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"vocab_size\": 28996,\n",
    "    \"layer_norm_epsilon\": 1e-12,\n",
    "    \"mask_mode\": \"user_defined\",\n",
    "}\n",
    "\n",
    "model = BERTEncoder(config=config, use_masked_lm_positions=True)\n",
    "model = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97fc5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_checkpoint(\"/tmp/tf_transformers_cache/bert-base-cased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ec4d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.losses import cross_entropy_loss\n",
    "\n",
    "def joint_mlm_loss_fn(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for i, layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        layer_loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                       logits=layer_output, \n",
    "                                       label_weights=y_true_dict['masked_lm_mask'])\n",
    "        loss_dict['layer_{}'.format(i)] = layer_loss\n",
    "        loss_holder.append(layer_loss)\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder)\n",
    "    return loss_dict\n",
    "\n",
    "\n",
    "def mlm_loss_fn(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    masked_lm_mask = tf.cast(tf.not_equal(y_true_dict['masked_lm_labels'], tokenizer.pad_token_id),\n",
    "                            y_pred_dict['token_logits'].dtype)\n",
    "    \n",
    "    loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                       logits=y_pred_dict['token_logits'], \n",
    "                                       label_weights=masked_lm_mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccf47a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTextGenerator(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tokenizer, top_k=5):\n",
    "        self.top_k = top_k\n",
    "        self.tokenizer = tokenizer\n",
    "        model = BERTEncoder(config=config)\n",
    "        model = model.get_model()\n",
    "        self.original_model = model\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.original_model.set_weights(self.model.get_weights())\n",
    "        sample_text = \"I have watched this [MASK] and it was awesome\"\n",
    "\n",
    "        input_ids = tf.constant(self.tokenizer.encode(sample_text))\n",
    "\n",
    "        masked_index = np.where(input_ids == self.tokenizer.mask_token_id)[0][0]\n",
    "        input_ids = tf.expand_dims(input_ids, axis=0)\n",
    "        input_type_ids = tf.zeros_like(input_ids)\n",
    "        input_mask     = tf.ones_like(input_ids)\n",
    "        inputs = {}\n",
    "        inputs[\"input_ids\"] = input_ids\n",
    "        inputs[\"input_type_ids\"] = input_type_ids\n",
    "        inputs[\"input_mask\"] = input_mask\n",
    "        outputs = self.original_model(inputs)\n",
    "        top_k_probs, top_k_indices = tf.nn.top_k(outputs['token_logits'][:, masked_index, :], k = self.top_k)\n",
    "        top_k_probs = top_k_probs[0]\n",
    "        top_k_indices = top_k_indices[0]\n",
    "        for i in range(self.top_k):\n",
    "            print(\"top {} , {} , {}\".format(i, top_k_probs[i], tokenizer.decode([top_k_indices[i]])))\n",
    "            \n",
    "        #for i, layer_output in enumerate(outputs['all_layer_token_logits']):\n",
    "        #  prob_value = tf.reduce_max(layer_output, axis=-1)[0][masked_index]\n",
    "        #  predicted_token = self.tokenizer.decode([tf.argmax(layer_output, axis=-1)[0][masked_index]])\n",
    "        #  print(\"Layer {}, {}, {}\".format(i, predicted_token, prob_value))\n",
    "\n",
    "\n",
    "generator_callback = MaskedTextGenerator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.core import optimization\n",
    "train_data_size = 40000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 5\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(EPOCHS * train_data_size * 0.1 / batch_size)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)\n",
    "model.compile2(optimizer=optimizer, custom_loss={'token_logits': mlm_loss_fn}, loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5667d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "top 0 , 2.2106940746307373 , sets\n",
      "top 1 , 1.8485941886901855 , A2\n",
      "top 2 , 1.719996690750122 , MGM\n",
      "top 3 , 1.712893009185791 , filters\n",
      "top 4 , 1.7128450870513916 , ##6th\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/bert/pooler_transform/kernel:0', 'tf_transformers/bert/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/bert/pooler_transform/kernel:0', 'tf_transformers/bert/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/bert/pooler_transform/kernel:0', 'tf_transformers/bert/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/bert/pooler_transform/kernel:0', 'tf_transformers/bert/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 349s 267ms/step - loss: 7.1914 - token_logits_loss: 7.1914\n",
      "Epoch 2/5\n",
      "top 0 , 3.6591522693634033 , .\n",
      "top 1 , 3.54352068901062 , the\n",
      "top 2 , 3.472245931625366 , ,\n",
      "top 3 , 2.9732494354248047 , a\n",
      "top 4 , 2.895054817199707 , '\n",
      "1250/1250 [==============================] - 338s 268ms/step - loss: 6.8074 - token_logits_loss: 6.8074\n",
      "Epoch 3/5\n",
      "top 0 , 4.1321258544921875 , .\n",
      "top 1 , 3.961129665374756 , the\n",
      "top 2 , 3.7622623443603516 , ,\n",
      "top 3 , 3.263580799102783 , a\n",
      "top 4 , 3.199124574661255 , of\n",
      "1250/1250 [==============================] - 338s 268ms/step - loss: 6.7807 - token_logits_loss: 6.7807\n",
      "Epoch 4/5\n",
      "top 0 , 4.193861961364746 , .\n",
      "top 1 , 3.902042865753174 , the\n",
      "top 2 , 3.817805290222168 , ,\n",
      "top 3 , 3.376988649368286 , a\n",
      "top 4 , 3.255675792694092 , to\n",
      "1250/1250 [==============================] - 338s 268ms/step - loss: 6.7790 - token_logits_loss: 6.7790\n",
      "Epoch 5/5\n",
      "top 0 , 4.476137161254883 , .\n",
      "top 1 , 4.145719528198242 , the\n",
      "top 2 , 4.076451778411865 , ,\n",
      "top 3 , 3.651524543762207 , and\n",
      "top 4 , 3.587021589279175 , to\n",
      " 849/1250 [===================>..........] - ETA: 1:47 - loss: 6.7704 - token_logits_loss: 6.7704"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3854c3db05ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(train_dataset, \n\u001b[0m\u001b[1;32m      3\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    callbacks=[generator_callback])\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "history = model.fit(train_dataset, \n",
    "                   epochs=EPOCHS, \n",
    "                   callbacks=[generator_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c279f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = generator_callback.original_model\n",
    "model2.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bf3ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs_copy = batch_inputs.copy()\n",
    "del batch_inputs_copy['masked_lm_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2d10096",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "857731e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = model2(batch_inputs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe7855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bb9654c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 28996), dtype=float32, numpy=\n",
       "array([[-9.598297, -9.752734, -9.611607, ..., -9.480362, -9.760493,\n",
       "        -9.632543],\n",
       "       [-9.598297, -9.752733, -9.611607, ..., -9.480361, -9.760493,\n",
       "        -9.632543],\n",
       "       [-9.598297, -9.752733, -9.611607, ..., -9.480361, -9.760493,\n",
       "        -9.632543],\n",
       "       ...,\n",
       "       [-9.598297, -9.752733, -9.611607, ..., -9.480362, -9.760493,\n",
       "        -9.632543],\n",
       "       [-9.598297, -9.752733, -9.611607, ..., -9.480362, -9.760493,\n",
       "        -9.632543],\n",
       "       [-9.598297, -9.752733, -9.611607, ..., -9.480362, -9.760493,\n",
       "        -9.632543]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['token_logits'][0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea44ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-9.598299  -9.752737  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299  -9.752736  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299  -9.752736  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752737 -9.611609 ... -9.480363 -9.760494 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598298  -9.752735  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299  -9.752734  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752737 -9.611609 ... -9.480364 -9.760495 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752736 -9.611609 ... -9.480363 -9.760494 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752736 -9.611609 ... -9.480364 -9.760495 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752735 -9.611609 ... -9.480364 -9.760496 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752736 -9.611609 ... -9.480364 -9.760495 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752737 -9.611609 ... -9.480363 -9.760494 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299  -9.752736  -9.6116085 ... -9.480363  -9.760494  -9.632545 ], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752736 -9.611609 ... -9.480364 -9.760496 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598298  -9.752734  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299  -9.752736  -9.6116085 ... -9.480363  -9.760494  -9.632545 ], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752735 -9.611609 ... -9.480364 -9.760495 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598299 -9.752735 -9.611609 ... -9.480363 -9.760494 -9.632545], shape=(28996,), dtype=float32)\n",
      "tf.Tensor([-9.598298  -9.752735  -9.6116085 ... -9.480363  -9.760494  -9.6325445], shape=(28996,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for index in batch_inputs['masked_lm_positions'][0]:\n",
    "    print(outputs2['token_logits'][0, :, :][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "389c057a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 28996), dtype=float32, numpy=\n",
       "array([[-9.598299 , -9.752736 , -9.6116085, ..., -9.480363 , -9.760494 ,\n",
       "        -9.6325445],\n",
       "       [-9.598299 , -9.752737 , -9.611609 , ..., -9.480364 , -9.760494 ,\n",
       "        -9.632545 ],\n",
       "       [-9.598299 , -9.752736 , -9.611609 , ..., -9.480363 , -9.760495 ,\n",
       "        -9.632545 ],\n",
       "       ...,\n",
       "       [-9.598299 , -9.752737 , -9.611609 , ..., -9.480364 , -9.760496 ,\n",
       "        -9.632545 ],\n",
       "       [-9.598299 , -9.752736 , -9.6116085, ..., -9.480363 , -9.760494 ,\n",
       "        -9.6325445],\n",
       "       [-9.598298 , -9.752735 , -9.6116085, ..., -9.480363 , -9.760494 ,\n",
       "        -9.6325445]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114ecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
