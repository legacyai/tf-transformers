{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2946d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddda4c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.do_lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f59411d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'bert_tokenizer_dir/vocab.txt'\n",
    "def _create_vocab_table_and_initializer(vocab_file):\n",
    "    vocab_initializer = tf.lookup.TextFileInitializer(\n",
    "        vocab_file,\n",
    "        key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "        value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "    vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=-1)\n",
    "    return vocab_table, vocab_initializer\n",
    "\n",
    "vocab_table , vocab_initializer = _create_vocab_table_and_initializer(vocab_file)\n",
    "bert_tokenizer = text.BertTokenizer(\n",
    "        vocab_table, lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db23f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc394f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Hello how are you', \n",
    "       'Plants are beautiful :-)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2897b8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8667, 1293, 1132, 1128], [25880, 1132, 2712, 131, 118, 114]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text, add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec1428d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
       "array([[ 8667,  1293,  1132,  1128,     0,     0],\n",
       "       [25880,  1132,  2712,   131,   118,   114]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bert_tokenizer.tokenize(tf.identity(text))\n",
    "enc.merge_dims(-2, -1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6613330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map(text):\n",
    "    enc = bert_tokenizer.tokenize(tf.identity(text))\n",
    "    return enc.merge_dims(-2, -1).to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd560d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69da2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e9a6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello how are you', shape=(), dtype=string)\n",
      "--------\n",
      "tf.Tensor(b'Plants are beautiful :-)', shape=(), dtype=string)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fede904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapped = dataset.map(tokenize_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08efc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[8667 1293 1132 1128]], shape=(1, 4), dtype=int64)\n",
      "--------\n",
      "tf.Tensor([[25880  1132  2712   131   118   114]], shape=(1, 6), dtype=int64)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_mapped:\n",
    "    print(item)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cf516b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    # inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cbcfbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(encoded_text):\n",
    "    # Input to `augment()` is a TensorFlow tensor which\n",
    "    # is not supported by `imgaug`. This is why we first\n",
    "    # convert it to its `numpy` variant.\n",
    "    return get_masked_input_and_labels(encoded_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "592dd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapped_new = dataset_mapped.map(\n",
    "        lambda x: tf.py_function(augment, [x], [tf.int32, tf.int32, tf.float32])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2db9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 103, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 0., 0., 0.]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,    69,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 1., 0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_mapped_new:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c49ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a276de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_masking(text):\n",
    "    encoded_text = tokenize_map(text)\n",
    "    return tf.py_function(augment, [encoded_text], [tf.int32, tf.int32, tf.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ac23f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset.map(dynamic_masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56205b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset_new.repeat(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7275237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667,  103, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 1., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,    56,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 1., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,    57,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 0., 1., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,    54,   131,    29,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 1., 0., 1., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   103]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   103]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 1.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667,  103, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 1., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   103,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 1., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 103,  103, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[8667, 1293, 1132, 1128]], dtype=int32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 1., 0., 0.]], dtype=float32)>)\n",
      "--------------\n",
      "(<tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[25880,  1132,  2712,   131,   118,   114]], dtype=int32)>, <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_new:\n",
    "    print(item)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(tf.keras.layers.Layer):\n",
    "  \"\"\"Wraps BertTokenizer with pre-defined vocab as a Keras Layer.\n",
    "  Attributes:\n",
    "    tokenize_with_offsets: If true, calls\n",
    "      `text.BertTokenizer.tokenize_with_offsets()` instead of plain\n",
    "      `text.BertTokenizer.tokenize()` and outputs a triple of\n",
    "      `(tokens, start_offsets, limit_offsets)`.\n",
    "    raw_table_access: An object with methods `.lookup(keys) and `.size()`\n",
    "      that operate on the raw lookup table of tokens. It can be used to\n",
    "      look up special token synbols like `[MASK]`.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, *,\n",
    "               vocab_file: str,\n",
    "               lower_case: bool,\n",
    "               tokenize_with_offsets: bool = False,\n",
    "               **kwargs):\n",
    "    \"\"\"Initialize a `BertTokenizer` layer.\n",
    "    Args:\n",
    "      vocab_file: A Python string with the path of the vocabulary file.\n",
    "        This is a text file with newline-separated wordpiece tokens.\n",
    "        This layer initializes a lookup table from it that gets used with\n",
    "        `text.BertTokenizer`.\n",
    "      lower_case: A Python boolean forwarded to `text.BertTokenizer`.\n",
    "        If true, input text is converted to lower case (where applicable)\n",
    "        before tokenization. This must be set to match the way in which\n",
    "        the `vocab_file` was created.\n",
    "      tokenize_with_offsets: A Python boolean. If true, this layer calls\n",
    "        `text.BertTokenizer.tokenize_with_offsets()` instead of plain\n",
    "        `text.BertTokenizer.tokenize()` and outputs a triple of\n",
    "        `(tokens, start_offsets, limit_offsets)`\n",
    "        insead of just tokens.\n",
    "      **kwargs: Standard arguments to `Layer()`.\n",
    "    Raises:\n",
    "      ImportError: If importing `tensorflow_text` failed.\n",
    "    \"\"\"\n",
    "    _check_if_tf_text_installed()\n",
    "\n",
    "    self.tokenize_with_offsets = tokenize_with_offsets\n",
    "    # TODO(b/177326279): Stop storing the vocab table initializer as an\n",
    "    # attribute when https://github.com/tensorflow/tensorflow/issues/46456\n",
    "    # has been fixed in the TensorFlow versions of the TF Hub users that load\n",
    "    # a SavedModel created from this layer. Due to that issue, loading such a\n",
    "    # SavedModel forgets to add .vocab_table._initializer as a trackable\n",
    "    # dependency of .vocab_table, so that saving it again to a second SavedModel\n",
    "    # (e.g., the final model built using TF Hub) does not properly track\n",
    "    # the ._vocab_table._initializer._filename as an Asset.\n",
    "    self._vocab_table, self._vocab_initializer_donotuse = (\n",
    "        self._create_vocab_table_and_initializer(vocab_file))\n",
    "    self._special_tokens_dict = self._create_special_tokens_dict(\n",
    "        self._vocab_table, vocab_file)\n",
    "    super().__init__(**kwargs)\n",
    "    self._bert_tokenizer = text.BertTokenizer(\n",
    "        self._vocab_table, lower_case=lower_case)\n",
    "\n",
    "  @property\n",
    "  def vocab_size(self):\n",
    "    return self._vocab_table.size()\n",
    "\n",
    "  def _create_vocab_table_and_initializer(self, vocab_file):\n",
    "    vocab_initializer = tf.lookup.TextFileInitializer(\n",
    "        vocab_file,\n",
    "        key_dtype=tf.string, key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "        value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER)\n",
    "    vocab_table = tf.lookup.StaticHashTable(vocab_initializer, default_value=-1)\n",
    "    return vocab_table, vocab_initializer\n",
    "\n",
    "  def call(self, inputs: tf.Tensor):\n",
    "    \"\"\"Calls `text.BertTokenizer` on inputs.\n",
    "    Args:\n",
    "      inputs: A string Tensor of shape `(batch_size,)`.\n",
    "    Returns:\n",
    "      One or three of `RaggedTensors` if `tokenize_with_offsets` is False or\n",
    "      True, respectively. These are\n",
    "        tokens: A `RaggedTensor` of shape\n",
    "          `[batch_size, (words), (pieces_per_word)]`\n",
    "          and type int32. `tokens[i,j,k]` contains the k-th wordpiece of the\n",
    "          j-th word in the i-th input.\n",
    "        start_offsets, limit_offsets: If `tokenize_with_offsets` is True,\n",
    "          RaggedTensors of type int64 with the same indices as tokens.\n",
    "          Element `[i,j,k]` contains the byte offset at the start, or past the\n",
    "          end, resp., for the k-th wordpiece of the j-th word in the i-th input.\n",
    "    \"\"\"\n",
    "    # Prepare to reshape the result to work around broken shape inference.\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    def _reshape(rt):\n",
    "      values = rt.values\n",
    "      row_splits = rt.row_splits\n",
    "      row_splits = tf.reshape(row_splits, [batch_size + 1])\n",
    "      return tf.RaggedTensor.from_row_splits(values, row_splits)\n",
    "\n",
    "    # Call the tokenizer.\n",
    "    if self.tokenize_with_offsets:\n",
    "      tokens, start_offsets, limit_offsets = (\n",
    "          self._bert_tokenizer.tokenize_with_offsets(inputs))\n",
    "      tokens = tf.cast(tokens, dtype=tf.int32)\n",
    "      return _reshape(tokens), _reshape(start_offsets), _reshape(limit_offsets)\n",
    "    else:\n",
    "      tokens = self._bert_tokenizer.tokenize(inputs)\n",
    "      tokens = tf.cast(tokens, dtype=tf.int32)\n",
    "      return _reshape(tokens)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Skip in tf.saved_model.save(); fail if called direcly.\n",
    "    raise NotImplementedError(\"TODO(b/170480226): implement\")\n",
    "\n",
    "  def get_special_tokens_dict(self):\n",
    "    \"\"\"Returns dict of token ids, keyed by standard names for their purpose.\n",
    "    Returns:\n",
    "      A dict from Python strings to Python integers. Each key is a standard\n",
    "      name for a special token describing its use. (For example, \"padding_id\"\n",
    "      is what BERT traditionally calls \"[PAD]\" but others may call \"<pad>\".)\n",
    "      The corresponding value is the integer token id. If a special token\n",
    "      is not found, its entry is omitted from the dict.\n",
    "      The supported keys and tokens are:\n",
    "        * start_of_sequence_id: looked up from \"[CLS]\"\n",
    "        * end_of_segment_id: looked up from \"[SEP]\"\n",
    "        * padding_id: looked up form \"[PAD]\"\n",
    "        * mask_id: looked up from \"[MASK]\"\n",
    "        * vocab_size: one past the largest token id used\n",
    "    \"\"\"\n",
    "    return self._special_tokens_dict\n",
    "\n",
    "  def _create_special_tokens_dict(self, vocab_table, vocab_file):\n",
    "    special_tokens = dict(start_of_sequence_id=\"[CLS]\",\n",
    "                          end_of_segment_id=\"[SEP]\",\n",
    "                          padding_id=\"[PAD]\",\n",
    "                          mask_id=\"[MASK]\")\n",
    "    with tf.init_scope():\n",
    "      if tf.executing_eagerly():\n",
    "        special_token_ids = vocab_table.lookup(\n",
    "            tf.constant(list(special_tokens.values()), tf.string))\n",
    "        vocab_size = vocab_table.size()\n",
    "      else:\n",
    "        # A blast from the past: non-eager init context while building Model.\n",
    "        # This can happen with Estimator or tf.compat.v1.disable_v2_behavior().\n",
    "        logging.warning(\n",
    "            \"Non-eager init context; computing \"\n",
    "            \"BertTokenizer's special_tokens_dict in tf.compat.v1.Session\")\n",
    "        with tf.Graph().as_default():\n",
    "          local_vocab_table, _ = self._create_vocab_table_and_initializer(\n",
    "              vocab_file)\n",
    "          special_token_ids_tensor = local_vocab_table.lookup(\n",
    "              tf.constant(list(special_tokens.values()), tf.string))\n",
    "          vocab_size_tensor = local_vocab_table.size()\n",
    "          init_ops = [tf.compat.v1.initialize_all_tables()]\n",
    "          with tf.compat.v1.Session() as sess:\n",
    "            sess.run(init_ops)\n",
    "            special_token_ids, vocab_size = sess.run(\n",
    "                [special_token_ids_tensor, vocab_size_tensor])\n",
    "      result = dict(\n",
    "          vocab_size=int(vocab_size)  # Numpy to Python.\n",
    "      )\n",
    "      for k, v in zip(special_tokens, special_token_ids):\n",
    "        v = int(v)\n",
    "        if v >= 0:\n",
    "          result[k] = v\n",
    "        else:\n",
    "          logging.warning(\"Could not find %s as token \\\"%s\\\" in vocab file %s\",\n",
    "                          k, special_tokens[k], vocab_file)\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
