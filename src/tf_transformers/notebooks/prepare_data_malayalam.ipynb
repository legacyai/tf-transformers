{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668274c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def hf_dump_chars_to_textfile(file, dataset, data_keys, max_char=-1):\n",
    "    \"\"\"Write part of a TFDS sentence dataset to lines in a text file.\n",
    "\n",
    "  Args:\n",
    "    dataset: tf.dataset containing string-data.\n",
    "    data_keys: what keys in dataset to dump from.\n",
    "    max_char: max character to dump to text file.\n",
    "\n",
    "  Returns:\n",
    "    name of temp file with dataset bytes, exact number of characters dumped.\n",
    "  \"\"\"\n",
    "    line_count = 0\n",
    "    with open(file, \"a+\") as outfp:\n",
    "        char_count = 0\n",
    "        for example in tqdm.tqdm(dataset):\n",
    "            for k in data_keys:\n",
    "                line = example[k]\n",
    "                line = line + \"\\n\"\n",
    "                char_count += len(line)\n",
    "                line_count += 1\n",
    "                outfp.write(line)\n",
    "\n",
    "    print(\"Total lines {}, chars {}\".format(line_count, char_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea06064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c19d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mc4\", \"ml\")\n",
    "dataset_oscar = load_dataset(\"oscar\", \"unshuffled_deduplicated_ml\")\n",
    "\n",
    "# Write to text\n",
    "OUT_FILE = '/home/sidhu/Datasets/ml4_ml.txt'\n",
    "hf_dump_chars_to_textfile(OUT_FILE, \n",
    "                          dataset[\"train\"],\n",
    "                          (\"text\",))\n",
    "\n",
    "OUT_FILE = '/home/sidhu/Datasets/oscar_ml.txt'\n",
    "hf_dump_chars_to_textfile(OUT_FILE, \n",
    "                          dataset_oscar[\"train\"],\n",
    "                          (\"text\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f83ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72149175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentencepice\n",
    "\n",
    "### Train Sentencepiece tokenizer (Albert)\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=['/home/sidhu/Datasets/ml4_ml.txt', '/home/sidhu/Datasets/oscar_ml.txt'],\n",
    "                               model_prefix='malayalam',\n",
    "                               vocab_size=30000,\n",
    "                               pad_id=0,\n",
    "                               unk_id=1,\n",
    "                               bos_id=-1,\n",
    "                               user_defined_symbols=['(', ')', '\"', '-', '.', '–', '£', '€'],\n",
    "                               control_symbols=['[CLS]','[SEP]','[MASK]'],\n",
    "                               shuffle_input_sentence=True,\n",
    "                               input_sentence_size=10000000,\n",
    "                               character_coverage=0.99995,\n",
    "                               model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffe939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import tqdm, tempfile, glob, os    \n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd045cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb00d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_text_tokenizer(model_file_path):\n",
    "    \n",
    "    def _create_tokenizer(model_serialized_proto, dtype, nbest_size, alpha):\n",
    "        return tf_text.SentencepieceTokenizer(\n",
    "            model=model_serialized_proto,\n",
    "            out_type=dtype,\n",
    "            nbest_size=nbest_size,\n",
    "            alpha=alpha)\n",
    "    \n",
    "    dtype = tf.int32\n",
    "    nbest_size = 0\n",
    "    alpha = 1.0\n",
    "    \n",
    "    model_serialized_proto = tf.io.gfile.GFile(model_file_path,\n",
    "                                                           \"rb\").read()\n",
    "\n",
    "    tokenizer_sp = _create_tokenizer(model_serialized_proto, \n",
    "                                 dtype,\n",
    "                                 nbest_size,\n",
    "                                 alpha)\n",
    "    \n",
    "    return tokenizer_sp\n",
    "\n",
    "\n",
    "def text_normalize(line):\n",
    "    \"\"\"Exclude empty string\"\"\"\n",
    "    line = tf.strings.strip(line)\n",
    "    return tf.not_equal(tf.strings.length(line),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf8e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd066166",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sp = get_tf_text_tokenizer(\"/home/sidhu/Projects/vocab/malayalam.model\")\n",
    "\n",
    "DATA_BATCH_SIZE = 1024\n",
    "\n",
    "all_files = ['/home/sidhu/Datasets/ml4_ml.txt', '/home/sidhu/Datasets/oscar_ml.txt']\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '/home/sidhu/Datasets/TFRECORD_malayalam'\n",
    "tfrecord_filename = 'c4'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    n_files=10,\n",
    "                    overwrite=True\n",
    "                    )\n",
    "    \n",
    "dataset = tf.data.TextLineDataset(all_files)\n",
    "dataset = dataset.filter(text_normalize)\n",
    "dataset = dataset.apply(tf.data.experimental.unique())\n",
    "dataset = dataset.batch(DATA_BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "def parse_train():\n",
    "    for batch_input in tqdm.tqdm(dataset):\n",
    "        batch_tokenized = tokenizer_sp.tokenize(batch_input).merge_dims(-1,1).to_list()\n",
    "        for input_ids in batch_tokenized:\n",
    "\n",
    "            yield {\"input_ids\": input_ids}\n",
    "# Process\n",
    "tfwriter.process(parse_fn=parse_train())\n",
    "\n",
    "# INFO:absl:Total individual observations/examples written is 31537867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e664cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Validation sentences\n",
    "\n",
    "validation_sentences = \"\"\"എനിക്ക് നിന്നെ വളരെ \n",
    "എഴുത്ത് ഉപകരണങ്ങൾ വെബിൽ എവിടേയും നിങ്ങൾ തിരഞ്ഞെടുക്കുന്ന ഭാഷയിൽ ടൈപ്പുചെയ്യുന്നതിനെ\n",
    "സച്ചിന്റെ ബാറ്റിംഗ് എല്ലാവർക്കും \n",
    "എല്ലാവരും വോട്ട് \n",
    "ഞാൻ കളിക്കാൻ \n",
    "മമ്മൂട്ടി ഇന്നലെ അവിടെ \n",
    "മയിൽ ഒരു മനോഹര \n",
    "ചന്ദ്രൻ ഇന്ന്\"\"\".split(\"\\n\") \n",
    "\n",
    "validation_encoded = tf.ragged.constant([[472, 3031, 305], [8580, 14521, 489, 375, 52, 18853, 24, 872, 2509, 106, 14555, 23787, 13839, 12571], [5860, 42, 20550, 8057], [1306, 2672], [212, 21227], [2195, 1014, 409], [62, 73, 21, 4386], [20465, 299]])\n",
    "\n",
    "validation_encoded = tf_text.combine_segments(\n",
    "              validation_encoded,\n",
    "              start_of_sequence_id=3,\n",
    "              end_of_segment_id=5) # Add mask\n",
    "\n",
    "validation_encoded = validation_encoded[0]\n",
    "validation_encoded = tf_text.combine_segments(\n",
    "              [validation_encoded],\n",
    "              start_of_sequence_id=3,\n",
    "              end_of_segment_id=4) # Add mask\n",
    "validation_encoded=validation_encoded[0]\n",
    "validation_encoded.to_tensor()[:, 1:]\n",
    "\n",
    "validation_encoded = tf.constant([[    3,   472,  3031,   305,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  8580, 14521,   489,   375,    52, 18853,    24,   872,\n",
    "         2509,   106, 14555, 23787, 13839, 12571,     5,     4],\n",
    "       [    3,  5860,    42, 20550,  8057,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  1306,  2672,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,   212, 21227,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  2195,  1014,   409,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,    62,    73,    21,  4386,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3, 20465,   299,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777517a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from tf_transformers.core import TPUTrainer\n",
    "from tf_transformers.data import TFReader\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "# from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823baca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_masking_from_features(\n",
    "                            max_seq_len,\n",
    "                            max_predictions_per_batch,\n",
    "                            vocab_size,\n",
    "                            cls_id,\n",
    "                            sep_id,\n",
    "                            unk_id,\n",
    "                            pad_id,\n",
    "                            mask_id\n",
    "                            ):\n",
    "    \n",
    "    \"\"\"Dynamic Masking from input_ids (saved as tfrecord)\"\"\"\n",
    "    # Truncate inputs to a maximum length.\n",
    "    trimmer = tf_text.RoundRobinTrimmer(max_seq_length=max_seq_len)\n",
    "\n",
    "    # Random Selector\n",
    "    random_selector = tf_text.RandomItemSelector(\n",
    "        max_selections_per_batch=max_predictions_per_batch,\n",
    "        selection_rate=0.2,\n",
    "        unselectable_ids=[cls_id, sep_id, unk_id, pad_id]\n",
    "    )\n",
    "\n",
    "    # Mask Value chooser (Encapsulates the BERT MLM token selection logic)\n",
    "    mask_values_chooser = tf_text.MaskValuesChooser(vocab_size, mask_id, 0.8)\n",
    "    \n",
    "    \n",
    "    def map_mlm(item):\n",
    "        \n",
    "        segments = item['input_ids']\n",
    "        trimmed_segments = trimmer.trim([segments])\n",
    "\n",
    "        # We replace trimmer with slice [:_MAX_SEQ_LEN-2] operation # 2 to add CLS and SEP\n",
    "        # input_ids = item['input_ids'][:_MAX_SEQ_LEN-2]\n",
    "\n",
    "        # Combine segments, get segment ids and add special tokens.\n",
    "        segments_combined, segment_ids = tf_text.combine_segments(\n",
    "              trimmed_segments,\n",
    "              start_of_sequence_id=cls_id,\n",
    "              end_of_segment_id=sep_id)\n",
    "\n",
    "        # We replace segment with concat\n",
    "        # input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "          segments_combined,\n",
    "          item_selector=random_selector,\n",
    "          mask_values_chooser=mask_values_chooser)\n",
    "\n",
    "        # Prepare and pad combined segment inputs\n",
    "        input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "            masked_token_ids, max_seq_length=max_seq_len)\n",
    "        input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "            segment_ids, max_seq_length=max_seq_len)\n",
    "\n",
    "        # Prepare and pad masking task inputs\n",
    "        # Masked lm weights will mask the weights\n",
    "        masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "          masked_pos, max_seq_length=max_predictions_per_batch)\n",
    "        masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "          masked_lm_ids, max_seq_length=max_predictions_per_batch)\n",
    "\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = input_word_ids\n",
    "        inputs['input_type_ids'] = input_type_ids\n",
    "        inputs['input_mask'] = input_mask\n",
    "        inputs['masked_lm_positions'] = masked_lm_positions\n",
    "\n",
    "        labels = {}\n",
    "        labels['masked_lm_labels'] = masked_lm_ids\n",
    "        labels['masked_lm_weights']   = masked_lm_weights # Mask\n",
    "\n",
    "        return (inputs, labels)\n",
    "    \n",
    "    return map_mlm\n",
    "\n",
    "    \n",
    "    \n",
    "def get_tfdataset_from_tfrecords(tfrecord_path_list):\n",
    "    \"\"\"Get tf dataset from tfrecords\"\"\"\n",
    "    all_files = []\n",
    "    for tfrecord_path in tfrecord_path_list:\n",
    "        all_files.extend(glob.glob(\"{}/*.tfrecord\".format(tfrecord_path)))\n",
    "    schema    = json.load(open(\"{}/schema.json\".format(tfrecord_path)))\n",
    "    tf_reader = TFReader(schema=schema, \n",
    "                        tfrecord_files=all_files)\n",
    "    train_dataset = tf_reader.read_record(\n",
    "                                      )\n",
    "    return train_dataset\n",
    "\n",
    "def filter_by_length(x, min_sen_len):\n",
    "    \"\"\"Filter by minimum sentence length (subwords)\"\"\"\n",
    "    return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']) ,tf.constant(min_sen_len)), axis=0)\n",
    "\n",
    "def filter_by_batch(x, y, batch_size):\n",
    "    \"\"\"Filter by batch size\"\"\"\n",
    "    x_batch = tf.shape(x['input_ids'])[0]\n",
    "    return tf.equal(x_batch, tf.constant(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe3452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Model\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": 30000,\n",
    "        \"layer_norm_epsilon\": 1e-12\n",
    "    }\n",
    "    \n",
    "    from tf_transformers.models import BertModel\n",
    "    model = BertModel.from_config(config,\n",
    "                                  batch_size=None,\n",
    "                                  use_masked_lm_positions=True, # Add batch_size to avoid dynamic shapes\n",
    "                                  return_all_layer_outputs=True) \n",
    "\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "    \"\"\"Optimizer\"\"\"\n",
    "    LEARNING_RATE = 5-e5\n",
    "    NUM_TRAIN_STEPS = 100000\n",
    "    NUM_WARMUP_STEPS = 30000\n",
    "    OPTIMIZER_TYPE = \"adamw\"\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=LEARNING_RATE, \n",
    "                                                 num_train_steps=NUM_TRAIN_STEPS,\n",
    "                                                 num_warmup_steps=NUM_WARMUP_STEPS, \n",
    "                                                 optimizer_type=OPTIMIZER_TYPE)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \"\"\"Joint loss over all layers\"\"\"    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        \n",
    "        loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                logits=per_layer_output, \n",
    "                                label_weights=y_true_dict['masked_lm_weights'])\n",
    "        loss_dict['loss_{}'.format(layer_count+1)] = loss\n",
    "        loss_holder.append(loss)\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015623bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad543d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "class MLMCallback():\n",
    "    \"\"\"Simple MLM Callback to check progress of the training\"\"\"\n",
    "    def __init__(self, tokenizer, input_ids, top_k=10):\n",
    "        \"\"\"Init\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = input_ids\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def get_inputs(self):\n",
    "        \"\"\"Text to features\"\"\"\n",
    "        inputs_tf = {}\n",
    "        inputs_tf[\"input_ids\"] = self.input_ids\n",
    "        inputs_tf[\"input_type_ids\"] = tf.zeros_like(inputs[\"input_ids\"])\n",
    "        inputs_tf[\"input_mask\"] = tf.ones_like(inputs[\"input_ids\"])\n",
    "        \n",
    "        seq_length = tf.shape(inputs_tf['input_ids'])[1]\n",
    "        inputs_tf['masked_lm_positions'] = tf.zeros_like(inputs_tf[\"input_ids\"]) + tf.range(seq_length)\n",
    "\n",
    "        return inputs_tf\n",
    "    \n",
    "    \n",
    "    def __call__(self, trainer_params):\n",
    "        \"\"\"Main Call\"\"\"\n",
    "        model = trainer_params['model']\n",
    "        inputs_tf = self.get_inputs()\n",
    "        outputs_tf = model(inputs_tf)\n",
    "        \n",
    "        # Get masked positions from each sentence\n",
    "        masked_positions = tf.argmax(tf.equal(inputs_tf[\"input_ids\"], 5), axis=1)\n",
    "        for layer_count,layer_logits in enumerate(outputs_tf['all_layer_token_logits']):\n",
    "            print(\"Layer {}\".format(layer_count+1))\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            for i,logits in enumerate(layer_logits):\n",
    "                mask_token_logits = logits[masked_positions[i]]\n",
    "                # 0 for probs and 1 for indexes from tf.nn.top_k\n",
    "                top_words = self.tokenizer.detokenize(tf.nn.top_k(mask_token_logits, k = self.top_k)[1].numpy())\n",
    "                print(\"Input ----> {}\".format(self.validation_sentences[i]))\n",
    "                print(\"Predicted words ----> {}\".format(top_words.numpy().decode().split()))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5216cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df844c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define Constants\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "MAX_PREDICTIONS_PER_BATCH = 20\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "TFRECORDS_PATH = ['/home/sidhu/Datasets/TFRECORD_malayalam']\n",
    "TPU_ADDRESS = 'local'\n",
    "DTYPE = 'bf16'\n",
    "\n",
    "MODEL_DIR  ='malayalam_bert'\n",
    "EPOCHS = 3\n",
    "STEPS_PER_EPOCH = 200\n",
    "CALLBACK_STEPS = 100\n",
    "TRAINING_LOSS_NAMES = ['loss_1',\n",
    " 'loss_2',\n",
    " 'loss_3',\n",
    " 'loss_4',\n",
    " 'loss_5',\n",
    " 'loss_6',\n",
    " 'loss_7',\n",
    " 'loss_8',\n",
    " 'loss_9',\n",
    " 'loss_10',\n",
    " 'loss_11',\n",
    " 'loss_12']\n",
    "\n",
    "tokenizer = get_tf_text_tokenizer(\"/home/sidhu/Projects/vocab/malayalam.model\")\n",
    "\n",
    "\n",
    "validation_encoded = tf.constant([[    3,   472,  3031,   305,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  8580, 14521,   489,   375,    52, 18853,    24,   872,\n",
    "         2509,   106, 14555, 23787, 13839, 12571,     5,     4],\n",
    "       [    3,  5860,    42, 20550,  8057,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  1306,  2672,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,   212, 21227,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  2195,  1014,   409,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,    62,    73,    21,  4386,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3, 20465,   299,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0]])\n",
    "\n",
    "mlm_callback = MLMCallback(tokenizer, validation_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0013dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TF dataset\n",
    "\n",
    "dynamic_mlm_fn = dynamic_masking_from_features(\n",
    "                            MAX_SEQ_LEN,\n",
    "                            MAX_PREDICTIONS_PER_BATCH,\n",
    "                            30000,\n",
    "                            3,\n",
    "                            4,\n",
    "                            1,\n",
    "                            0,\n",
    "                            5\n",
    "                            )\n",
    "\n",
    "train_dataset = get_tfdataset_from_tfrecords(TFRECORDS_PATH)\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=BATCH_SIZE))\n",
    "train_dataset = train_dataset.map(dynamic_mlm_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.filter(lambda x, y: filter_by_batch(x, y, BATCH_SIZE))\n",
    "train_dataset = train_dataset.shuffle(100)\n",
    "train_dataset = train_dataset.prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4a7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17164f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "def get_tf_text_tokenizer(model_file_path):\n",
    "    \n",
    "    def _create_tokenizer(model_serialized_proto, dtype, nbest_size, alpha):\n",
    "        return tf_text.SentencepieceTokenizer(\n",
    "            model=model_serialized_proto,\n",
    "            out_type=dtype,\n",
    "            nbest_size=nbest_size,\n",
    "            alpha=alpha)\n",
    "    \n",
    "    dtype = tf.int32\n",
    "    nbest_size = 0\n",
    "    alpha = 1.0\n",
    "    \n",
    "    model_serialized_proto = tf.io.gfile.GFile(model_file_path,\n",
    "                                                           \"rb\").read()\n",
    "\n",
    "    tokenizer_sp = _create_tokenizer(model_serialized_proto, \n",
    "                                 dtype,\n",
    "                                 nbest_size,\n",
    "                                 alpha)\n",
    "    \n",
    "    return tokenizer_sp\n",
    "\n",
    "tokenizer = get_tf_text_tokenizer(\"/home/sidhu/Projects/vocab/malayalam.model\")\n",
    "validation_sentences = \"\"\"എനിക്ക് നിന്നെ വളരെ \n",
    "എഴുത്ത് ഉപകരണങ്ങൾ വെബിൽ എവിടേയും നിങ്ങൾ തിരഞ്ഞെടുക്കുന്ന ഭാഷയിൽ ടൈപ്പുചെയ്യുന്നതിനെ\n",
    "സച്ചിന്റെ ബാറ്റിംഗ് എല്ലാവർക്കും \n",
    "എല്ലാവരും വോട്ട് \n",
    "ഞാൻ കളിക്കാൻ \n",
    "മമ്മൂട്ടി ഇന്നലെ അവിടെ \n",
    "മയിൽ ഒരു മനോഹര \n",
    "ചന്ദ്രൻ ഇന്ന്\"\"\".split(\"\\n\") \n",
    "validation_encoded = tf.constant([[    3,   472,  3031,   305,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  8580, 14521,   489,   375,    52, 18853,    24,   872,\n",
    "         2509,   106, 14555, 23787, 13839, 12571,     5,     4],\n",
    "       [    3,  5860,    42, 20550,  8057,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  1306,  2672,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,   212, 21227,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,  2195,  1014,   409,     5,     4,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3,    62,    73,    21,  4386,     5,     4,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0],\n",
    "       [    3, 20465,   299,     5,     4,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0]])\n",
    "\n",
    "inputs_tf = {}\n",
    "inputs_tf[\"input_ids\"] = validation_encoded\n",
    "inputs_tf[\"input_type_ids\"] = tf.zeros_like(inputs_tf[\"input_ids\"])\n",
    "inputs_tf[\"input_mask\"] = tf.ones_like(inputs_tf[\"input_ids\"]) * tf.cast(tf.not_equal(inputs_tf[\"input_ids\"], 0), tf.int32)\n",
    "\n",
    "config = {\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"intermediate_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"embedding_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"attention_head_size\": 64,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"vocab_size\": 30000,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "}\n",
    "    \n",
    "from tf_transformers.models import BertModel\n",
    "model = BertModel.from_config(config,\n",
    "                              batch_size=None,\n",
    "                              return_all_layer_outputs=True) \n",
    "\n",
    "model.load_checkpoint(\"malayalam_bert/\")\n",
    "\n",
    "outputs_tf = model(inputs_tf)\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "# Get masked positions from each sentence\n",
    "masked_positions = tf.argmax(tf.equal(inputs_tf[\"input_ids\"], 5), axis=1)\n",
    "for layer_count,layer_logits in enumerate(outputs_tf['all_layer_token_logits']):\n",
    "    print(\"Layer {}\".format(layer_count+1))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    for i,logits in enumerate(layer_logits):\n",
    "        mask_token_logits = logits[masked_positions[i]]\n",
    "        # 0 for probs and 1 for indexes from tf.nn.top_k\n",
    "        top_words = tokenizer.detokenize(tf.nn.top_k(mask_token_logits, k = top_k)[1].numpy())\n",
    "        print(\"Input ----> {}\".format(validation_sentences[i]))\n",
    "        print(\"Predicted words ----> {}\".format(top_words.numpy().decode().split()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfd6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93289eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3dc22d10b3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccented_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'Málaga'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# accented_string is of type 'unicode'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0munaccented_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munidecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccented_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240cc01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
