{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 + Squad + Text Generation\n",
    "\n",
    "This tutorial contains code to fine-tune an T5 Model for Squad as Text Generation or Auto-Regressive task\n",
    "\n",
    "In this notebook:\n",
    "- Load the data + create ```tf.data.Dataset``` using TFWriter\n",
    "- Load and fine-tune T5\n",
    "- Train using ```tf.keras.Model.fit``` and ```Custom Trainer``` \n",
    "- Minimze LM loss\n",
    "- Evaluate EM/F1 score\n",
    "- In production using faster ```tf.SavedModel``` + no architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from tf_transformers.models import T5Model\n",
    "from transformers import T5Tokenizer\n",
    "from tf_transformers.data.squad_utils_sp import (\n",
    "    read_squad_examples, evaluate_v1)\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.losses import cross_entropy_loss_label_smoothing\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.8381352424621582\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/train-v1.1.json'\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 1. Read Examples\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    "      )\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "\n",
    "max_passage_length = 384\n",
    "max_question_length = 64\n",
    "max_answer_length = 40\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        question_input_ids =  tokenizer.tokenize('question: ' + f['question_text'])[: max_question_length] \n",
    "        passage_input_ids  =  tokenizer.tokenize('context: '  + f['paragraph_text'])[: max_passage_length -1]  + [tokenizer.eos_token] # -1 to add </s>\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(question_input_ids + passage_input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        decoder_input_ids = [tokenizer.pad_token] + tokenizer.tokenize(f['orig_answer_text'])[: max_answer_length-2] + [tokenizer.eos_token]\n",
    "        decoder_input_ids = tokenizer.convert_tokens_to_ids(decoder_input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['encoder_input_ids'] = input_ids\n",
    "        result['encoder_input_mask'] = input_mask\n",
    "        result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "        \n",
    "        result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "        result['labels_mask'] = [1] * len(decoder_input_ids[1:])\n",
    "        \n",
    "        # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "    \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "    \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "    \"labels_mask\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/squad_as_generation/t5/train'\n",
    "tfrecord_filename = 'squad'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['encoder_input_ids', 'encoder_input_mask', 'decoder_input_ids']\n",
    "y_keys = ['labels', 'labels_mask']\n",
    "batch_size = 16\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load t5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwride mask_mode with user_defined\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Overwride mask_mode with causal\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n",
      "INFO:absl:Encoder loaded succesfully from /mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/roberta-base/\n",
      "INFO:absl:Warm started decoder 197/202 variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_layer, model, config = T5Model(model_name='t5-small')\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/t5-small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss + Label Smoothing\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 2D (batch_size x sequence_length)\n",
    "* logits: 3D (batch_size x sequence_length x vocab_size)\n",
    "* label_weights: 2D (batch_size x sequence_length) # we don't want all words in the sequence to have loss so, we mask them and don't calculate for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict, smoothing=0.1):\n",
    "    \n",
    "    return cross_entropy_loss_label_smoothing(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'],\n",
    "                                   smoothing=smoothing,\n",
    "                                      label_weights=y_true_dict['labels_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer\n",
    "\n",
    "* learning_rate is the key\n",
    "\n",
    "**PRO TIP**: These models are very sensitive to optimizer, especially learning rates. So, make sure you play around to find a good combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimization.AdamWeightDecay(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 36s 7s/step - loss: 16.4347 - token_logits_loss: 16.4347\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 36s 7s/step - loss: 11.4058 - token_logits_loss: 11.4058\n"
     ]
    }
   ],
   "source": [
    "# Keras Fit\n",
    "\n",
    "keras_loss_fn = {'token_logits': lm_loss\n",
    "                }\n",
    "model.compile2(optimizer=optimizer, \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn, \n",
    "                            run_eagerly=False)\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = 87000\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 4\n",
    "\n",
    "# Custom training\n",
    "history2 = SimpleTrainer(model = model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = lm_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"../OFFICIAL_MODELS/squad_as_generation/t5_small_label_smoothing\"\n",
    "model.save_checkpoint(model_save_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model for Text Genration (Auto-Regressive)\n",
    "\n",
    "1. For any model to use for auto-regressive tasks we have to provide **\"pipeline_mode='auto-regressive'\"**\n",
    "\n",
    "tf-transformers will handle everything for you internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model by disabling dropout and add pipeline_mode = 'auto-regressive'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model_layer, model, config = T5Model(model_name='t5-small', is_training=False, pipeline_mode='auto-regressive')\n",
    "model.load_checkpoint(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model as serialized version\n",
    "\n",
    "This is very important, because serialized model is significantly faster.\n",
    "tf-transfomers provide **save_as_serialize_module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=True)\n",
    "loaded = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/dev-v1.1.json'\n",
    "\n",
    "is_training = False\n",
    "\n",
    "start_time = time.time()\n",
    "dev_examples = read_squad_examples(\n",
    "      input_file=dev_input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "def parse_dev():\n",
    "    for f in dev_examples:\n",
    "        question_input_ids =  tokenizer.tokenize('question: ' + f['question_text'])[: max_question_length] \n",
    "        passage_input_ids  =  tokenizer.tokenize('context: '  + f['paragraph_text'])[: max_passage_length -1]  + [tokenizer.eos_token] # -1 to add </s>\n",
    "       \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(question_input_ids + passage_input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "        result = {}\n",
    "        result['encoder_input_ids'] = input_ids\n",
    "        result['encoder_input_mask'] = input_mask\n",
    "       \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(predicted_ids, eos_id):\n",
    "    all_ids = []\n",
    "    for per_example_id in predicted_ids:\n",
    "        try:\n",
    "            index = per_example_id.index(eos_id)\n",
    "        except:\n",
    "            index = -1\n",
    "        sliced_ids = per_example_id[:index]\n",
    "        all_ids.append(sliced_ids)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Generation for dev dataset\n",
    "\n",
    "1. For **EncoderDecoder** models like Roberta2Roberts, Bert2GPT, t5, BART use **TextDecoderSeq2Seq**\n",
    "2. For **Encoder** only models like GPT2, BERT, Roberta use **TextDecoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "\n",
    "decoder = TextDecoderSeq2Seq(model=loaded, \n",
    "                            decoder_start_token_id=0, \n",
    "                            )\n",
    "# OR if keras.model\n",
    "\n",
    "#decoder = TextDecoderSeq2Seq(model=model, \n",
    "#                            decoder_start_token_id=0\n",
    "#                            )\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_answers = []\n",
    "for batch_inputs in dev_dataset:\n",
    "    model_outputs = decoder.decode(batch_inputs, \n",
    "                   mode='greedy',                 \n",
    "                   max_iterations=40, \n",
    "                   eos_id=tokenizer.eos_token_id)\n",
    "\n",
    "    predicted_ids = model_outputs['predicted_ids'][:, 0, :].numpy().tolist()\n",
    "    predicted_ids_sliced = split_by_id(predicted_ids, tokenizer.eos_token_id)\n",
    "    predicted_text = [tokenizer.decode(p_ids, skip_special_tokens=True) for p_ids in predicted_ids_sliced]\n",
    "        \n",
    "    \n",
    "    predicted_answers.extend(predicted_text)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "\n",
    "squad_dev_data = json.load(open(dev_input_file_path))['data']\n",
    "qas_id_answer  = {item['qas_id']: predicted_answers[i] for(i, item) in enumerate(dev_examples)}\n",
    "eval_results = evaluate_v1(squad_dev_data, qas_id_answer)\n",
    "\n",
    "# {'exact_match': 75.38315988647115, 'f1': 83.89726910155875}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Production\n",
    "1. Lets see how we can deploy this model in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "# 1. Load Saved Model\n",
    "loaded = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))\n",
    "\n",
    "# 2. Initiate a decode object\n",
    "decoder = TextDecoderSeq2Seq(model=loaded, \n",
    "                            decoder_start_token_id=0 # Decoder always expect a start_token_id\n",
    "                            )\n",
    "\n",
    "# 3. Convert text to inputs\n",
    "\n",
    "# Tokenizer fn convert text -> model inputs\n",
    "# Make sure you return dict with key-> list of list\n",
    "# pad_dataset is a decorator, hich will automatically taken care of padding\n",
    "\n",
    "# If you want to write your own function, please. model expect inputs in a specifed format thats all.\n",
    "@pad_dataset\n",
    "def tokenizer_fn(contexts, questions):\n",
    "    input_ids      = []\n",
    "    input_mask     = []\n",
    "    for (question, context) in zip(contexts, questions):\n",
    "        question_input_ids =  tokenizer.tokenize('question: ' + question)[: max_question_length] \n",
    "        passage_input_ids  =  tokenizer.tokenize('context: '  + context)[: max_passage_length -1]  + [tokenizer.eos_token] # -1 to add </s>\n",
    "       \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(question_input_ids + passage_input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "    result = {}\n",
    "    result['encoder_input_ids'] = input_ids\n",
    "    result['encoder_input_mask'] = input_mask\n",
    "    \n",
    "    return result\n",
    "\n",
    "questions = ['When was Kerala formed?']\n",
    "questions = ['What was prominent in Kerala?']\n",
    "questions = ['How many districts are there in Kerala']\n",
    "questions = ['When was Kerala formed?']\n",
    "\n",
    "contexts = ['''Kerala (English: /ˈkɛrələ/; Malayalam: [ke:ɾɐɭɐm] About this soundlisten (help·info)) is a\n",
    "state on the southwestern Malabar Coast of India. It was formed on 1 November 1956, \n",
    "following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of \n",
    "the erstwhile states of Travancore-Cochin and Madras. \n",
    "Spread over 38,863 km2 (15,005 sq mi), Kerala is the twenty-first largest Indian state by area. \n",
    "It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea[14] to the west. With 33,387,677 inhabitants as per the 2011 Census, Kerala is the thirteenth-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.[15]\n",
    "\n",
    "The Chera Dynasty was the first prominent kingdom based in Kerala. The Ay kingdom in the deep south and the Ezhimala kingdom in the north formed the other kingdoms in the early years of the Common Era (CE). The region had been a prominent spice exporter since 3000 BCE. The region's prominence in trade was noted in the works of Pliny as well as the Periplus around 100 CE. In the 15th century, the spice trade attracted Portuguese traders to Kerala, and paved the way for European colonisation of India. At the time of Indian independence movement in the early 20th century, there were two major princely states in Kerala-Travancore State and the Kingdom of Cochin. They united to form the state of Thiru-Kochi in 1949. The Malabar region, in the northern part of Kerala, had been a part of the Madras province of British India, which later became a part of the Madras State post-independence. After the States Reorganisation Act, 1956, the modern-day state of Kerala was formed by merging the Malabar district of Madras State (excluding Gudalur taluk of Nilgiris district, Lakshadweep Islands, Topslip, the Attappadi Forest east of Anakatti), the state of Thiru-Kochi (excluding four southern taluks of Kanyakumari district, Shenkottai and Tenkasi taluks), and the taluk of Kasaragod (now Kasaragod District) in South Canara (Tulunad) which was a part of Madras State.''']\n",
    "\n",
    "\n",
    "\n",
    "# 5. Choose the type of decoding\n",
    "batch_inputs = tokenizer_fn(questions, contexts)\n",
    "model_outputs = decoder.decode(batch_inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=40, \n",
    "               eos_id=tokenizer.eos_token_id)\n",
    "\n",
    "output_answer = tokenizer.batch_decode(tf.squeeze(model_outputs['predicted_ids'], 1), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
