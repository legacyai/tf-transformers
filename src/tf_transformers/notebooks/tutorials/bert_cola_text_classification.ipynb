{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import BertModel\n",
    "from tf_transformers.tasks import Classification_Model\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.pipeline import Classification_Pipeline\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COLA dataset from Huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/glue/cola/\")\n",
    "train_examples = examples[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 1, 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\"}\n"
     ]
    }
   ],
   "source": [
    "for item in train_examples:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 8551\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=128\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "    \"input_mask\": (\"var_len\", \"int\"),\n",
    "    \"input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../../OFFICIAL_TFRECORDS/glue/bert/cola/train'\n",
    "tfrecord_filename = 'cola'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "batch_size = 32\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT base-uncased Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "\n",
    "model_layer, model, config = BertModel(model_name='bert-base-uncased', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False \n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/bert_base_uncased/bert_base_uncased/\")\n",
    "\n",
    "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
    "# model -> legacyModel inherited from tf.keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classification_layer = Classification_Model(model=model,\n",
    "                                      num_classes=2,\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=True)\n",
    "classification_model = classification_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save up memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del classification_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # class indices\n",
    "* logits: 2D (batch_size x num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true_dict, y_pred_dict):\n",
    "    logits = y_pred_dict['class_logits']\n",
    "    labels = y_true_dict['labels']\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(labels, axis=1)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 8500\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 5s 490ms/step - loss: 0.6858 - class_logits_loss: 0.6858\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 6s 634ms/step - loss: 0.6583 - class_logits_loss: 0.6583\n"
     ]
    }
   ],
   "source": [
    "# # Compile\n",
    "keras_loss_fn = {'class_logits': loss_fn}\n",
    "classification_model.compile2(optimizer=optimizer, \n",
    "                             loss=None, \n",
    "                             custom_loss=keras_loss_fn)\n",
    "# Change steps per epoch to large value/ ignore it completely to train\n",
    "# on full dataset\n",
    "history = classification_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = SimpleTrainer(model = classification_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = loss_fn,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"../../OFFICIAL_MODELS/glue/cola/bert\"\n",
    "classification_model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1043\n"
     ]
    }
   ],
   "source": [
    "dev_examples = examples['validation']\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, shuffle=False, x_keys=x_keys, y_keys=y_keys, batch_size=32, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dev dataset - Mathews Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dev_dataset:\n",
    "    model_outputs = classification_model(batch_inputs)['class_logits']\n",
    "\n",
    "    predictions.append(tf.argmax(model_outputs, axis=1))\n",
    "    original_labels.append(batch_labels['labels'].numpy())\n",
    "    \n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "eval_metrics =  matthews_corrcoef(np.hstack(predictions), np.hstack(original_labels))\n",
    "print(\"Mathews corelation\", eval_metrics)\n",
    "\n",
    "# Mathews corelation 0.5952198946938653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Serialized version \n",
    "\n",
    "- Now we can use ```save_as_serialize_module``` to save a model directly to saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=False)\n",
    "classification_model_serialized = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLite Conversion\n",
    "\n",
    "TFlite conversion requires:\n",
    "- static batch size\n",
    "- static sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer, model, config = BertModel(model_name='bert-base-uncased', \n",
    "                                     batch_size=1, \n",
    "                                     sequence_length=128, \n",
    "                                     is_training=False\n",
    "                                     )\n",
    "\n",
    "\n",
    "classification_layer = Classification_Model(model=model,\n",
    "                                      num_classes=2,\n",
    "                                      is_training=False)\n",
    "classification_model = classification_layer.get_model()\n",
    "classification_model.load_checkpoint(model_save_dir)\n",
    "\n",
    "# Save to .pb format , we need it for tflite\n",
    "\n",
    "classification_model.save_as_serialize_module(\"{}/saved_model_for_tflite\".format(model_save_dir))\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}/saved_model_for_tflite\".format(model_save_dir)) # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"{}/converted_model.tflite\".format(model_save_dir), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In production**\n",
    "\n",
    "- We can use either ```tf.keras.Model``` or ```saved_model```. I recommend saved_model, which is much much faster and no hassle of having architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.data import pad_dataset_normal\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_seq_length = 128\n",
    "\n",
    "@pad_dataset_normal\n",
    "def tokenizer_fn(texts):\n",
    "    \"\"\"\n",
    "    feature: tokenized text (tokenizer.tokenize)\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    input_type_ids = []\n",
    "    input_mask = []\n",
    "    for text in texts:\n",
    "        input_ids_ex = [tokenizer.cls_token] + tokenizer.tokenize(text)[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids_ex = tokenizer.convert_tokens_to_ids(input_ids_ex)\n",
    "        input_mask_ex = [1] * len(input_ids_ex)\n",
    "        input_type_ids_ex = [0] * len(input_ids_ex)\n",
    "        \n",
    "        input_ids.append(input_ids_ex)\n",
    "        input_type_ids.append(input_type_ids_ex)\n",
    "        input_mask.append(input_mask_ex)\n",
    "        \n",
    "    result = {}\n",
    "    result['input_ids'] = input_ids\n",
    "    result['input_type_ids'] = input_type_ids\n",
    "    result['input_mask'] = input_mask\n",
    "    return result\n",
    "\n",
    "\n",
    "# load serialized model\n",
    "label_map_reverse = {0: 'unacceptable', 1: 'acceptable'}\n",
    "pipeline = Classification_Pipeline( model = classification_model_serialized, \n",
    "                tokenizer_fn = tokenizer_fn, \n",
    "                label_map = label_map_reverse,\n",
    "                batch_size=32)\n",
    "\n",
    "sentences = ['In which way is Sandy very anxious to see if the students will be able to solve the homework problem?',\n",
    "            'The book was written by John.', \n",
    "            'Play Carnatic Fusion by Various Artists', \n",
    "            'She voted herself.']\n",
    "result = pipeline(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check for TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check same model with tflite\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(model_save_dir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "tflite_seq_length = 128\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "sample_inputs = {}\n",
    "sample_inputs['input_ids'] = tf.random.uniform(minval=0, maxval=100, \n",
    "                                                                    shape=(1, tflite_seq_length), dtype=tf.int32)\n",
    "sample_inputs['input_type_ids'] = tf.zeros_like(sample_inputs['input_ids'])\n",
    "sample_inputs['input_mask'] = tf.ones_like(sample_inputs['input_ids'])\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_inputs['input_ids'])\n",
    "interpreter.set_tensor(input_details[1]['index'],  sample_inputs['input_mask'])\n",
    "interpreter.set_tensor(input_details[2]['index'], sample_inputs['input_type_ids'])\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "model_output = classification_model_serialized(**sample_inputs) # Why ** ? because it is a saved model\n",
    "\n",
    "# Check tf.reduce_sum(tflite_output), tf.reduce_sum(model_output['token_logits'])\n",
    "# Both matches :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
