{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "import collections\n",
    "\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer, ROBERTA_SPECIAL_PEICE\n",
    "from tf_transformers.utils import fast_sp_alignment\n",
    "from tf_transformers.data.squad_utils_sp import (\n",
    "    read_squad_examples,\n",
    "    post_clean_train_squad,\n",
    "    example_to_features_using_fast_sp_alignment_train,\n",
    "    example_to_features_using_fast_sp_alignment_test, \n",
    "    _get_best_indexes, evaluate_v1\n",
    ")\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import RobertaModel\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.tasks import Span_Selection_Model\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")\n",
    "\n",
    "from tf_transformers.pipeline.span_extraction_pipeline import Span_Extraction_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "basic_tokenizer = BasicTokenizer(do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert train data to Features\n",
    "\n",
    "* using Fast Sentence Piece Alignment, we convert text to features (text -> list of sub words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Time taken 0.06583905220031738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.7573883533477783\n",
      "time taken 1.051743984222412 seconds\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/train-v1.1.json'\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 1. Read Examples\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    "      )\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "# 2.Postprocess (clean text to avoid some unwanted unicode charcaters)\n",
    "train_examples_processed, failed_examples = post_clean_train_squad(train_examples, basic_tokenizer, is_training=is_training)\n",
    "\n",
    "\n",
    "# 3.Convert question, context and answer to proper features (tokenized words) not word indices\n",
    "feature_generator = example_to_features_using_fast_sp_alignment_train(tokenizer, train_examples_processed, max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=ROBERTA_SPECIAL_PEICE) \n",
    "\n",
    "all_features = []\n",
    "for feature in feature_generator:\n",
    "    all_features.append(feature)\n",
    "end_time = time.time()\n",
    "print(\"time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to TFRecords using TFWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 100\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in all_features:\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(f['input_ids'])\n",
    "        input_type_ids = tf.zeros_like(input_ids).numpy().tolist()\n",
    "        input_mask = tf.ones_like(input_ids).numpy().tolist()\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['start_position'] = f['start_position']\n",
    "        result['end_position']   = f['end_position']\n",
    "        yield result\n",
    "        \n",
    "\n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smaller data\n",
    "\n",
    "schema = {'input_ids': (\"var_len\", \"int\"), \n",
    "         'input_type_ids': (\"var_len\", \"int\"), \n",
    "         'input_mask': (\"var_len\", \"int\"), \n",
    "         'start_position': (\"var_len\", \"int\"), \n",
    "         'end_position': (\"var_len\", \"int\")}\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/squad/train'\n",
    "tfrecord_filename = 'squad'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['start_position', 'end_position']\n",
    "batch_size = 16\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Roberta base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_layer, model, config = RobertaModel(model_name='roberta-base', return_all_layer_token_embeddings=False)\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/roberta-base/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Span Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=True)\n",
    "span_selection_model = span_selection_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save up memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del span_selection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # start or end positions\n",
    "* logits: 2D (batch_size x sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross Entropy\n",
    "def span_loss(position, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(position, axis=1)))\n",
    "    return loss\n",
    "\n",
    "# Start logits loss\n",
    "def start_loss(y_true_dict, y_pred_dict):\n",
    "    start_loss = span_loss(y_true_dict['start_position'], y_pred_dict['start_logits'])\n",
    "    return start_loss\n",
    "\n",
    "# End logits loss\n",
    "def end_loss(y_true_dict, y_pred_dict):\n",
    "    end_loss = span_loss(y_true_dict['end_position'], y_pred_dict['end_logits'])\n",
    "    return end_loss\n",
    "# (start_loss + end_loss) / 2.0\n",
    "def joint_loss(y_true_dict, y_pred_dict):\n",
    "    sl = start_loss(y_true_dict, y_pred_dict)\n",
    "    el = end_loss(y_true_dict, y_pred_dict)\n",
    "    return (sl + el)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 89000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Start logits (16, None)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits (16, None)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/10 [=================>............] - ETA: 23s - loss: 10.2776 - end_logits_loss: 5.1788 - start_logits_loss: 5.0988WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 6/10 [=================>............] - 35s 6s/step - loss: 10.2776 - end_logits_loss: 5.1788 - start_logits_loss: 5.0988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90801d0c70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Fit\n",
    "\n",
    "keras_loss_fn = {'start_logits': start_loss, \n",
    "           'end_logits': end_loss}\n",
    "span_selection_model.compile2(optimizer=tf.keras.optimizers.Adam(), \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn\n",
    "                            )\n",
    "history = span_selection_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training\n",
    "history = SimpleTrainer(model = span_selection_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = joint_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = '../OFFICIAL_MODELS/squad/roberta_base2'\n",
    "span_selection_model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.07536649703979492\n"
     ]
    }
   ],
   "source": [
    "dev_input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/dev-v1.1.json'\n",
    "\n",
    "is_training = False\n",
    "\n",
    "start_time = time.time()\n",
    "dev_examples = read_squad_examples(\n",
    "      input_file=dev_input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "dev_examples_cleaned = post_clean_train_squad(dev_examples, basic_tokenizer, is_training=False)\n",
    "qas_id_info, dev_features = example_to_features_using_fast_sp_alignment_test(tokenizer, dev_examples_cleaned,  max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=ROBERTA_SPECIAL_PEICE)\n",
    "\n",
    "\n",
    "\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_features:\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(f['input_ids'])\n",
    "        input_type_ids = tf.zeros_like(input_ids).numpy().tolist()\n",
    "        input_mask = tf.ones_like(input_ids).numpy().tolist()\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        yield result\n",
    "        \n",
    "\n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Exact Match\n",
    "\n",
    "* Make Predictions\n",
    "* Extract Answers\n",
    "* Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_dict(dict_items, key):\n",
    "    holder = []\n",
    "    for item in dict_items:\n",
    "        holder.append(item[key])\n",
    "    return holder\n",
    "\n",
    "qas_id_list = extract_from_dict(dev_features, 'qas_id')\n",
    "doc_offset_list = extract_from_dict(dev_features, 'doc_offset')\n",
    "\n",
    "# Make batch predictions\n",
    "\n",
    "per_layer_start_logits = []\n",
    "per_layer_end_logits   = []\n",
    "\n",
    "start_time = time.time()\n",
    "for (batch_inputs) in dev_dataset:\n",
    "    model_outputs = span_selection_model(batch_inputs)\n",
    "    per_layer_start_logits.append(model_outputs['start_logits'])\n",
    "    per_layer_end_logits.append(model_outputs['end_logits'])\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Answers (text) from Predictions\n",
    "\n",
    "* Its little tricky as there will be multiple features for one example, if it is longer than max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batch predictions\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n",
    "squad_dev_data = json.load(open(dev_input_file_path))['data']\n",
    "predicted_results = []\n",
    "\n",
    "# Unstack (matrxi tensor) into list arrays (list)\n",
    "start_logits_unstcaked = []\n",
    "end_logits_unstacked = []\n",
    "for batch_start_logits in per_layer_start_logits:\n",
    "    start_logits_unstcaked.extend(tf.unstack(batch_start_logits))\n",
    "for batch_end_logits in per_layer_end_logits:\n",
    "    end_logits_unstacked.extend(tf.unstack(batch_end_logits))\n",
    "\n",
    "# Group (multiple predictions) of one example, due to big passage/context\n",
    "# We need to choose the best anser of all the chunks of a examples\n",
    "qas_id_logits = {}\n",
    "for i in range(len(qas_id_list)):\n",
    "    qas_id = qas_id_list[i]\n",
    "    example = qas_id_info[qas_id]\n",
    "    feature = dev_features[i]\n",
    "    assert qas_id == feature['qas_id']\n",
    "    if qas_id not in qas_id_logits:\n",
    "        qas_id_logits[qas_id] = {'tok_to_orig_index': example['tok_to_orig_index'],\n",
    "                                            'aligned_words': example['aligned_words'],\n",
    "                                            'feature_length': [len(feature['input_ids'])],\n",
    "                                            'doc_offset': [doc_offset_list[i]],\n",
    "                                            'passage_start_pos': [feature['input_ids'].index(tokenizer.sep_token) + 1],\n",
    "                                            'start_logits': [start_logits_unstcaked[i]], \n",
    "                                            'end_logits': [end_logits_unstacked[i]]}\n",
    "\n",
    "    else:\n",
    "        qas_id_logits[qas_id]['start_logits'].append(start_logits_unstcaked[i])\n",
    "        qas_id_logits[qas_id]['end_logits'].append(end_logits_unstacked[i])\n",
    "        qas_id_logits[qas_id]['feature_length'].append(len(feature['input_ids']))\n",
    "        qas_id_logits[qas_id]['doc_offset'].append(doc_offset_list[i])\n",
    "        qas_id_logits[qas_id]['passage_start_pos'].append(feature['input_ids'].index(tokenizer.sep_token) + 1)\n",
    "\n",
    "# Extract answer assoiate it with single (qas_id) unique identifier     \n",
    "qas_id_answer = {}\n",
    "skipped = []\n",
    "skipped_null = []\n",
    "global_counter = 0\n",
    "for qas_id in qas_id_logits:\n",
    "\n",
    "    current_example = qas_id_logits[qas_id]\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\",\n",
    "         \"start_log_prob\", \"end_log_prob\"])\n",
    "    prelim_predictions = []\n",
    "    example_features = []\n",
    "    for i in range(len( current_example['start_logits'])):\n",
    "        f = dev_features[global_counter]\n",
    "        assert f['qas_id'] == qas_id\n",
    "        example_features.append(f)\n",
    "        global_counter += 1\n",
    "        passage_start_pos = current_example['passage_start_pos'][i]\n",
    "        feature_length = current_example['feature_length'][i]\n",
    "\n",
    "        start_log_prob_list = current_example['start_logits'][i].numpy().tolist()[:feature_length]\n",
    "        end_log_prob_list = current_example['end_logits'][i].numpy().tolist()[:feature_length]\n",
    "        start_indexes = _get_best_indexes(start_log_prob_list, n_best_size)\n",
    "        end_indexes   = _get_best_indexes(end_log_prob_list, n_best_size)\n",
    "\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "              # We could hypothetically create invalid predictions, e.g., predict\n",
    "              # that the start of the span is in the question. We throw out all\n",
    "              # invalid predictions.\n",
    "              if start_index < passage_start_pos or end_index < passage_start_pos:\n",
    "                continue\n",
    "              if end_index < start_index:\n",
    "                continue\n",
    "              length = end_index - start_index + 1\n",
    "              if length > max_answer_length:\n",
    "                continue\n",
    "              start_log_prob = start_log_prob_list[start_index]\n",
    "              end_log_prob = end_log_prob_list[end_index]\n",
    "              start_idx = start_index - passage_start_pos\n",
    "              end_idx = end_index - passage_start_pos\n",
    "\n",
    "              prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=i,\n",
    "                            start_index=start_idx,\n",
    "                            end_index=end_idx,\n",
    "                            start_log_prob=start_log_prob,\n",
    "                            end_log_prob=end_log_prob))\n",
    "\n",
    "\n",
    "\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "        reverse=True)\n",
    "\n",
    "    if prelim_predictions:\n",
    "        best_index = prelim_predictions[0].feature_index\n",
    "        aligned_words = current_example['aligned_words']\n",
    "        try:\n",
    "            tok_to_orig_index = current_example['tok_to_orig_index']\n",
    "            reverse_start_index_align = tok_to_orig_index[prelim_predictions[0].start_index + example_features[best_index]['doc_offset']] # aligned index\n",
    "            reverse_end_index_align   = tok_to_orig_index[prelim_predictions[0].end_index + example_features[best_index]['doc_offset']]\n",
    "\n",
    "            predicted_words = [w for w in aligned_words[reverse_start_index_align: reverse_end_index_align + 1] if w != ROBERTA_SPECIAL_PEICE]\n",
    "            predicted_text = ' '.join(predicted_words)\n",
    "            qas_id_answer[qas_id] = predicted_text\n",
    "        except:\n",
    "            qas_id_answer[qas_id] = \"\"\n",
    "            skipped.append(qas_id)\n",
    "    else:\n",
    "        qas_id_answer[qas_id] = \"\"\n",
    "        skipped_null.append(qas_id)\n",
    "            \n",
    "            \n",
    "eval_results = evaluate_v1(squad_dev_data, qas_id_answer)\n",
    "# {'exact_match': 81.46641438032167, 'f1': 89.72853269935702}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Serialized version \n",
    "\n",
    "- Now we can use ```save_as_serialize_module``` to save a model directly to saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as optimized version\n",
    "span_selection_model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=True)\n",
    "\n",
    "# Load optimized version\n",
    "span_selection_model_serialized = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLite Conversion\n",
    "\n",
    "TFlite conversion requires:\n",
    "- static batch size\n",
    "- static sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence_length = 384\n",
    "# batch_size = 1\n",
    "\n",
    "# Lets convert it to a TFlite model\n",
    "\n",
    "# Load base model with specified sequence length and batch_size\n",
    "model_layer, model, config = RobertaModel(model_name='roberta-base', \n",
    "                                          sequence_length=384, # Fix a sequence length for TFlite (it shouldnt be None)\n",
    "                                          batch_size=1, \n",
    "                                          use_dropout=False) # batch_size=1\n",
    "\n",
    "# Disable dropout (important) for TFlite\n",
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      is_training=False)\n",
    "span_selection_model = span_selection_layer.get_model()\n",
    "span_selection_model.load_checkpoint(model_save_dir)\n",
    "\n",
    "# Save to .pb format , we need it for tflite\n",
    "span_selection_model.save_as_serialize_module(\"{}/saved_model_for_tflite\".format(model_save_dir))\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}/saved_model_for_tflite\".format(model_save_dir)) # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"{}/converted_model.tflite\".format(model_save_dir), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In production**\n",
    "\n",
    "- We can use either ```tf.keras.Model``` or ```saved_model```. I recommend saved_model, which is much much faster and no hassle of having architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer_fn(features):\n",
    "    \"\"\"\n",
    "    features: dict of tokenized text\n",
    "    Convert them into ids\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(features['input_ids'])\n",
    "    input_type_ids = tf.zeros_like(input_ids).numpy().tolist()\n",
    "    input_mask = tf.ones_like(input_ids).numpy().tolist()\n",
    "    result['input_ids'] = input_ids\n",
    "    result['input_type_ids'] = input_type_ids\n",
    "    result['input_mask'] = input_mask\n",
    "    return result\n",
    "\n",
    "# Span Extraction Pipeline\n",
    "pipeline = Span_Extraction_Pipeline(model = span_selection_model_serialized,\n",
    "                tokenizer = tokenizer, \n",
    "                tokenizer_fn = tokenizer_fn, \n",
    "                SPECIAL_PIECE = ROBERTA_SPECIAL_PEICE, \n",
    "                n_best_size = 20, \n",
    "                n_best = 5, \n",
    "                max_answer_length = 30, \n",
    "                max_seq_length = 384, \n",
    "                max_query_length=64, \n",
    "                doc_stride=20)\n",
    "\n",
    "questions = ['What was prominent in Kerala?']\n",
    "\n",
    "questions = ['When was Kerala formed?']\n",
    "\n",
    "questions = ['How many districts are there in Kerala']\n",
    "\n",
    "contexts = ['''Kerala (English: /ˈkɛrələ/; Malayalam: [ke:ɾɐɭɐm] About this soundlisten (help·info)) is a state on the southwestern Malabar Coast of India. It was formed on 1 November 1956, following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of the erstwhile states of Travancore-Cochin and Madras. Spread over 38,863 km2 (15,005 sq mi), Kerala is the twenty-first largest Indian state by area. It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea[14] to the west. With 33,387,677 inhabitants as per the 2011 Census, Kerala is the thirteenth-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.[15]\n",
    "\n",
    "The Chera Dynasty was the first prominent kingdom based in Kerala. The Ay kingdom in the deep south and the Ezhimala kingdom in the north formed the other kingdoms in the early years of the Common Era (CE). The region had been a prominent spice exporter since 3000 BCE. The region's prominence in trade was noted in the works of Pliny as well as the Periplus around 100 CE. In the 15th century, the spice trade attracted Portuguese traders to Kerala, and paved the way for European colonisation of India. At the time of Indian independence movement in the early 20th century, there were two major princely states in Kerala-Travancore State and the Kingdom of Cochin. They united to form the state of Thiru-Kochi in 1949. The Malabar region, in the northern part of Kerala, had been a part of the Madras province of British India, which later became a part of the Madras State post-independence. After the States Reorganisation Act, 1956, the modern-day state of Kerala was formed by merging the Malabar district of Madras State (excluding Gudalur taluk of Nilgiris district, Lakshadweep Islands, Topslip, the Attappadi Forest east of Anakatti), the state of Thiru-Kochi (excluding four southern taluks of Kanyakumari district, Shenkottai and Tenkasi taluks), and the taluk of Kasaragod (now Kasaragod District) in South Canara (Tulunad) which was a part of Madras State.''']\n",
    "\n",
    "result = pipeline(questions=questions, contexts=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check TFlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets do a sanity check\n",
    "\n",
    "sample_inputs = {}\n",
    "input_ids = tf.random.uniform(minval=0, maxval=100, shape=(1, 384), dtype=tf.int32)\n",
    "sample_inputs['input_ids'] = input_ids\n",
    "sample_inputs['input_type_ids'] = tf.zeros_like(sample_inputs['input_ids'])\n",
    "sample_inputs['input_mask'] = tf.ones_like(sample_inputs['input_ids'])\n",
    "\n",
    "model_outputs = span_selection_model(sample_inputs)\n",
    "\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(model_save_dir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_inputs['input_ids'])\n",
    "\n",
    "interpreter.set_tensor(input_details[1]['index'], sample_inputs['input_mask'])\n",
    "\n",
    "interpreter.set_tensor(input_details[2]['index'], sample_inputs['input_type_ids'])\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "end_logits = interpreter.get_tensor(output_details[0]['index'])\n",
    "start_logits   = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "# Assertion \n",
    "\n",
    "print(\"Start logits\", tf.reduce_sum(model_outputs['start_logits']), tf.reduce_sum(start_logits))\n",
    "print(\"End logits\", tf.reduce_sum(model_outputs['end_logits']), tf.reduce_sum(end_logits))\n",
    "\n",
    "# We are good :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
