{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta2Roberta + Summarization + Xsum\n",
    "\n",
    "This tutorial contains code to fine-tune an Roberta2Roberta Encoder Decoder Model for Summarization\n",
    "\n",
    "In this notebook:\n",
    "- Load the data + create ```tf.data.Dataset``` using TFWriter\n",
    "- Load and warmstart Roberta base and use it to create a Summarization Model\n",
    "- Train using ```tf.keras.Model.fit``` and ```Custom Trainer``` \n",
    "- Minimze LM loss\n",
    "- Evaluate ROUGE score\n",
    "- In production using faster ```tf.SavedModel``` + no architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from tf_transformers.models import EncoderDecoderModel\n",
    "from transformers import RobertaTokenizer\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load XSum sumarization data using Huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/xsum/\")\n",
    "train_examples = examples[\"train\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Wrote 9000 tfrecods\n",
      "INFO:absl:Wrote 10000 tfrecods\n",
      "INFO:absl:Wrote 11000 tfrecods\n",
      "INFO:absl:Wrote 12000 tfrecods\n",
      "INFO:absl:Wrote 13000 tfrecods\n",
      "INFO:absl:Wrote 14000 tfrecods\n",
      "INFO:absl:Wrote 15000 tfrecods\n",
      "INFO:absl:Wrote 16000 tfrecods\n",
      "INFO:absl:Wrote 17000 tfrecods\n",
      "INFO:absl:Wrote 18000 tfrecods\n",
      "INFO:absl:Wrote 19000 tfrecods\n",
      "INFO:absl:Wrote 20000 tfrecods\n",
      "INFO:absl:Wrote 21000 tfrecods\n",
      "INFO:absl:Wrote 22000 tfrecods\n",
      "INFO:absl:Wrote 23000 tfrecods\n",
      "INFO:absl:Wrote 24000 tfrecods\n",
      "INFO:absl:Wrote 25000 tfrecods\n",
      "INFO:absl:Wrote 26000 tfrecods\n",
      "INFO:absl:Wrote 27000 tfrecods\n",
      "INFO:absl:Wrote 28000 tfrecods\n",
      "INFO:absl:Wrote 29000 tfrecods\n",
      "INFO:absl:Wrote 30000 tfrecods\n",
      "INFO:absl:Wrote 31000 tfrecods\n",
      "INFO:absl:Wrote 32000 tfrecods\n",
      "INFO:absl:Wrote 33000 tfrecods\n",
      "INFO:absl:Wrote 34000 tfrecods\n",
      "INFO:absl:Wrote 35000 tfrecods\n",
      "INFO:absl:Wrote 36000 tfrecods\n",
      "INFO:absl:Wrote 37000 tfrecods\n",
      "INFO:absl:Wrote 38000 tfrecods\n",
      "INFO:absl:Wrote 39000 tfrecods\n",
      "INFO:absl:Wrote 40000 tfrecods\n",
      "INFO:absl:Wrote 41000 tfrecods\n",
      "INFO:absl:Wrote 42000 tfrecods\n",
      "INFO:absl:Wrote 43000 tfrecods\n",
      "INFO:absl:Wrote 44000 tfrecods\n",
      "INFO:absl:Wrote 45000 tfrecods\n",
      "INFO:absl:Wrote 46000 tfrecods\n",
      "INFO:absl:Wrote 47000 tfrecods\n",
      "INFO:absl:Wrote 48000 tfrecods\n",
      "INFO:absl:Wrote 49000 tfrecods\n",
      "INFO:absl:Wrote 50000 tfrecods\n",
      "INFO:absl:Wrote 51000 tfrecods\n",
      "INFO:absl:Wrote 52000 tfrecods\n",
      "INFO:absl:Wrote 53000 tfrecods\n",
      "INFO:absl:Wrote 54000 tfrecods\n",
      "INFO:absl:Wrote 55000 tfrecods\n",
      "INFO:absl:Wrote 56000 tfrecods\n",
      "INFO:absl:Wrote 57000 tfrecods\n",
      "INFO:absl:Wrote 58000 tfrecods\n",
      "INFO:absl:Wrote 59000 tfrecods\n",
      "INFO:absl:Wrote 60000 tfrecods\n",
      "INFO:absl:Wrote 61000 tfrecods\n",
      "INFO:absl:Wrote 62000 tfrecods\n",
      "INFO:absl:Wrote 63000 tfrecods\n",
      "INFO:absl:Wrote 64000 tfrecods\n",
      "INFO:absl:Wrote 65000 tfrecods\n",
      "INFO:absl:Wrote 66000 tfrecods\n",
      "INFO:absl:Wrote 67000 tfrecods\n",
      "INFO:absl:Wrote 68000 tfrecods\n",
      "INFO:absl:Wrote 69000 tfrecods\n",
      "INFO:absl:Wrote 70000 tfrecods\n",
      "INFO:absl:Wrote 71000 tfrecods\n",
      "INFO:absl:Wrote 72000 tfrecods\n",
      "INFO:absl:Wrote 73000 tfrecods\n",
      "INFO:absl:Wrote 74000 tfrecods\n",
      "INFO:absl:Wrote 75000 tfrecods\n",
      "INFO:absl:Wrote 76000 tfrecods\n",
      "INFO:absl:Wrote 77000 tfrecods\n",
      "INFO:absl:Wrote 78000 tfrecods\n",
      "INFO:absl:Wrote 79000 tfrecods\n",
      "INFO:absl:Wrote 80000 tfrecods\n",
      "INFO:absl:Wrote 81000 tfrecods\n",
      "INFO:absl:Wrote 82000 tfrecods\n",
      "INFO:absl:Wrote 83000 tfrecods\n",
      "INFO:absl:Wrote 84000 tfrecods\n",
      "INFO:absl:Wrote 85000 tfrecods\n",
      "INFO:absl:Wrote 86000 tfrecods\n",
      "INFO:absl:Wrote 87000 tfrecods\n",
      "INFO:absl:Wrote 88000 tfrecods\n",
      "INFO:absl:Wrote 89000 tfrecods\n",
      "INFO:absl:Wrote 90000 tfrecods\n",
      "INFO:absl:Wrote 91000 tfrecods\n",
      "INFO:absl:Wrote 92000 tfrecods\n",
      "INFO:absl:Wrote 93000 tfrecods\n",
      "INFO:absl:Wrote 94000 tfrecods\n",
      "INFO:absl:Wrote 95000 tfrecods\n",
      "INFO:absl:Wrote 96000 tfrecods\n",
      "INFO:absl:Wrote 97000 tfrecods\n",
      "INFO:absl:Wrote 98000 tfrecods\n",
      "INFO:absl:Wrote 99000 tfrecods\n",
      "INFO:absl:Wrote 100000 tfrecods\n",
      "INFO:absl:Wrote 101000 tfrecods\n",
      "INFO:absl:Wrote 102000 tfrecods\n",
      "INFO:absl:Wrote 103000 tfrecods\n",
      "INFO:absl:Wrote 104000 tfrecods\n",
      "INFO:absl:Wrote 105000 tfrecods\n",
      "INFO:absl:Wrote 106000 tfrecods\n",
      "INFO:absl:Wrote 107000 tfrecods\n",
      "INFO:absl:Wrote 108000 tfrecods\n",
      "INFO:absl:Wrote 109000 tfrecods\n",
      "INFO:absl:Wrote 110000 tfrecods\n",
      "INFO:absl:Wrote 111000 tfrecods\n",
      "INFO:absl:Wrote 112000 tfrecods\n",
      "INFO:absl:Wrote 113000 tfrecods\n",
      "INFO:absl:Wrote 114000 tfrecods\n",
      "INFO:absl:Wrote 115000 tfrecods\n",
      "INFO:absl:Wrote 116000 tfrecods\n",
      "INFO:absl:Wrote 117000 tfrecods\n",
      "INFO:absl:Wrote 118000 tfrecods\n",
      "INFO:absl:Wrote 119000 tfrecods\n",
      "INFO:absl:Wrote 120000 tfrecods\n",
      "INFO:absl:Wrote 121000 tfrecods\n",
      "INFO:absl:Wrote 122000 tfrecods\n",
      "INFO:absl:Wrote 123000 tfrecods\n",
      "INFO:absl:Wrote 124000 tfrecods\n",
      "INFO:absl:Wrote 125000 tfrecods\n",
      "INFO:absl:Wrote 126000 tfrecods\n",
      "INFO:absl:Wrote 127000 tfrecods\n",
      "INFO:absl:Wrote 128000 tfrecods\n",
      "INFO:absl:Wrote 129000 tfrecods\n",
      "INFO:absl:Wrote 130000 tfrecods\n",
      "INFO:absl:Wrote 131000 tfrecods\n",
      "INFO:absl:Wrote 132000 tfrecods\n",
      "INFO:absl:Wrote 133000 tfrecods\n",
      "INFO:absl:Wrote 134000 tfrecods\n",
      "INFO:absl:Wrote 135000 tfrecods\n",
      "INFO:absl:Wrote 136000 tfrecods\n",
      "INFO:absl:Wrote 137000 tfrecods\n",
      "INFO:absl:Wrote 138000 tfrecods\n",
      "INFO:absl:Wrote 139000 tfrecods\n",
      "INFO:absl:Wrote 140000 tfrecods\n",
      "INFO:absl:Wrote 141000 tfrecods\n",
      "INFO:absl:Wrote 142000 tfrecods\n",
      "INFO:absl:Wrote 143000 tfrecods\n",
      "INFO:absl:Wrote 144000 tfrecods\n",
      "INFO:absl:Wrote 145000 tfrecods\n",
      "INFO:absl:Wrote 146000 tfrecods\n",
      "INFO:absl:Wrote 147000 tfrecods\n",
      "INFO:absl:Wrote 148000 tfrecods\n",
      "INFO:absl:Wrote 149000 tfrecods\n",
      "INFO:absl:Wrote 150000 tfrecods\n",
      "INFO:absl:Wrote 151000 tfrecods\n",
      "INFO:absl:Wrote 152000 tfrecods\n",
      "INFO:absl:Wrote 153000 tfrecods\n",
      "INFO:absl:Wrote 154000 tfrecods\n",
      "INFO:absl:Wrote 155000 tfrecods\n",
      "INFO:absl:Wrote 156000 tfrecods\n",
      "INFO:absl:Wrote 157000 tfrecods\n",
      "INFO:absl:Wrote 158000 tfrecods\n",
      "INFO:absl:Wrote 159000 tfrecods\n",
      "INFO:absl:Wrote 160000 tfrecods\n",
      "INFO:absl:Wrote 161000 tfrecods\n",
      "INFO:absl:Wrote 162000 tfrecods\n",
      "INFO:absl:Wrote 163000 tfrecods\n",
      "INFO:absl:Wrote 164000 tfrecods\n",
      "INFO:absl:Wrote 165000 tfrecods\n",
      "INFO:absl:Wrote 166000 tfrecods\n",
      "INFO:absl:Wrote 167000 tfrecods\n",
      "INFO:absl:Wrote 168000 tfrecods\n",
      "INFO:absl:Wrote 169000 tfrecods\n",
      "INFO:absl:Wrote 170000 tfrecods\n",
      "INFO:absl:Wrote 171000 tfrecods\n",
      "INFO:absl:Wrote 172000 tfrecods\n",
      "INFO:absl:Wrote 173000 tfrecods\n",
      "INFO:absl:Wrote 174000 tfrecods\n",
      "INFO:absl:Wrote 175000 tfrecods\n",
      "INFO:absl:Wrote 176000 tfrecods\n",
      "INFO:absl:Wrote 177000 tfrecods\n",
      "INFO:absl:Wrote 178000 tfrecods\n",
      "INFO:absl:Wrote 179000 tfrecods\n",
      "INFO:absl:Wrote 180000 tfrecods\n",
      "INFO:absl:Wrote 181000 tfrecods\n",
      "INFO:absl:Wrote 182000 tfrecods\n",
      "INFO:absl:Wrote 183000 tfrecods\n",
      "INFO:absl:Wrote 184000 tfrecods\n",
      "INFO:absl:Wrote 185000 tfrecods\n",
      "INFO:absl:Wrote 186000 tfrecods\n",
      "INFO:absl:Wrote 187000 tfrecods\n",
      "INFO:absl:Wrote 188000 tfrecods\n",
      "INFO:absl:Wrote 189000 tfrecods\n",
      "INFO:absl:Wrote 190000 tfrecods\n",
      "INFO:absl:Wrote 191000 tfrecods\n",
      "INFO:absl:Wrote 192000 tfrecods\n",
      "INFO:absl:Wrote 193000 tfrecods\n",
      "INFO:absl:Wrote 194000 tfrecods\n",
      "INFO:absl:Wrote 195000 tfrecods\n",
      "INFO:absl:Wrote 196000 tfrecods\n",
      "INFO:absl:Wrote 197000 tfrecods\n",
      "INFO:absl:Wrote 198000 tfrecods\n",
      "INFO:absl:Wrote 199000 tfrecods\n",
      "INFO:absl:Wrote 200000 tfrecods\n",
      "INFO:absl:Wrote 201000 tfrecods\n",
      "INFO:absl:Wrote 202000 tfrecods\n",
      "INFO:absl:Wrote 203000 tfrecods\n",
      "INFO:absl:Wrote 204000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 204045\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=64\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['document'])[: encoder_max_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        decoder_input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['summary'])[: decoder_max_length-2] + [tokenizer.sep_token]\n",
    "        decoder_input_ids = tokenizer.convert_tokens_to_ids(decoder_input_ids)\n",
    "        decoder_input_type_ids = [0] * len(decoder_input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['encoder_input_ids'] = input_ids\n",
    "        result['encoder_input_mask'] = input_mask\n",
    "        result['encoder_input_type_ids'] = input_type_ids\n",
    "        result['decoder_input_ids'] = decoder_input_ids[:-1] # except last word\n",
    "        result['decoder_input_type_ids'] = decoder_input_type_ids[:-1] # except last word\n",
    "        \n",
    "        result['labels'] = decoder_input_ids[1:] # not including first word\n",
    "        result['labels_mask'] = [1] * len(decoder_input_ids[1:])\n",
    "        \n",
    "        # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"encoder_input_ids\": (\"var_len\", \"int\"),\n",
    "    \"encoder_input_mask\": (\"var_len\", \"int\"),\n",
    "    \"encoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"decoder_input_ids\": (\"var_len\", \"int\"),\n",
    "    \"decoder_input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "    \"labels_mask\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/bbc_xsum/roberta/train'\n",
    "tfrecord_filename = 'bbc_xsum'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['encoder_input_ids', 'encoder_input_type_ids', 'encoder_input_mask', 'decoder_input_ids', 'decoder_input_type_ids']\n",
    "y_keys = ['labels', 'labels_mask']\n",
    "batch_size = 8\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_input_ids': <tf.Tensor: shape=(8, 31), dtype=int32, numpy=\n",
      "array([[    0,   133,  3940,    23, 19931,   230,  4183,  4869,   108,\n",
      "         5494, 13700,    13,  5345,    34,   554,    63, 21727,     4,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [    0,   250,    92,   471,  3254,    16,   145,  2952,    13,\n",
      "           10,   881, 25209,   334,    15,    10,  5411,  2946,  1602,\n",
      "           25,   145,    15,     5,  3543,     9,     5,   232,     4,\n",
      "            0,     0,     0,     0],\n",
      "       [    0,  4030,   291, 17055,  8905,    33,    57,  2942,    11,\n",
      "         1667,     9, 12426,    11,    10,  2311,     7,   847,  2078,\n",
      "         4971,    11,  4860,   911,     4,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [    0, 16764,  3121, 13398,    34,  1147, 23750,    10,   780,\n",
      "         1612,  1967,  1887,     7, 21662,    41,  1704, 16010,    18,\n",
      "         2040,     9,  2429, 25711,     4,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [    0,   250,   320,  9716,    34,  1550,    10,  8963,    12,\n",
      "         7800,    36, 24675,  7203,    43,  1656,    31,     5, 25660,\n",
      "            9,     5, 28283, 24070,    23, 11215, 20481,     7, 13077,\n",
      "            4,     0,     0,     0],\n",
      "       [    0,  4993,  7015,  8405,    79,    21,    45,   145,  1432,\n",
      "            6,   854,  1742,  5930,     5,  8751,     7,    69,   745,\n",
      "           15,    10,  6787,  6625,    11, 14794,     8,  3475,     7,\n",
      "           69, 16425,  3269,     4],\n",
      "       [    0,  4688,   803,    88,     5,   744,     9,    10,  1928,\n",
      "         2421,    23, 21024,  5481,  2573,  2392,    21,    22,  3654,\n",
      "         2564,    13,  3508,   845,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [    0,   133,   346,     9,    82,  9871,    30,   997,     6,\n",
      "         3050,    50, 19913,  1348,    10,   638,   239,     9,   823,\n",
      "         1191,   153,   198,     5,   232,    11,   777,     6,    10,\n",
      "         2604,   266,   161,     4]], dtype=int32)>, 'decoder_input_type_ids': <tf.Tensor: shape=(8, 31), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'encoder_input_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[    0,   133, 12813, ...,     0,     0,     0],\n",
      "       [    0, 27918,   858, ...,   486,  9465,     2],\n",
      "       [    0, 23818,  4822, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    0,   133,   664, ...,    12,   463,     2],\n",
      "       [    0, 34079, 16294, ...,     0,     0,     0],\n",
      "       [    0,   133,  3780, ...,     0,     0,     0]], dtype=int32)>, 'encoder_input_mask': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'encoder_input_type_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>} {'labels': <tf.Tensor: shape=(8, 31), dtype=int32, numpy=\n",
      "array([[  133,  3940,    23, 19931,   230,  4183,  4869,   108,  5494,\n",
      "        13700,    13,  5345,    34,   554,    63, 21727,     4,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [  250,    92,   471,  3254,    16,   145,  2952,    13,    10,\n",
      "          881, 25209,   334,    15,    10,  5411,  2946,  1602,    25,\n",
      "          145,    15,     5,  3543,     9,     5,   232,     4,     2,\n",
      "            0,     0,     0,     0],\n",
      "       [ 4030,   291, 17055,  8905,    33,    57,  2942,    11,  1667,\n",
      "            9, 12426,    11,    10,  2311,     7,   847,  2078,  4971,\n",
      "           11,  4860,   911,     4,     2,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [16764,  3121, 13398,    34,  1147, 23750,    10,   780,  1612,\n",
      "         1967,  1887,     7, 21662,    41,  1704, 16010,    18,  2040,\n",
      "            9,  2429, 25711,     4,     2,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [  250,   320,  9716,    34,  1550,    10,  8963,    12,  7800,\n",
      "           36, 24675,  7203,    43,  1656,    31,     5, 25660,     9,\n",
      "            5, 28283, 24070,    23, 11215, 20481,     7, 13077,     4,\n",
      "            2,     0,     0,     0],\n",
      "       [ 4993,  7015,  8405,    79,    21,    45,   145,  1432,     6,\n",
      "          854,  1742,  5930,     5,  8751,     7,    69,   745,    15,\n",
      "           10,  6787,  6625,    11, 14794,     8,  3475,     7,    69,\n",
      "        16425,  3269,     4,     2],\n",
      "       [ 4688,   803,    88,     5,   744,     9,    10,  1928,  2421,\n",
      "           23, 21024,  5481,  2573,  2392,    21,    22,  3654,  2564,\n",
      "           13,  3508,   845,     2,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0],\n",
      "       [  133,   346,     9,    82,  9871,    30,   997,     6,  3050,\n",
      "           50, 19913,  1348,    10,   638,   239,     9,   823,  1191,\n",
      "          153,   198,     5,   232,    11,   777,     6,    10,  2604,\n",
      "          266,   161,     4,     2]], dtype=int32)>, 'labels_mask': <tf.Tensor: shape=(8, 31), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# Look at inputs, labels\n",
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Roberta2Roberta (Encoder Decoder Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwride mask_mode with user_defined\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Overwride mask_mode with causal\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n",
      "INFO:absl:Encoder loaded succesfully from /mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/roberta-base/\n",
      "INFO:absl:Warm started decoder 197/202 variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "model_layer, model, config = EncoderDecoderModel(model_name='roberta-base', \n",
    "                                                 is_training=True, \n",
    "                                                 encoder_checkpoint_dir='/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/roberta-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 2D (batch_size x sequence_length)\n",
    "* logits: 3D (batch_size x sequence_length x vocab_size)\n",
    "* label_weights: 2D (batch_size x sequence_length) # we don't want all words in the sequence to have loss so, we mask them and don't calculate for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    return cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'], \n",
    "                                      label_weights=y_true_dict['labels_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer\n",
    "\n",
    "**PRO TIP**: These models are very sensitive to optimizer, especially learning rates. So, make sure you play around to find a good combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 204045\n",
    "learning_rate   = 1e-05\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "adam_beta2=0.997\n",
    "adam_epsilon=1e-09\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type = optimizer_type, \n",
    "                                                learning_rate_type = 'linear',\n",
    "                                                adam_beta_2 = adam_beta2, \n",
    "                                                adam_epsilon = adam_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 33s 7s/step - loss: 18.2898 - token_logits_loss: 18.2898\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 32s 6s/step - loss: 18.2744 - token_logits_loss: 18.2744\n"
     ]
    }
   ],
   "source": [
    "# Keras Fit\n",
    "# Change epochs and steps_per_epoch for full training\n",
    "# If steps_per_epoch is not familiar, dont use it, provide only epochs\n",
    "\n",
    "keras_loss_fn = {'token_logits': lm_loss\n",
    "                }\n",
    "model.compile2(optimizer=optimizer, \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn\n",
    "              )\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training\n",
    "# You can provide gradient_accumulation_steps if required\n",
    "# I find it hurting the performance, don't know why\n",
    "history = SimpleTrainer(model = model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = lm_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)\n",
    "model.save_checkpoint(\"../OFFICIAL_MODELS/bbc_xsum/roberta2roberta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"../OFFICIAL_MODELS/bbc_xsum/roberta2roberta\"\n",
    "model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model for Text Genration (Auto-Regressive)\n",
    "\n",
    "1. For any model to use for auto-regressive tasks we have to provide **\"pipeline_mode='auto-regressive'\"**\n",
    "\n",
    "tf-transformers will handle everything for you internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwride mask_mode with user_defined\n",
      "INFO:absl:We are overwriding `is_training` is False to                         `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Overwride mask_mode with causal\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 768), dtype=float32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_all_cache_key ---> Tensor(\"all_cache_key:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:decoder_all_cache_value ---> Tensor(\"all_cache_value:0\", shape=(None, None, 12, None, 64), dtype=float32)\n",
      "INFO:absl:encoder_hidden_states ---> Tensor(\"encoder_hidden_states:0\", shape=(None, None, 768), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x7f88ac6b7220> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f886029f4f0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.EncoderDecoder object at 0x7f88ac6b7220> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f886029f4f0>).\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Load the model by disabling dropout and add pipeline_mode = 'auto-regressive'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "model_layer, model, config = EncoderDecoderModel(model_name='roberta-base', \n",
    "                                                 is_training=False, \n",
    "                                                 pipeline_mode='auto-regressive'\n",
    "                                                )\n",
    "\n",
    "model.load_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model as serialized version\n",
    "\n",
    "This is very important, because serialized model is significantly faster.\n",
    "tf-transfomers provide **save_as_serialize_module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-transformers provide \"save_as_serialize_module\" for this\n",
    "model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir))\n",
    "\n",
    "loaded = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processed  10000 examples so far\n",
      "INFO:absl:Total individual observations/examples written is 11332\n"
     ]
    }
   ],
   "source": [
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/xsum/\")\n",
    "dev_examples = examples['validation']\n",
    "\n",
    "encoder_max_length=512\n",
    "decoder_max_length=64\n",
    "\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['document'])[: encoder_max_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "        \n",
    "        result = {}\n",
    "        result['encoder_input_ids'] = input_ids\n",
    "        result['encoder_input_mask'] = input_mask\n",
    "        result['encoder_input_type_ids'] = input_type_ids\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_input_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[   0,  133,  247, ...,    0,    0,    0],\n",
      "       [   0, 3084,  758, ..., 1341,  239,    2],\n",
      "       [   0, 1121,   10, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   0,  133,  517, ..., 8874,    9,    2],\n",
      "       [   0,  500,  636, ...,    0,    0,    0],\n",
      "       [   0,  487, 1116, ...,    0,    0,    0]], dtype=int32)>, 'encoder_input_mask': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'encoder_input_type_ids': <tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for (batch_inputs) in dev_datasetset.take(1):\n",
    "    print(batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Generation for dev dataset\n",
    "\n",
    "1. For **EncoderDecoder** models like Roberta2Roberts, Bert2GPT, t5, BART use **TextDecoderSeq2Seq**\n",
    "2. For **Encoder** only models like GPT2, BERT, Roberta use **TextDecoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "\n",
    "# You can pass model = (saved_model or keras model)\n",
    "# Saved model take 1200 seconds \n",
    "# Keras Model take 2300 seconds\n",
    "\n",
    "# Thats why always choose saved_model for faster inference in production\n",
    "decoder = TextDecoderSeq2Seq(model=loaded, \n",
    "                            decoder_start_token_id=tokenizer.cls_token_id, # Decoder always expect a start_token_id\n",
    "                            decoder_input_type_ids=0 # If you have input_type_ids\n",
    "                            )\n",
    "\n",
    "# Greedy Decoding\n",
    "start_time = time.time()\n",
    "predicted_summaries = []\n",
    "for batch_inputs in dev_dataset:\n",
    "    model_outputs = decoder.decode(batch_inputs, \n",
    "                   mode='greedy', \n",
    "                   max_iterations=64, \n",
    "                   eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "    output_summaries = tokenizer.batch_decode(tf.squeeze(model_outputs['predicted_ids'], 1), skip_special_tokens=True)\n",
    "    predicted_summaries.extend(output_summaries)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ROUGE score using Huggingface datasets metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_summaries = [item['summary'] for item in dev_examples]\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "rouge_output2 = rouge.compute(predictions=predicted_summaries, references=original_summaries, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "rouge_output1 = rouge.compute(predictions=predicted_summaries, references=original_summaries, rouge_types=[\"rouge1\"])[\"rouge1\"].mid\n",
    "rouge_outputL = rouge.compute(predictions=predicted_summaries, references=original_summaries, rouge_types=[\"rougeL\"])[\"rougeL\"].mid\n",
    "print(\"Rouge1\", rouge_output1)\n",
    "print(\"Rouge2\", rouge_output2)\n",
    "print(\"RougeL\", rouge_outputL)\n",
    "\n",
    "Rouge1 Score(precision=0.4030931388183621, recall=0.36466254213804195, fmeasure=0.37530511219642493)\n",
    "Rouge2 Score(precision=0.16782261295821255, recall=0.15203057122838165, fmeasure=0.1561568579032115)\n",
    "RougeL Score(precision=0.327351036176015, recall=0.2969254630660535, fmeasure=0.30522124104859427)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ROUGE score using Google rouge_score library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from rouge_score import scoring\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeLsum\"], use_stemmer=True)\n",
    "aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "for i in range(len(original_summaries)):\n",
    "    score = scorer.score(original_summaries[i], predicted_summaries[i])\n",
    "    aggregator.add_scores(score)\n",
    "    \n",
    "print(\"Aggregated scores\", aggregator.aggregate())\n",
    "\n",
    "{'rouge1': AggregateScore(low=Score(precision=0.4140161871825283, recall=0.37430234257946526, fmeasure=0.38546441367482415), mid=Score(precision=0.4170448579572875, recall=0.377157245705547, fmeasure=0.3882117449428194), high=Score(precision=0.4201437437963473, recall=0.37995542626327267, fmeasure=0.39091534567006364)),\n",
    " 'rouge2': AggregateScore(low=Score(precision=0.1691630552674417, recall=0.1529611842223162, fmeasure=0.1573395483096894),mid=Score(precision=0.17186887531131723, recall=0.1556336324896489, fmeasure=0.15993720424744032), high=Score(precision=0.17473545118625014, recall=0.1582221241761583, fmeasure=0.16259483307298692)),\n",
    " 'rougeLsum': AggregateScore(low=Score(precision=0.33240856683967057, recall=0.301583552325229, fmeasure=0.3101081853974168), mid=Score(precision=0.33521687141849427, recall=0.3040523188928863, fmeasure=0.3125425409744611), high=Score(precision=0.33844188019812493, recall=0.3066532822640133, fmeasure=0.31522376216006714))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Production\n",
    "1. Lets see how we can deploy this model in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.text import TextDecoderSeq2Seq\n",
    "from tf_transformers.data import pad_dataset\n",
    "\n",
    "# 1. Load Saved Model\n",
    "loaded = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))\n",
    "\n",
    "# 2. Initiate a decode object\n",
    "decoder = TextDecoderSeq2Seq(model=loaded, \n",
    "                            decoder_start_token_id=tokenizer.cls_token_id, # Decoder always expect a start_token_id\n",
    "                            decoder_input_type_ids=0 # If you have input_type_ids\n",
    "                            )\n",
    "\n",
    "# 3. Convert text to inputs\n",
    "\n",
    "# Tokenizer fn convert text -> model inputs\n",
    "# Make sure you return dict with key-> list of list\n",
    "# pad_dataset is a decorator, hich will automatically taken care of padding\n",
    "\n",
    "# If you want to write your own function, please. model expect inputs in a specifed format thats all.\n",
    "@pad_dataset\n",
    "def tokenizer_fn(texts):\n",
    "    input_ids      = []\n",
    "    input_mask     = []\n",
    "    input_type_ids = []\n",
    "    for text in texts:\n",
    "        input_ids_ex = [tokenizer.cls_token] + tokenizer.tokenize(text)[: encoder_max_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids_ex = tokenizer.convert_tokens_to_ids(input_ids_ex)\n",
    "        input_mask_ex = [1] * len(input_ids_ex)\n",
    "        input_type_ids_ex = [0] * len(input_ids_ex)\n",
    "        input_ids.append(input_ids_ex)\n",
    "        input_mask.append(input_mask_ex)\n",
    "        input_type_ids.append(input_type_ids_ex)\n",
    "        \n",
    "    result = {}\n",
    "    result['encoder_input_ids'] = input_ids\n",
    "    result['encoder_input_mask'] = input_mask\n",
    "    result['encoder_input_type_ids'] = input_type_ids\n",
    "    \n",
    "    return result\n",
    "        \n",
    "    \n",
    "# 4. Examples\n",
    "text1 = '''Tulips (Tulipa) form a genus of spring-blooming perennial herbaceous bulbiferous geophytes (having bulbs as storage organs). The flowers are usually large, showy and brightly colored, generally red, pink, yellow, or white (usually in warm colors). They often have a different colored blotch at the base of the tepals (petals and sepals, collectively), internally. Because of a degree of variability within the populations, and a long history of cultivation, classification has been complex and controversial. The tulip is a member of the lily family, Liliaceae, along with 14 other genera, where it is most closely related to Amana, Erythronium and Gagea in the tribe Lilieae. There are about 75 species, and these are divided among four subgenera. The name \"tulip\" is thought to be derived from a Persian word for turban, which it may have been thought to resemble. Tulips originally were found in a band stretching from Southern Europe to Central Asia, but since the seventeenth century have become widely naturalised and cultivated (see map). In their natural state they are adapted to steppes and mountainous areas with temperate climates. Flowering in the spring, they become dormant in the summer once the flowers and leaves die back, emerging above ground as a shoot from the underground bulb in early spring.\n",
    "\n",
    "Originally growing wild in the valleys of the Tian Shan Mountains, tulips were cultivated in Constantinople as early as 1055. By the 15th century, tulips were among the most prized flowers; becoming the symbol of the Ottomans.[2] While tulips had probably been cultivated in Persia from the tenth century, they did not come to the attention of the West until the sixteenth century, when Western diplomats to the Ottoman court observed and reported on them. They were rapidly introduced into Europe and became a frenzied commodity during Tulip mania. Tulips were frequently depicted in Dutch Golden Age paintings, and have become associated with the Netherlands, the major producer for world markets, ever since. In the seventeenth century Netherlands, during the time of the Tulip mania, an infection of tulip bulbs by the tulip breaking virus created variegated patterns in the tulip flowers that were much admired and valued. While truly broken tulips do not exist anymore, the closest available specimens today are part of the group known as the Rembrandts  so named because Rembrandt painted some of the most admired breaks of his time.[3]'''\n",
    "\n",
    "\n",
    "text2 = '''By any yardstick, the 2013 blockbuster Drishyam is a hard act to follow. Writer-director Jeethu Josephs crime thriller starring Mohanlal, Meena, Asha Sharath and Siddique was so well-rounded in the writing and execution of its murder-and-subsequent-cover-up mystery and such a box-office superhit that it was remade in Tamil, Telugu, Hindi and Kannada, headlined by some of the biggest male stars of those industries, in addition to foreign revisitations in Sinhalese and Mandarin.\n",
    "\n",
    "At the time, Jeethu was questioned about his script drawing on Japanese novelist Keigo Higashinos The Devotion of Suspect X, but he denied the charge and said he was inspired instead by a real-life incident. Be that as it may, Drishyam 2: The Resumption is all the redemption he needs. In a country that does not have a great track record with whodunnits, pulling off a brilliant howdunnit and howhegotawaywithit like Drishyam was an achievement. Returning with a howhesstillgettingawaywithit and actually pulling it off is nothing short of incredible.\n",
    "\n",
    "Drishyam 2 is a surprisingly satisfying sequel to a spectacular first film.\n",
    "\n",
    "Jeethu Josephs new crime drama is set in the same Kerala town where the events of its precursor took place. Georgekutty (Mohanlal) is now the owner of a cinema theatre. His prosperity is reflected in the larger, posher house he currently occupies with his wife Rani (Meena) and their daughters Anju (Ansiba) and Anu (Esther Anil) on the same land where they earlier lived. He is still movie crazy. Rani and he are still a committed couple yet constantly sniping at each other as before. And they are still a rock-solid team in the upbringing of their girls.\n",
    "\n",
    "The difference between then and now is twofold. First, the townsfolk had backed the family when IG Geetha Prabhakar (Asha Sharath) got after them on the suspicion that they killed her son. They are not so supportive any more, driven as they are by jealousy at Georgekuttys rise in life.\n",
    "\n",
    "Second, the experiences of Drishyam have had a deep psychological impact on both Rani and Anju. Rani is tormented by Georgekuttys refusal to ever discuss what happened back then. The first half of Drishyam 2 constructs their continuing trauma and gradually establishes the fact that the police never gave up on the case. The second half is about the resumed investigation.'''\n",
    "\n",
    "# 5. Choose the type of decoding\n",
    "batch_inputs = tokenizer_fn([text1, text2])\n",
    "model_outputs = decoder.decode(batch_inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=64, \n",
    "               eos_id=tokenizer.sep_token_id)\n",
    "\n",
    "output_summaries = tokenizer.batch_decode(tf.squeeze(model_outputs['predicted_ids'], 1), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The world's most famous flower, the tulip, is one of the most important and important species of flower.\",\n",
       " 'The sequel to the cult film Drishyam is a timely sequel to one of the most popular films in the country.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced\n",
    "\n",
    "**TextDecoderSerializable** internally uses for loop.\n",
    "\n",
    "Can we do better. If we could use ```tf.while_loop```, we can save the whole model as serialized.\n",
    "which no only improves speed, but also make life much easier in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the end-to-end decoder as seriazed model\n",
    "\n",
    "from tf_transformers.text import TextDecoderSerializableSeq2Seq\n",
    "from tf_transformers.core import LegacyModule\n",
    "\n",
    "decoder_layer = TextDecoderSerializableSeq2Seq(model=model, \n",
    "                            decoder_start_token_id=tokenizer.cls_token_id, # Decoder always expect a start_token_id\n",
    "                            decoder_input_type_ids=0, # If you have input_type_ids\n",
    "                            mode=\"greedy\",\n",
    "                            max_iterations=64,\n",
    "                            eos_id=tokenizer.sep_token_id\n",
    "                            )\n",
    "decoder_model = decoder_layer.get_model()\n",
    "decoder_module = LegacyModule(decoder_model)\n",
    "decoder_module.save(\"{}/saved_decoder_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Production (Advanced) - Just 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Load serialized model\n",
    "decoder_serialized = tf.saved_model.load(\"{}/saved_decoder_model\".format(model_save_dir))\n",
    "\n",
    "# 2. text to inputs\n",
    "model_outputs2 = decoder_serialized(**batch_inputs)\n",
    "output_summaries2 = tokenizer.batch_decode(tf.squeeze(model_outputs2['predicted_ids'], 1), skip_special_tokens=True)\n",
    "\n",
    "# Output summaries matches with TextDecoderSerializableSeq2Seq\n",
    "assert(output_summaries == output_summaries2)\n",
    "\n",
    "# Succesful :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
