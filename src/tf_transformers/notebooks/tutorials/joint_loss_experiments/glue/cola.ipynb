{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/home/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import AlbertModel\n",
    "from tf_transformers.tasks import Classification_Model\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "\n",
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load COLA dataset from Huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/glue/cola/\")\n",
    "train_examples = examples[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 1, 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\"}\n"
     ]
    }
   ],
   "source": [
    "for item in train_examples:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 8551\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=128\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "    \"input_mask\": (\"var_len\", \"int\"),\n",
    "    \"input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../../OFFICIAL_TFRECORDS/glue/alberta/cola/train'\n",
    "tfrecord_filename = 'cola'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "batch_size = 32\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Albert V2 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "\n",
    "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False\n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
    "# model -> legacyModel inherited from tf.keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classification_layer = Classification_Model(model=model,\n",
    "                                      num_classes=2,\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "classification_model = classification_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save up memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del classification_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # class indices\n",
    "* logits: 2D (batch_size x num_classes)\n",
    "\n",
    "**Joint loss** - We minimze loss over each hidden layer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(labels, axis=1)))\n",
    "    return loss\n",
    "\n",
    "def joint_loss(y_true_dict, y_pred_dict):\n",
    "    layer_loss = []\n",
    "    for class_logits in y_pred_dict['class_logits']:\n",
    "        loss = loss_fn(y_true_dict['labels'], class_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 8500\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 5s 490ms/step - loss: 0.6858 - class_logits_loss: 0.6858\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 6s 634ms/step - loss: 0.6583 - class_logits_loss: 0.6583\n"
     ]
    }
   ],
   "source": [
    "# # Compile\n",
    "keras_loss_fn = {'class_logits': joint_loss}\n",
    "classification_model.compile2(optimizer=optimizer, \n",
    "                             loss=None, \n",
    "                             custom_loss=keras_loss_fn)\n",
    "# Change steps per epoch to large value/ ignore it completely to train\n",
    "# on full dataset\n",
    "history = classification_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = SimpleTrainer(model = classification_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = joint_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"../../OFFICIAL_MODELS/glue/cola/albert\"\n",
    "classification_model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1043\n"
     ]
    }
   ],
   "source": [
    "dev_examples = examples['validation']\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, shuffle=False, x_keys=x_keys, y_keys=y_keys, batch_size=32, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dev dataset - Mathews Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 12\n",
    "predictions_per_layer = {i:[] for i in range(num_hidden_layers)}\n",
    "\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dev_dataset:\n",
    "    model_outputs = classification_model(batch_inputs)['class_logits']\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        predictions_per_layer[i].append(tf.argmax(model_outputs[i], axis=1).numpy())\n",
    "    \n",
    "    original_labels.append(batch_labels['labels'].numpy())\n",
    "    \n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "eval_metrics = {}\n",
    "for i in range(num_hidden_layers):\n",
    "    eval_metrics[i] = matthews_corrcoef(np.hstack(predictions_per_layer[i]), np.hstack(original_labels))\n",
    "    print(i, eval_metrics[i])\n",
    "\n",
    "with open('eval_cola.json', 'w') as f:\n",
    "    json.dump(eval_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
