{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/home/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import AlbertModel\n",
    "from tf_transformers.tasks import Classification_Model\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "\n",
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNLI dataset from Huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/glue/sst2/\")\n",
    "train_examples = examples[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 1, 'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'sentence2': 'Weapons of Mass Destruction Found in Iraq.'}\n"
     ]
    }
   ],
   "source": [
    "for item in train_examples:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Wrote 9000 tfrecods\n",
      "INFO:absl:Wrote 10000 tfrecods\n",
      "INFO:absl:Wrote 11000 tfrecods\n",
      "INFO:absl:Wrote 12000 tfrecods\n",
      "INFO:absl:Wrote 13000 tfrecods\n",
      "INFO:absl:Wrote 14000 tfrecods\n",
      "INFO:absl:Wrote 15000 tfrecods\n",
      "INFO:absl:Wrote 16000 tfrecods\n",
      "INFO:absl:Wrote 17000 tfrecods\n",
      "INFO:absl:Wrote 18000 tfrecods\n",
      "INFO:absl:Wrote 19000 tfrecods\n",
      "INFO:absl:Wrote 20000 tfrecods\n",
      "INFO:absl:Wrote 21000 tfrecods\n",
      "INFO:absl:Wrote 22000 tfrecods\n",
      "INFO:absl:Wrote 23000 tfrecods\n",
      "INFO:absl:Wrote 24000 tfrecods\n",
      "INFO:absl:Wrote 25000 tfrecods\n",
      "INFO:absl:Wrote 26000 tfrecods\n",
      "INFO:absl:Wrote 27000 tfrecods\n",
      "INFO:absl:Wrote 28000 tfrecods\n",
      "INFO:absl:Wrote 29000 tfrecods\n",
      "INFO:absl:Wrote 30000 tfrecods\n",
      "INFO:absl:Wrote 31000 tfrecods\n",
      "INFO:absl:Wrote 32000 tfrecods\n",
      "INFO:absl:Wrote 33000 tfrecods\n",
      "INFO:absl:Wrote 34000 tfrecods\n",
      "INFO:absl:Wrote 35000 tfrecods\n",
      "INFO:absl:Wrote 36000 tfrecods\n",
      "INFO:absl:Wrote 37000 tfrecods\n",
      "INFO:absl:Wrote 38000 tfrecods\n",
      "INFO:absl:Wrote 39000 tfrecods\n",
      "INFO:absl:Wrote 40000 tfrecods\n",
      "INFO:absl:Wrote 41000 tfrecods\n",
      "INFO:absl:Wrote 42000 tfrecods\n",
      "INFO:absl:Wrote 43000 tfrecods\n",
      "INFO:absl:Wrote 44000 tfrecods\n",
      "INFO:absl:Wrote 45000 tfrecods\n",
      "INFO:absl:Wrote 46000 tfrecods\n",
      "INFO:absl:Wrote 47000 tfrecods\n",
      "INFO:absl:Wrote 48000 tfrecods\n",
      "INFO:absl:Wrote 49000 tfrecods\n",
      "INFO:absl:Wrote 50000 tfrecods\n",
      "INFO:absl:Wrote 51000 tfrecods\n",
      "INFO:absl:Wrote 52000 tfrecods\n",
      "INFO:absl:Wrote 53000 tfrecods\n",
      "INFO:absl:Wrote 54000 tfrecods\n",
      "INFO:absl:Wrote 55000 tfrecods\n",
      "INFO:absl:Wrote 56000 tfrecods\n",
      "INFO:absl:Wrote 57000 tfrecods\n",
      "INFO:absl:Wrote 58000 tfrecods\n",
      "INFO:absl:Wrote 59000 tfrecods\n",
      "INFO:absl:Wrote 60000 tfrecods\n",
      "INFO:absl:Wrote 61000 tfrecods\n",
      "INFO:absl:Wrote 62000 tfrecods\n",
      "INFO:absl:Wrote 63000 tfrecods\n",
      "INFO:absl:Wrote 64000 tfrecods\n",
      "INFO:absl:Wrote 65000 tfrecods\n",
      "INFO:absl:Wrote 66000 tfrecods\n",
      "INFO:absl:Wrote 67000 tfrecods\n",
      "INFO:absl:Wrote 68000 tfrecods\n",
      "INFO:absl:Wrote 69000 tfrecods\n",
      "INFO:absl:Wrote 70000 tfrecods\n",
      "INFO:absl:Wrote 71000 tfrecods\n",
      "INFO:absl:Wrote 72000 tfrecods\n",
      "INFO:absl:Wrote 73000 tfrecods\n",
      "INFO:absl:Wrote 74000 tfrecods\n",
      "INFO:absl:Wrote 75000 tfrecods\n",
      "INFO:absl:Wrote 76000 tfrecods\n",
      "INFO:absl:Wrote 77000 tfrecods\n",
      "INFO:absl:Wrote 78000 tfrecods\n",
      "INFO:absl:Wrote 79000 tfrecods\n",
      "INFO:absl:Wrote 80000 tfrecods\n",
      "INFO:absl:Wrote 81000 tfrecods\n",
      "INFO:absl:Wrote 82000 tfrecods\n",
      "INFO:absl:Wrote 83000 tfrecods\n",
      "INFO:absl:Wrote 84000 tfrecods\n",
      "INFO:absl:Wrote 85000 tfrecods\n",
      "INFO:absl:Wrote 86000 tfrecods\n",
      "INFO:absl:Wrote 87000 tfrecods\n",
      "INFO:absl:Wrote 88000 tfrecods\n",
      "INFO:absl:Wrote 89000 tfrecods\n",
      "INFO:absl:Wrote 90000 tfrecods\n",
      "INFO:absl:Wrote 91000 tfrecods\n",
      "INFO:absl:Wrote 92000 tfrecods\n",
      "INFO:absl:Wrote 93000 tfrecods\n",
      "INFO:absl:Wrote 94000 tfrecods\n",
      "INFO:absl:Wrote 95000 tfrecods\n",
      "INFO:absl:Wrote 96000 tfrecods\n",
      "INFO:absl:Wrote 97000 tfrecods\n",
      "INFO:absl:Wrote 98000 tfrecods\n",
      "INFO:absl:Wrote 99000 tfrecods\n",
      "INFO:absl:Wrote 100000 tfrecods\n",
      "INFO:absl:Wrote 101000 tfrecods\n",
      "INFO:absl:Wrote 102000 tfrecods\n",
      "INFO:absl:Wrote 103000 tfrecods\n",
      "INFO:absl:Wrote 104000 tfrecods\n",
      "INFO:absl:Wrote 105000 tfrecods\n",
      "INFO:absl:Wrote 106000 tfrecods\n",
      "INFO:absl:Wrote 107000 tfrecods\n",
      "INFO:absl:Wrote 108000 tfrecods\n",
      "INFO:absl:Wrote 109000 tfrecods\n",
      "INFO:absl:Wrote 110000 tfrecods\n",
      "INFO:absl:Wrote 111000 tfrecods\n",
      "INFO:absl:Wrote 112000 tfrecods\n",
      "INFO:absl:Wrote 113000 tfrecods\n",
      "INFO:absl:Wrote 114000 tfrecods\n",
      "INFO:absl:Wrote 115000 tfrecods\n",
      "INFO:absl:Wrote 116000 tfrecods\n",
      "INFO:absl:Wrote 117000 tfrecods\n",
      "INFO:absl:Wrote 118000 tfrecods\n",
      "INFO:absl:Wrote 119000 tfrecods\n",
      "INFO:absl:Wrote 120000 tfrecods\n",
      "INFO:absl:Wrote 121000 tfrecods\n",
      "INFO:absl:Wrote 122000 tfrecods\n",
      "INFO:absl:Wrote 123000 tfrecods\n",
      "INFO:absl:Wrote 124000 tfrecods\n",
      "INFO:absl:Wrote 125000 tfrecods\n",
      "INFO:absl:Wrote 126000 tfrecods\n",
      "INFO:absl:Wrote 127000 tfrecods\n",
      "INFO:absl:Wrote 128000 tfrecods\n",
      "INFO:absl:Wrote 129000 tfrecods\n",
      "INFO:absl:Wrote 130000 tfrecods\n",
      "INFO:absl:Wrote 131000 tfrecods\n",
      "INFO:absl:Wrote 132000 tfrecods\n",
      "INFO:absl:Wrote 133000 tfrecods\n",
      "INFO:absl:Wrote 134000 tfrecods\n",
      "INFO:absl:Wrote 135000 tfrecods\n",
      "INFO:absl:Wrote 136000 tfrecods\n",
      "INFO:absl:Wrote 137000 tfrecods\n",
      "INFO:absl:Wrote 138000 tfrecods\n",
      "INFO:absl:Wrote 139000 tfrecods\n",
      "INFO:absl:Wrote 140000 tfrecods\n",
      "INFO:absl:Wrote 141000 tfrecods\n",
      "INFO:absl:Wrote 142000 tfrecods\n",
      "INFO:absl:Wrote 143000 tfrecods\n",
      "INFO:absl:Wrote 144000 tfrecods\n",
      "INFO:absl:Wrote 145000 tfrecods\n",
      "INFO:absl:Wrote 146000 tfrecods\n",
      "INFO:absl:Wrote 147000 tfrecods\n",
      "INFO:absl:Wrote 148000 tfrecods\n",
      "INFO:absl:Wrote 149000 tfrecods\n",
      "INFO:absl:Wrote 150000 tfrecods\n",
      "INFO:absl:Wrote 151000 tfrecods\n",
      "INFO:absl:Wrote 152000 tfrecods\n",
      "INFO:absl:Wrote 153000 tfrecods\n",
      "INFO:absl:Wrote 154000 tfrecods\n",
      "INFO:absl:Wrote 155000 tfrecods\n",
      "INFO:absl:Wrote 156000 tfrecods\n",
      "INFO:absl:Wrote 157000 tfrecods\n",
      "INFO:absl:Wrote 158000 tfrecods\n",
      "INFO:absl:Wrote 159000 tfrecods\n",
      "INFO:absl:Wrote 160000 tfrecods\n",
      "INFO:absl:Wrote 161000 tfrecods\n",
      "INFO:absl:Wrote 162000 tfrecods\n",
      "INFO:absl:Wrote 163000 tfrecods\n",
      "INFO:absl:Wrote 164000 tfrecods\n",
      "INFO:absl:Wrote 165000 tfrecods\n",
      "INFO:absl:Wrote 166000 tfrecods\n",
      "INFO:absl:Wrote 167000 tfrecods\n",
      "INFO:absl:Wrote 168000 tfrecods\n",
      "INFO:absl:Wrote 169000 tfrecods\n",
      "INFO:absl:Wrote 170000 tfrecods\n",
      "INFO:absl:Wrote 171000 tfrecods\n",
      "INFO:absl:Wrote 172000 tfrecods\n",
      "INFO:absl:Wrote 173000 tfrecods\n",
      "INFO:absl:Wrote 174000 tfrecods\n",
      "INFO:absl:Wrote 175000 tfrecods\n",
      "INFO:absl:Wrote 176000 tfrecods\n",
      "INFO:absl:Wrote 177000 tfrecods\n",
      "INFO:absl:Wrote 178000 tfrecods\n",
      "INFO:absl:Wrote 179000 tfrecods\n",
      "INFO:absl:Wrote 180000 tfrecods\n",
      "INFO:absl:Wrote 181000 tfrecods\n",
      "INFO:absl:Wrote 182000 tfrecods\n",
      "INFO:absl:Wrote 183000 tfrecods\n",
      "INFO:absl:Wrote 184000 tfrecods\n",
      "INFO:absl:Wrote 185000 tfrecods\n",
      "INFO:absl:Wrote 186000 tfrecods\n",
      "INFO:absl:Wrote 187000 tfrecods\n",
      "INFO:absl:Wrote 188000 tfrecods\n",
      "INFO:absl:Wrote 189000 tfrecods\n",
      "INFO:absl:Wrote 190000 tfrecods\n",
      "INFO:absl:Wrote 191000 tfrecods\n",
      "INFO:absl:Wrote 192000 tfrecods\n",
      "INFO:absl:Wrote 193000 tfrecods\n",
      "INFO:absl:Wrote 194000 tfrecods\n",
      "INFO:absl:Wrote 195000 tfrecods\n",
      "INFO:absl:Wrote 196000 tfrecods\n",
      "INFO:absl:Wrote 197000 tfrecods\n",
      "INFO:absl:Wrote 198000 tfrecods\n",
      "INFO:absl:Wrote 199000 tfrecods\n",
      "INFO:absl:Wrote 200000 tfrecods\n",
      "INFO:absl:Wrote 201000 tfrecods\n",
      "INFO:absl:Wrote 202000 tfrecods\n",
      "INFO:absl:Wrote 203000 tfrecods\n",
      "INFO:absl:Wrote 204000 tfrecods\n",
      "INFO:absl:Wrote 205000 tfrecods\n",
      "INFO:absl:Wrote 206000 tfrecods\n",
      "INFO:absl:Wrote 207000 tfrecods\n",
      "INFO:absl:Wrote 208000 tfrecods\n",
      "INFO:absl:Wrote 209000 tfrecods\n",
      "INFO:absl:Wrote 210000 tfrecods\n",
      "INFO:absl:Wrote 211000 tfrecods\n",
      "INFO:absl:Wrote 212000 tfrecods\n",
      "INFO:absl:Wrote 213000 tfrecods\n",
      "INFO:absl:Wrote 214000 tfrecods\n",
      "INFO:absl:Wrote 215000 tfrecods\n",
      "INFO:absl:Wrote 216000 tfrecods\n",
      "INFO:absl:Wrote 217000 tfrecods\n",
      "INFO:absl:Wrote 218000 tfrecods\n",
      "INFO:absl:Wrote 219000 tfrecods\n",
      "INFO:absl:Wrote 220000 tfrecods\n",
      "INFO:absl:Wrote 221000 tfrecods\n",
      "INFO:absl:Wrote 222000 tfrecods\n",
      "INFO:absl:Wrote 223000 tfrecods\n",
      "INFO:absl:Wrote 224000 tfrecods\n",
      "INFO:absl:Wrote 225000 tfrecods\n",
      "INFO:absl:Wrote 226000 tfrecods\n",
      "INFO:absl:Wrote 227000 tfrecods\n",
      "INFO:absl:Wrote 228000 tfrecods\n",
      "INFO:absl:Wrote 229000 tfrecods\n",
      "INFO:absl:Wrote 230000 tfrecods\n",
      "INFO:absl:Wrote 231000 tfrecods\n",
      "INFO:absl:Wrote 232000 tfrecods\n",
      "INFO:absl:Wrote 233000 tfrecods\n",
      "INFO:absl:Wrote 234000 tfrecods\n",
      "INFO:absl:Wrote 235000 tfrecods\n",
      "INFO:absl:Wrote 236000 tfrecods\n",
      "INFO:absl:Wrote 237000 tfrecods\n",
      "INFO:absl:Wrote 238000 tfrecods\n",
      "INFO:absl:Wrote 239000 tfrecods\n",
      "INFO:absl:Wrote 240000 tfrecods\n",
      "INFO:absl:Wrote 241000 tfrecods\n",
      "INFO:absl:Wrote 242000 tfrecods\n",
      "INFO:absl:Wrote 243000 tfrecods\n",
      "INFO:absl:Wrote 244000 tfrecods\n",
      "INFO:absl:Wrote 245000 tfrecods\n",
      "INFO:absl:Wrote 246000 tfrecods\n",
      "INFO:absl:Wrote 247000 tfrecods\n",
      "INFO:absl:Wrote 248000 tfrecods\n",
      "INFO:absl:Wrote 249000 tfrecods\n",
      "INFO:absl:Wrote 250000 tfrecods\n",
      "INFO:absl:Wrote 251000 tfrecods\n",
      "INFO:absl:Wrote 252000 tfrecods\n",
      "INFO:absl:Wrote 253000 tfrecods\n",
      "INFO:absl:Wrote 254000 tfrecods\n",
      "INFO:absl:Wrote 255000 tfrecods\n",
      "INFO:absl:Wrote 256000 tfrecods\n",
      "INFO:absl:Wrote 257000 tfrecods\n",
      "INFO:absl:Wrote 258000 tfrecods\n",
      "INFO:absl:Wrote 259000 tfrecods\n",
      "INFO:absl:Wrote 260000 tfrecods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 261000 tfrecods\n",
      "INFO:absl:Wrote 262000 tfrecods\n",
      "INFO:absl:Wrote 263000 tfrecods\n",
      "INFO:absl:Wrote 264000 tfrecods\n",
      "INFO:absl:Wrote 265000 tfrecods\n",
      "INFO:absl:Wrote 266000 tfrecods\n",
      "INFO:absl:Wrote 267000 tfrecods\n",
      "INFO:absl:Wrote 268000 tfrecods\n",
      "INFO:absl:Wrote 269000 tfrecods\n",
      "INFO:absl:Wrote 270000 tfrecods\n",
      "INFO:absl:Wrote 271000 tfrecods\n",
      "INFO:absl:Wrote 272000 tfrecods\n",
      "INFO:absl:Wrote 273000 tfrecods\n",
      "INFO:absl:Wrote 274000 tfrecods\n",
      "INFO:absl:Wrote 275000 tfrecods\n",
      "INFO:absl:Wrote 276000 tfrecods\n",
      "INFO:absl:Wrote 277000 tfrecods\n",
      "INFO:absl:Wrote 278000 tfrecods\n",
      "INFO:absl:Wrote 279000 tfrecods\n",
      "INFO:absl:Wrote 280000 tfrecods\n",
      "INFO:absl:Wrote 281000 tfrecods\n",
      "INFO:absl:Wrote 282000 tfrecods\n",
      "INFO:absl:Wrote 283000 tfrecods\n",
      "INFO:absl:Wrote 284000 tfrecods\n",
      "INFO:absl:Wrote 285000 tfrecods\n",
      "INFO:absl:Wrote 286000 tfrecods\n",
      "INFO:absl:Wrote 287000 tfrecods\n",
      "INFO:absl:Wrote 288000 tfrecods\n",
      "INFO:absl:Wrote 289000 tfrecods\n",
      "INFO:absl:Wrote 290000 tfrecods\n",
      "INFO:absl:Wrote 291000 tfrecods\n",
      "INFO:absl:Wrote 292000 tfrecods\n",
      "INFO:absl:Wrote 293000 tfrecods\n",
      "INFO:absl:Wrote 294000 tfrecods\n",
      "INFO:absl:Wrote 295000 tfrecods\n",
      "INFO:absl:Wrote 296000 tfrecods\n",
      "INFO:absl:Wrote 297000 tfrecods\n",
      "INFO:absl:Wrote 298000 tfrecods\n",
      "INFO:absl:Wrote 299000 tfrecods\n",
      "INFO:absl:Wrote 300000 tfrecods\n",
      "INFO:absl:Wrote 301000 tfrecods\n",
      "INFO:absl:Wrote 302000 tfrecods\n",
      "INFO:absl:Wrote 303000 tfrecods\n",
      "INFO:absl:Wrote 304000 tfrecods\n",
      "INFO:absl:Wrote 305000 tfrecods\n",
      "INFO:absl:Wrote 306000 tfrecods\n",
      "INFO:absl:Wrote 307000 tfrecods\n",
      "INFO:absl:Wrote 308000 tfrecods\n",
      "INFO:absl:Wrote 309000 tfrecods\n",
      "INFO:absl:Wrote 310000 tfrecods\n",
      "INFO:absl:Wrote 311000 tfrecods\n",
      "INFO:absl:Wrote 312000 tfrecods\n",
      "INFO:absl:Wrote 313000 tfrecods\n",
      "INFO:absl:Wrote 314000 tfrecods\n",
      "INFO:absl:Wrote 315000 tfrecods\n",
      "INFO:absl:Wrote 316000 tfrecods\n",
      "INFO:absl:Wrote 317000 tfrecods\n",
      "INFO:absl:Wrote 318000 tfrecods\n",
      "INFO:absl:Wrote 319000 tfrecods\n",
      "INFO:absl:Wrote 320000 tfrecods\n",
      "INFO:absl:Wrote 321000 tfrecods\n",
      "INFO:absl:Wrote 322000 tfrecods\n",
      "INFO:absl:Wrote 323000 tfrecods\n",
      "INFO:absl:Wrote 324000 tfrecods\n",
      "INFO:absl:Wrote 325000 tfrecods\n",
      "INFO:absl:Wrote 326000 tfrecods\n",
      "INFO:absl:Wrote 327000 tfrecods\n",
      "INFO:absl:Wrote 328000 tfrecods\n",
      "INFO:absl:Wrote 329000 tfrecods\n",
      "INFO:absl:Wrote 330000 tfrecods\n",
      "INFO:absl:Wrote 331000 tfrecods\n",
      "INFO:absl:Wrote 332000 tfrecods\n",
      "INFO:absl:Wrote 333000 tfrecods\n",
      "INFO:absl:Wrote 334000 tfrecods\n",
      "INFO:absl:Wrote 335000 tfrecods\n",
      "INFO:absl:Wrote 336000 tfrecods\n",
      "INFO:absl:Wrote 337000 tfrecods\n",
      "INFO:absl:Wrote 338000 tfrecods\n",
      "INFO:absl:Wrote 339000 tfrecods\n",
      "INFO:absl:Wrote 340000 tfrecods\n",
      "INFO:absl:Wrote 341000 tfrecods\n",
      "INFO:absl:Wrote 342000 tfrecods\n",
      "INFO:absl:Wrote 343000 tfrecods\n",
      "INFO:absl:Wrote 344000 tfrecods\n",
      "INFO:absl:Wrote 345000 tfrecods\n",
      "INFO:absl:Wrote 346000 tfrecods\n",
      "INFO:absl:Wrote 347000 tfrecods\n",
      "INFO:absl:Wrote 348000 tfrecods\n",
      "INFO:absl:Wrote 349000 tfrecods\n",
      "INFO:absl:Wrote 350000 tfrecods\n",
      "INFO:absl:Wrote 351000 tfrecods\n",
      "INFO:absl:Wrote 352000 tfrecods\n",
      "INFO:absl:Wrote 353000 tfrecods\n",
      "INFO:absl:Wrote 354000 tfrecods\n",
      "INFO:absl:Wrote 355000 tfrecods\n",
      "INFO:absl:Wrote 356000 tfrecods\n",
      "INFO:absl:Wrote 357000 tfrecods\n",
      "INFO:absl:Wrote 358000 tfrecods\n",
      "INFO:absl:Wrote 359000 tfrecods\n",
      "INFO:absl:Wrote 360000 tfrecods\n",
      "INFO:absl:Wrote 361000 tfrecods\n",
      "INFO:absl:Wrote 362000 tfrecods\n",
      "INFO:absl:Wrote 363000 tfrecods\n",
      "INFO:absl:Wrote 364000 tfrecods\n",
      "INFO:absl:Wrote 365000 tfrecods\n",
      "INFO:absl:Wrote 366000 tfrecods\n",
      "INFO:absl:Wrote 367000 tfrecods\n",
      "INFO:absl:Wrote 368000 tfrecods\n",
      "INFO:absl:Wrote 369000 tfrecods\n",
      "INFO:absl:Wrote 370000 tfrecods\n",
      "INFO:absl:Wrote 371000 tfrecods\n",
      "INFO:absl:Wrote 372000 tfrecods\n",
      "INFO:absl:Wrote 373000 tfrecods\n",
      "INFO:absl:Wrote 374000 tfrecods\n",
      "INFO:absl:Wrote 375000 tfrecods\n",
      "INFO:absl:Wrote 376000 tfrecods\n",
      "INFO:absl:Wrote 377000 tfrecods\n",
      "INFO:absl:Wrote 378000 tfrecods\n",
      "INFO:absl:Wrote 379000 tfrecods\n",
      "INFO:absl:Wrote 380000 tfrecods\n",
      "INFO:absl:Wrote 381000 tfrecods\n",
      "INFO:absl:Wrote 382000 tfrecods\n",
      "INFO:absl:Wrote 383000 tfrecods\n",
      "INFO:absl:Wrote 384000 tfrecods\n",
      "INFO:absl:Wrote 385000 tfrecods\n",
      "INFO:absl:Wrote 386000 tfrecods\n",
      "INFO:absl:Wrote 387000 tfrecods\n",
      "INFO:absl:Wrote 388000 tfrecods\n",
      "INFO:absl:Wrote 389000 tfrecods\n",
      "INFO:absl:Wrote 390000 tfrecods\n",
      "INFO:absl:Wrote 391000 tfrecods\n",
      "INFO:absl:Wrote 392000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 392702\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=128\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "    \"input_mask\": (\"var_len\", \"int\"),\n",
    "    \"input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../../OFFICIAL_TFRECORDS/glue/alberta/sst2/train'\n",
    "tfrecord_filename = 'sst2'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "batch_size = 32\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Albert V2 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "\n",
    "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False\n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
    "# model -> legacyModel inherited from tf.keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classification_layer = Classification_Model(model=model,\n",
    "                                      num_classes=2,\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "classification_model = classification_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save up memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del classification_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # class indices\n",
    "* logits: 2D (batch_size x num_classes)\n",
    "\n",
    "**Joint loss** - We minimze loss over each hidden layer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(labels, axis=1)))\n",
    "    return loss\n",
    "\n",
    "def joint_loss(y_true_dict, y_pred_dict):\n",
    "    layer_loss = []\n",
    "    for class_logits in y_pred_dict['class_logits']:\n",
    "        loss = loss_fn(y_true_dict['labels'], class_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 68000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 5s 490ms/step - loss: 0.6858 - class_logits_loss: 0.6858\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 6s 634ms/step - loss: 0.6583 - class_logits_loss: 0.6583\n"
     ]
    }
   ],
   "source": [
    "# # Compile\n",
    "keras_loss_fn = {'class_logits': joint_loss}\n",
    "classification_model.compile2(optimizer=optimizer, \n",
    "                             loss=None, \n",
    "                             custom_loss=keras_loss_fn)\n",
    "# Change steps per epoch to large value/ ignore it completely to train\n",
    "# on full dataset\n",
    "history = classification_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = SimpleTrainer(model = classification_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = joint_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = \"../../OFFICIAL_MODELS/glue/sst2/albert\"\n",
    "classification_model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1043\n"
     ]
    }
   ],
   "source": [
    "dev_examples = examples['validation']\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_examples:\n",
    "        input_ids = [tokenizer.cls_token] + tokenizer.tokenize(f['sentence'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        input_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, shuffle=False, x_keys=x_keys, y_keys=y_keys, batch_size=32, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dev dataset - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 12\n",
    "predictions_per_layer = {i:[] for i in range(num_hidden_layers)}\n",
    "\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dev_dataset:\n",
    "    model_outputs = classification_model(batch_inputs)['class_logits']\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        predictions_per_layer[i].append(tf.argmax(model_outputs[i], axis=1).numpy())\n",
    "    \n",
    "    original_labels.append(batch_labels['labels'].numpy())\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "eval_metrics = {}\n",
    "for i in range(num_hidden_layers):\n",
    "    acc = accuracy_score(np.hstack(predictions_per_layer[i]), np.hstack(original_labels))\n",
    "    eval_metrics[i] = acc\n",
    "    print(i, eval_metrics[i])\n",
    "\n",
    "with open('eval_sst2.json', 'w') as f:\n",
    "    json.dump(eval_metrics, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
