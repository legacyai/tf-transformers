{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "import collections\n",
    "\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer, SPIECE_UNDERLINE\n",
    "from tf_transformers.utils import fast_sp_alignment\n",
    "from tf_transformers.data.squad_utils_sp import (\n",
    "    read_squad_examples,\n",
    "    post_clean_train_squad,\n",
    "    example_to_features_using_fast_sp_alignment_train,\n",
    "    example_to_features_using_fast_sp_alignment_test, \n",
    "    _get_best_indexes\n",
    ")\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import AlbertModel\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.tasks import Span_Selection_Model\n",
    "\n",
    "from transformers import AlbertTokenizer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")\n",
    "\n",
    "from tf_transformers.pipeline.span_extraction_pipeline import Span_Extraction_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "basic_tokenizer = BasicTokenizer(do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert train data to Features\n",
    "\n",
    "* using Fast Sentence Piece Alignment, we convert text to features (text -> list of sub words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.7315692901611328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Time taken 41.1900794506073\n",
      "INFO:absl:Wrote 1012 pos and 0 neg examples\n",
      "INFO:absl:Wrote 2019 pos and 0 neg examples\n",
      "INFO:absl:Wrote 3031 pos and 0 neg examples\n",
      "INFO:absl:Wrote 4036 pos and 0 neg examples\n",
      "INFO:absl:Wrote 5042 pos and 0 neg examples\n",
      "INFO:absl:Wrote 6042 pos and 0 neg examples\n",
      "INFO:absl:Wrote 7043 pos and 0 neg examples\n",
      "INFO:absl:Wrote 8043 pos and 0 neg examples\n",
      "INFO:absl:Wrote 9056 pos and 0 neg examples\n",
      "INFO:absl:Wrote 10056 pos and 0 neg examples\n",
      "INFO:absl:Wrote 11058 pos and 0 neg examples\n",
      "INFO:absl:Wrote 12064 pos and 0 neg examples\n",
      "INFO:absl:Wrote 13065 pos and 0 neg examples\n",
      "INFO:absl:Wrote 14065 pos and 0 neg examples\n",
      "INFO:absl:Wrote 15067 pos and 0 neg examples\n",
      "INFO:absl:Wrote 16067 pos and 0 neg examples\n",
      "INFO:absl:Wrote 17067 pos and 0 neg examples\n",
      "INFO:absl:Wrote 18067 pos and 0 neg examples\n",
      "INFO:absl:Wrote 19067 pos and 0 neg examples\n",
      "INFO:absl:Wrote 20072 pos and 0 neg examples\n",
      "INFO:absl:Wrote 21081 pos and 0 neg examples\n",
      "INFO:absl:Wrote 22081 pos and 0 neg examples\n",
      "INFO:absl:Wrote 23086 pos and 0 neg examples\n",
      "INFO:absl:Wrote 24089 pos and 0 neg examples\n",
      "INFO:absl:Wrote 25089 pos and 0 neg examples\n",
      "INFO:absl:Wrote 26091 pos and 0 neg examples\n",
      "INFO:absl:Wrote 27098 pos and 0 neg examples\n",
      "INFO:absl:Wrote 28103 pos and 0 neg examples\n",
      "INFO:absl:Wrote 29108 pos and 0 neg examples\n",
      "INFO:absl:Wrote 30124 pos and 0 neg examples\n",
      "INFO:absl:Wrote 31138 pos and 0 neg examples\n",
      "INFO:absl:Wrote 32146 pos and 0 neg examples\n",
      "INFO:absl:Wrote 33151 pos and 0 neg examples\n",
      "INFO:absl:Wrote 34153 pos and 0 neg examples\n",
      "INFO:absl:Wrote 35154 pos and 0 neg examples\n",
      "INFO:absl:Wrote 36171 pos and 0 neg examples\n",
      "INFO:absl:Wrote 37172 pos and 0 neg examples\n",
      "INFO:absl:Wrote 38173 pos and 0 neg examples\n",
      "INFO:absl:Wrote 39181 pos and 0 neg examples\n",
      "INFO:absl:Wrote 40185 pos and 0 neg examples\n",
      "INFO:absl:Wrote 41188 pos and 0 neg examples\n",
      "INFO:absl:Wrote 42192 pos and 0 neg examples\n",
      "INFO:absl:Wrote 43196 pos and 0 neg examples\n",
      "INFO:absl:Wrote 44196 pos and 0 neg examples\n",
      "INFO:absl:Wrote 45197 pos and 0 neg examples\n",
      "INFO:absl:Wrote 46211 pos and 0 neg examples\n",
      "INFO:absl:Wrote 47220 pos and 0 neg examples\n",
      "INFO:absl:Wrote 48221 pos and 0 neg examples\n",
      "INFO:absl:Wrote 49227 pos and 0 neg examples\n",
      "INFO:absl:Wrote 50235 pos and 0 neg examples\n",
      "INFO:absl:Wrote 51240 pos and 0 neg examples\n",
      "INFO:absl:Wrote 52248 pos and 0 neg examples\n",
      "INFO:absl:Wrote 53252 pos and 0 neg examples\n",
      "INFO:absl:Wrote 54254 pos and 0 neg examples\n",
      "INFO:absl:Wrote 55263 pos and 0 neg examples\n",
      "INFO:absl:Wrote 56263 pos and 0 neg examples\n",
      "INFO:absl:Wrote 57265 pos and 0 neg examples\n",
      "INFO:absl:Wrote 58276 pos and 0 neg examples\n",
      "INFO:absl:Wrote 59287 pos and 0 neg examples\n",
      "INFO:absl:Wrote 60289 pos and 0 neg examples\n",
      "INFO:absl:Wrote 61299 pos and 0 neg examples\n",
      "INFO:absl:Wrote 62305 pos and 0 neg examples\n",
      "INFO:absl:Wrote 63313 pos and 0 neg examples\n",
      "INFO:absl:Wrote 64318 pos and 0 neg examples\n",
      "INFO:absl:Wrote 65324 pos and 0 neg examples\n",
      "INFO:absl:Wrote 66351 pos and 0 neg examples\n",
      "INFO:absl:Wrote 67354 pos and 0 neg examples\n",
      "INFO:absl:Wrote 68354 pos and 0 neg examples\n",
      "INFO:absl:Wrote 69365 pos and 0 neg examples\n",
      "INFO:absl:Wrote 70367 pos and 0 neg examples\n",
      "INFO:absl:Wrote 71375 pos and 0 neg examples\n",
      "INFO:absl:Wrote 72378 pos and 0 neg examples\n",
      "INFO:absl:Wrote 73383 pos and 0 neg examples\n",
      "INFO:absl:Wrote 74388 pos and 0 neg examples\n",
      "INFO:absl:Wrote 75392 pos and 0 neg examples\n",
      "INFO:absl:Wrote 76394 pos and 0 neg examples\n",
      "INFO:absl:Wrote 77397 pos and 0 neg examples\n",
      "INFO:absl:Wrote 78402 pos and 0 neg examples\n",
      "INFO:absl:Wrote 79406 pos and 0 neg examples\n",
      "INFO:absl:Wrote 80409 pos and 0 neg examples\n",
      "INFO:absl:Wrote 81412 pos and 0 neg examples\n",
      "INFO:absl:Wrote 82413 pos and 0 neg examples\n",
      "INFO:absl:Wrote 83425 pos and 0 neg examples\n",
      "INFO:absl:Wrote 84432 pos and 0 neg examples\n",
      "INFO:absl:Wrote 85432 pos and 0 neg examples\n",
      "INFO:absl:Wrote 86438 pos and 0 neg examples\n",
      "INFO:absl:Wrote 87440 pos and 0 neg examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 173.22707796096802 seconds\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/train-v1.1.json'\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 1. Read Examples\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    "      )\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "# 2.Postprocess (clean text to avoid some unwanted unicode charcaters)\n",
    "train_examples_processed, failed_examples = post_clean_train_squad(train_examples, basic_tokenizer, is_training=is_training)\n",
    "\n",
    "\n",
    "# 3.Convert question, context and answer to proper features (tokenized words) not word indices\n",
    "feature_generator = example_to_features_using_fast_sp_alignment_train(tokenizer, train_examples_processed, max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=SPIECE_UNDERLINE) \n",
    "\n",
    "all_features = []\n",
    "for feature in feature_generator:\n",
    "    all_features.append(feature)\n",
    "end_time = time.time()\n",
    "print(\"time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to TFRecords using TFWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Wrote 9000 tfrecods\n",
      "INFO:absl:Wrote 10000 tfrecods\n",
      "INFO:absl:Wrote 11000 tfrecods\n",
      "INFO:absl:Wrote 12000 tfrecods\n",
      "INFO:absl:Wrote 13000 tfrecods\n",
      "INFO:absl:Wrote 14000 tfrecods\n",
      "INFO:absl:Wrote 15000 tfrecods\n",
      "INFO:absl:Wrote 16000 tfrecods\n",
      "INFO:absl:Wrote 17000 tfrecods\n",
      "INFO:absl:Wrote 18000 tfrecods\n",
      "INFO:absl:Wrote 19000 tfrecods\n",
      "INFO:absl:Wrote 20000 tfrecods\n",
      "INFO:absl:Wrote 21000 tfrecods\n",
      "INFO:absl:Wrote 22000 tfrecods\n",
      "INFO:absl:Wrote 23000 tfrecods\n",
      "INFO:absl:Wrote 24000 tfrecods\n",
      "INFO:absl:Wrote 25000 tfrecods\n",
      "INFO:absl:Wrote 26000 tfrecods\n",
      "INFO:absl:Wrote 27000 tfrecods\n",
      "INFO:absl:Wrote 28000 tfrecods\n",
      "INFO:absl:Wrote 29000 tfrecods\n",
      "INFO:absl:Wrote 30000 tfrecods\n",
      "INFO:absl:Wrote 31000 tfrecods\n",
      "INFO:absl:Wrote 32000 tfrecods\n",
      "INFO:absl:Wrote 33000 tfrecods\n",
      "INFO:absl:Wrote 34000 tfrecods\n",
      "INFO:absl:Wrote 35000 tfrecods\n",
      "INFO:absl:Wrote 36000 tfrecods\n",
      "INFO:absl:Wrote 37000 tfrecods\n",
      "INFO:absl:Wrote 38000 tfrecods\n",
      "INFO:absl:Wrote 39000 tfrecods\n",
      "INFO:absl:Wrote 40000 tfrecods\n",
      "INFO:absl:Wrote 41000 tfrecods\n",
      "INFO:absl:Wrote 42000 tfrecods\n",
      "INFO:absl:Wrote 43000 tfrecods\n",
      "INFO:absl:Wrote 44000 tfrecods\n",
      "INFO:absl:Wrote 45000 tfrecods\n",
      "INFO:absl:Wrote 46000 tfrecods\n",
      "INFO:absl:Wrote 47000 tfrecods\n",
      "INFO:absl:Wrote 48000 tfrecods\n",
      "INFO:absl:Wrote 49000 tfrecods\n",
      "INFO:absl:Wrote 50000 tfrecods\n",
      "INFO:absl:Wrote 51000 tfrecods\n",
      "INFO:absl:Wrote 52000 tfrecods\n",
      "INFO:absl:Wrote 53000 tfrecods\n",
      "INFO:absl:Wrote 54000 tfrecods\n",
      "INFO:absl:Wrote 55000 tfrecods\n",
      "INFO:absl:Wrote 56000 tfrecods\n",
      "INFO:absl:Wrote 57000 tfrecods\n",
      "INFO:absl:Wrote 58000 tfrecods\n",
      "INFO:absl:Wrote 59000 tfrecods\n",
      "INFO:absl:Wrote 60000 tfrecods\n",
      "INFO:absl:Wrote 61000 tfrecods\n",
      "INFO:absl:Wrote 62000 tfrecods\n",
      "INFO:absl:Wrote 63000 tfrecods\n",
      "INFO:absl:Wrote 64000 tfrecods\n",
      "INFO:absl:Wrote 65000 tfrecods\n",
      "INFO:absl:Wrote 66000 tfrecods\n",
      "INFO:absl:Wrote 67000 tfrecods\n",
      "INFO:absl:Wrote 68000 tfrecods\n",
      "INFO:absl:Wrote 69000 tfrecods\n",
      "INFO:absl:Wrote 70000 tfrecods\n",
      "INFO:absl:Wrote 71000 tfrecods\n",
      "INFO:absl:Wrote 72000 tfrecods\n",
      "INFO:absl:Wrote 73000 tfrecods\n",
      "INFO:absl:Wrote 74000 tfrecods\n",
      "INFO:absl:Wrote 75000 tfrecods\n",
      "INFO:absl:Wrote 76000 tfrecods\n",
      "INFO:absl:Wrote 77000 tfrecods\n",
      "INFO:absl:Wrote 78000 tfrecods\n",
      "INFO:absl:Wrote 79000 tfrecods\n",
      "INFO:absl:Wrote 80000 tfrecods\n",
      "INFO:absl:Wrote 81000 tfrecods\n",
      "INFO:absl:Wrote 82000 tfrecods\n",
      "INFO:absl:Wrote 83000 tfrecods\n",
      "INFO:absl:Wrote 84000 tfrecods\n",
      "INFO:absl:Wrote 85000 tfrecods\n",
      "INFO:absl:Wrote 86000 tfrecods\n",
      "INFO:absl:Wrote 87000 tfrecods\n",
      "INFO:absl:Wrote 88000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 88040\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in all_features:\n",
    "        sep_index = f['input_ids'].index(tokenizer.sep_token)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(f['input_ids'])\n",
    "        input_type_ids = [0] * len(input_ids[:sep_index]) + [1] * len(input_ids[sep_index:])\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['start_position'] = f['start_position']\n",
    "        result['end_position']   = f['end_position']\n",
    "        yield result\n",
    "        \n",
    "\n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smaller data\n",
    "\n",
    "schema = {'input_ids': (\"var_len\", \"int\"), \n",
    "         'input_type_ids': (\"var_len\", \"int\"), \n",
    "         'input_mask': (\"var_len\", \"int\"), \n",
    "         'start_position': (\"var_len\", \"int\"), \n",
    "         'end_position': (\"var_len\", \"int\")}\n",
    "\n",
    "tfrecord_train_dir = '../../OFFICIAL_TFRECORDS/squad/albert/train'\n",
    "tfrecord_filename = 'squad'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['start_position', 'end_position']\n",
    "batch_size = 16\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Albert V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "tf.keras.backend.clear_session()\n",
    "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False\n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
    "# model -> legacyModel inherited from tf.keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Span Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "span_selection_model = span_selection_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save up memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del span_selection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # start or end positions\n",
    "* logits: 2D (batch_size x sequence_length)\n",
    "\n",
    "**Joint loss** - We minimze loss over each hidden layer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def span_loss(position, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(position, axis=1)))\n",
    "    return loss\n",
    "\n",
    "    \n",
    "def start_span_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    layer_loss = []\n",
    "    model_outputs = y_pred_dict['start_logits']\n",
    "    for start_logits in model_outputs:\n",
    "        loss = span_loss(y_true_dict['start_position'], start_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)\n",
    "\n",
    "def end_span_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    \n",
    "    layer_loss = []\n",
    "    model_outputs = y_pred_dict['end_logits']\n",
    "    for end_logits in model_outputs:\n",
    "        loss = span_loss(y_true_dict['end_position'], end_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)\n",
    "\n",
    "# Sum of start_loss + end_loss\n",
    "def joint_loss(y_true_dict, y_pred_dict):\n",
    "    start_loss = start_span_loss_all_layers(y_true_dict, y_pred_dict)\n",
    "    end_loss = end_span_loss_all_layers(y_true_dict, y_pred_dict)\n",
    "    return (start_loss + end_loss)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(16, 323), dtype=int32, numpy=\n",
      "array([[   2,   19,   98, ...,    0,    0,    0],\n",
      "       [   2,   98, 3338, ...,    0,    0,    0],\n",
      "       [   2,  496,   20, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   2,  303,   29, ...,    0,    0,    0],\n",
      "       [   2,  184,  151, ...,    0,    0,    0],\n",
      "       [   2,   98, 2091, ...,    0,    0,    0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(16, 323), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'input_type_ids': <tf.Tensor: shape=(16, 323), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>} {'end_position': <tf.Tensor: shape=(16, 1), dtype=int32, numpy=\n",
      "array([[ 48],\n",
      "       [114],\n",
      "       [136],\n",
      "       [ 13],\n",
      "       [157],\n",
      "       [ 96],\n",
      "       [244],\n",
      "       [ 23],\n",
      "       [ 23],\n",
      "       [175],\n",
      "       [ 39],\n",
      "       [ 44],\n",
      "       [ 96],\n",
      "       [115],\n",
      "       [ 30],\n",
      "       [135]], dtype=int32)>, 'start_position': <tf.Tensor: shape=(16, 1), dtype=int32, numpy=\n",
      "array([[ 46],\n",
      "       [113],\n",
      "       [136],\n",
      "       [ 12],\n",
      "       [145],\n",
      "       [ 90],\n",
      "       [242],\n",
      "       [ 19],\n",
      "       [ 19],\n",
      "       [174],\n",
      "       [ 35],\n",
      "       [ 43],\n",
      "       [ 80],\n",
      "       [114],\n",
      "       [ 30],\n",
      "       [134]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 89000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 55s 6s/step - loss: 11.7206 - end_logits_loss: 5.8510 - start_logits_loss: 5.8696\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 43s 4s/step - loss: 11.0268 - end_logits_loss: 5.5092 - start_logits_loss: 5.5177\n"
     ]
    }
   ],
   "source": [
    "# Keras Fit\n",
    "\n",
    "keras_loss_fn = {'start_logits': start_span_loss_all_layers, \n",
    "           'end_logits': end_span_loss_all_layers}\n",
    "span_selection_model.compile2(optimizer=tf.keras.optimizers.Adam(), \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn\n",
    "                             )\n",
    "history = span_selection_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Global Steps 165\n",
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# Custom training\n",
    "history = SimpleTrainer(model = span_selection_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = joint_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100, \n",
    "             gradient_accumulation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = \"../../OFFICIAL_MODELS/joint_loss/squad/albert\"\n",
    "span_selection_model.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.07536649703979492\n"
     ]
    }
   ],
   "source": [
    "# Convert to features\n",
    "dev_input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/dev-v1.1.json'\n",
    "\n",
    "is_training = False\n",
    "\n",
    "start_time = time.time()\n",
    "dev_examples = read_squad_examples(\n",
    "      input_file=dev_input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "dev_examples_cleaned = post_clean_train_squad(dev_examples, basic_tokenizer, is_training=False)\n",
    "qas_id_info, dev_features = example_to_features_using_fast_sp_alignment_test(tokenizer, dev_examples_cleaned,  max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=SPIECE_UNDERLINE)\n",
    "\n",
    "# Features to TF Dataset\n",
    "\n",
    "\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_features:\n",
    "        sep_index = f['input_ids'].index(tokenizer.sep_token)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(f['input_ids'])\n",
    "        input_type_ids = [0] * len(input_ids[:sep_index]) + [1] * len(input_ids[sep_index:])\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        \n",
    "        yield result     \n",
    "\n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Exact Match\n",
    "\n",
    "* Make Predictions\n",
    "* Extract Answers\n",
    "* Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_dict(dict_items, key):\n",
    "    holder = []\n",
    "    for item in dict_items:\n",
    "        holder.append(item[key])\n",
    "    return holder\n",
    "qas_id_list = extract_from_dict(dev_features, 'qas_id')\n",
    "doc_offset_list = extract_from_dict(dev_features, 'doc_offset')\n",
    "\n",
    "# Make batch predictions\n",
    "num_layers = 12\n",
    "per_layer_start_logits = {i:[] for i in range(num_layers)}\n",
    "per_layer_end_logits = {i:[] for i in range(num_layers)}\n",
    "\n",
    "start_time = time.time()\n",
    "for (batch_inputs) in dev_dataset:\n",
    "    model_outputs = span_selection_model(batch_inputs)\n",
    "    for i, start_logits in enumerate(model_outputs['start_logits']):\n",
    "        per_layer_start_logits[i].append(start_logits)\n",
    "    for i, end_logits in enumerate(model_outputs['end_logits']):\n",
    "        per_layer_end_logits[i].append(end_logits)\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Answers (text) from Predictions\n",
    "\n",
    "* Its little tricky as there will be multiple features for one example, if it is longer than max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20 # top N answers\n",
    "max_answer_length = 30 # Max answer length\n",
    "squad_dev_data = json.load(open(dev_input_file_path))['data']\n",
    "layer_results = []\n",
    "for layer in range(num_layers):\n",
    "\n",
    "    start_logits_unstcaked = []\n",
    "    end_logits_unstacked = []\n",
    "    for batch_start_logits in per_layer_start_logits[layer]:\n",
    "        start_logits_unstcaked.extend(tf.unstack(batch_start_logits))\n",
    "    for batch_end_logits in per_layer_end_logits[layer]:\n",
    "        end_logits_unstacked.extend(tf.unstack(batch_end_logits))\n",
    "\n",
    "    qas_id_logits = {}\n",
    "    for i in range(len(qas_id_list)):\n",
    "        qas_id = qas_id_list[i]\n",
    "        example = qas_id_info[qas_id]\n",
    "        feature = dev_features[i]\n",
    "        assert qas_id == feature['qas_id']\n",
    "        if qas_id not in qas_id_logits:\n",
    "            qas_id_logits[qas_id] = {'tok_to_orig_index': example['tok_to_orig_index'],\n",
    "                                                'aligned_words': example['aligned_words'],\n",
    "                                                'feature_length': [len(feature['input_ids'])],\n",
    "                                                'doc_offset': [doc_offset_list[i]],\n",
    "                                                'passage_start_pos': [feature['input_ids'].index(tokenizer.sep_token) + 1],\n",
    "                                                'start_logits': [start_logits_unstcaked[i]], \n",
    "                                                'end_logits': [end_logits_unstacked[i]]}\n",
    "\n",
    "        else:\n",
    "            qas_id_logits[qas_id]['start_logits'].append(start_logits_unstcaked[i])\n",
    "            qas_id_logits[qas_id]['end_logits'].append(end_logits_unstacked[i])\n",
    "            qas_id_logits[qas_id]['feature_length'].append(len(feature['input_ids']))\n",
    "            qas_id_logits[qas_id]['doc_offset'].append(doc_offset_list[i])\n",
    "            qas_id_logits[qas_id]['passage_start_pos'].append(feature['input_ids'].index(tokenizer.sep_token) + 1)\n",
    "\n",
    "            \n",
    "    qas_id_answer = {}\n",
    "    skipped = []\n",
    "    skipped_null = []\n",
    "    global_counter = 0\n",
    "    for qas_id in qas_id_logits:\n",
    "\n",
    "        current_example = qas_id_logits[qas_id]\n",
    "\n",
    "        _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"PrelimPrediction\",\n",
    "            [\"feature_index\", \"start_index\", \"end_index\",\n",
    "             \"start_log_prob\", \"end_log_prob\"])\n",
    "        prelim_predictions = []\n",
    "        example_features = []\n",
    "        for i in range(len( current_example['start_logits'])):\n",
    "            f = dev_features[global_counter]\n",
    "            assert f['qas_id'] == qas_id\n",
    "            example_features.append(f)\n",
    "            global_counter += 1\n",
    "            passage_start_pos = current_example['passage_start_pos'][i]\n",
    "            feature_length = current_example['feature_length'][i]\n",
    "\n",
    "            start_log_prob_list = current_example['start_logits'][i].numpy().tolist()[:feature_length]\n",
    "            end_log_prob_list = current_example['end_logits'][i].numpy().tolist()[:feature_length]\n",
    "            start_indexes = _get_best_indexes(start_log_prob_list, n_best_size)\n",
    "            end_indexes   = _get_best_indexes(end_log_prob_list, n_best_size)\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                  # We could hypothetically create invalid predictions, e.g., predict\n",
    "                  # that the start of the span is in the question. We throw out all\n",
    "                  # invalid predictions.\n",
    "                  if start_index < passage_start_pos or end_index < passage_start_pos:\n",
    "                    continue\n",
    "                  if end_index < start_index:\n",
    "                    continue\n",
    "                  length = end_index - start_index + 1\n",
    "                  if length > max_answer_length:\n",
    "                    continue\n",
    "                  start_log_prob = start_log_prob_list[start_index]\n",
    "                  end_log_prob = end_log_prob_list[end_index]\n",
    "                  start_idx = start_index - passage_start_pos\n",
    "                  end_idx = end_index - passage_start_pos\n",
    "\n",
    "                  prelim_predictions.append(\n",
    "                            _PrelimPrediction(\n",
    "                                feature_index=i,\n",
    "                                start_index=start_idx,\n",
    "                                end_index=end_idx,\n",
    "                                start_log_prob=start_log_prob,\n",
    "                                end_log_prob=end_log_prob))\n",
    "\n",
    "\n",
    "\n",
    "        prelim_predictions = sorted(\n",
    "            prelim_predictions,\n",
    "            key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "            reverse=True)\n",
    "\n",
    "        if prelim_predictions:\n",
    "            best_index = prelim_predictions[0].feature_index\n",
    "            aligned_words = current_example['aligned_words']\n",
    "            try:\n",
    "                tok_to_orig_index = current_example['tok_to_orig_index']\n",
    "                reverse_start_index_align = tok_to_orig_index[prelim_predictions[0].start_index + example_features[best_index]['doc_offset']] # aligned index\n",
    "                reverse_end_index_align   = tok_to_orig_index[prelim_predictions[0].end_index + example_features[best_index]['doc_offset']]\n",
    "\n",
    "                predicted_words = [w for w in aligned_words[reverse_start_index_align: reverse_end_index_align + 1] if w != SPIECE_UNDERLINE]\n",
    "                predicted_text = ' '.join(predicted_words)\n",
    "                qas_id_answer[qas_id] = predicted_text\n",
    "            except:\n",
    "                qas_id_answer[qas_id] = \"\"\n",
    "                skipped.append(qas_id)\n",
    "        else:\n",
    "            qas_id_answer[qas_id] = \"\"\n",
    "            skipped_null.append(qas_id)\n",
    "    eval_results = evaluate_v1(squad_dev_data, qas_id_answer)\n",
    "    layer_results.append(eval_results)\n",
    "    print(\"Layer {} , results {}\".format(layer, eval_results))\n",
    "    \n",
    "    \n",
    "with open(\"squad_albert_joint_loss.json\", 'w') as f:\n",
    "    json.dump(layer_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import string\n",
    "import six\n",
    "import re\n",
    "    \n",
    "from tf_transformers.data.squad_utils_sp import _get_best_indexes, _compute_softmax\n",
    "\n",
    "####### following are from official SQuAD v1.1 evaluation scripts\n",
    "def normalize_answer_v1(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "  def remove_articles(text):\n",
    "    return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "  prediction_tokens = normalize_answer_v1(prediction).split()\n",
    "  ground_truth_tokens = normalize_answer_v1(ground_truth).split()\n",
    "  common = (\n",
    "      collections.Counter(prediction_tokens)\n",
    "      & collections.Counter(ground_truth_tokens))\n",
    "  num_same = sum(common.values())\n",
    "  if num_same == 0:\n",
    "    return 0\n",
    "  precision = 1.0 * num_same / len(prediction_tokens)\n",
    "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "  return (normalize_answer_v1(prediction) == normalize_answer_v1(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "  scores_for_ground_truths = []\n",
    "  for ground_truth in ground_truths:\n",
    "    score = metric_fn(prediction, ground_truth)\n",
    "    scores_for_ground_truths.append(score)\n",
    "  return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate_v1(dataset, predictions):\n",
    "  f1 = exact_match = total = 0\n",
    "  for article in dataset:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "      for qa in paragraph[\"qas\"]:\n",
    "        total += 1\n",
    "        if qa[\"id\"] not in predictions:\n",
    "          # message = (\"Unanswered question \" + six.ensure_str(qa[\"id\"]) +\n",
    "          #           \"  will receive score 0.\")\n",
    "          # print(message, file=sys.stderr)\n",
    "          continue\n",
    "        ground_truths = [x[\"text\"] for x in qa[\"answers\"]]\n",
    "        # ground_truths = list(map(lambda x: x[\"text\"], qa[\"answers\"]))\n",
    "        prediction = predictions[qa[\"id\"]]\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score,\n",
    "                                                     prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "  exact_match = 100.0 * exact_match / total\n",
    "  f1 = 100.0 * f1 / total\n",
    "\n",
    "  return {\"exact_match\": exact_match, \"f1\": f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
