{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Encoder based models like (GPT2, BERT, Albert, Roberta)\n",
    "\n",
    "* There are 2 ways to load Encoder Models\n",
    "\n",
    "**a.** Load a model using \"model_name\" . This is essentially a wrapper around core class.\n",
    "\n",
    "**b.** Load a model by passing config to the core class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Load using model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to                         `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models import BertModel\n",
    "\n",
    "model_layer , model, config = BertModel(model_name='bert-base-uncased')\n",
    "\n",
    "# model_layer --> LegacyLayer inherited from tf.keras.layers.Layer\n",
    "# model       --> LegacyModel inherited from tf.keras.Model\n",
    "# config      --> Python dict config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Load by passing config to the core class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_transformers.models.model_configs.bert import bert_base_uncased\n",
    "from tf_transformers.models.bert import BERTEncoder\n",
    "# All configs in tf-transformers can be accessed by changing \"bert\" to corresponding model .\n",
    "# For eg: gpt2, t5, roberta, albert etc\n",
    "\n",
    "# model_layer --> LegacyLayer inherited from tf.keras.layers.Layer\n",
    "# model       --> LegacyModel inherited from tf.keras.Model\n",
    "# bert_config      --> Python dict config of the model\n",
    "bert_config = bert_base_uncased.config\n",
    "model_layer = BERTEncoder(config=bert_config, name='bert', is_training=False)\n",
    "model = model_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HuggingFace models to tf-transformers\n",
    "\n",
    "* We can convert (GPT2, BERT, Albert, Roberta, t5, mt5) models from HF to tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at /mnt/home/PRE_MODELS/HuggingFace_models/bert_base_uncased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:Done assigning variables weights. Total 199\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "from tf_transformers.utils import convert_bert_hf_to_tf_transformers\n",
    "\n",
    "# I have downloaded HF model and saved locally\n",
    "model_hf_location = '/mnt/home/PRE_MODELS/HuggingFace_models/bert_base_uncased/'\n",
    "model_hf = TFBertModel.from_pretrained(model_hf_location)\n",
    "\n",
    "# from tf_transformers.utils -> you ca import other definitions for GPT2, t5 etc\n",
    "# Convert\n",
    "convert_bert_hf_to_tf_transformers(model_hf, model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to tensorflow checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### All legacyModel (inherited from tf.keras.Model) has save_checkpoint option\n",
    "\n",
    "model_save_dir = \"tf_transformer_models/bert\"\n",
    "model.save_checkpoint(model_save_dir)\n",
    "\n",
    "# You can load back the model using load_checkpoint\n",
    "\n",
    "model.load_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write TFRecords using TFWriter\n",
    "\n",
    "* TFWriter is a simple class, with which you can write any TFRECORD of any type (int. float, string)\n",
    "* Just pass a function, which return a dict (must) as a generator\n",
    "\n",
    "* TFWriter expects a schema which is very simple\n",
    "\n",
    "* I recommend using **var_len** in **schema**, so that you can have **dynamic batching** while batching usig **TFReader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Load MNLI dataset from HuggingFace\n",
    "examples = datasets.load_from_disk(\"/mnt/home/PRE_MODELS/HuggingFace_models/datasets/glue/mnli/\")\n",
    "train_examples = examples[\"train\"]\n",
    "\n",
    "max_seq_length=64\n",
    "\n",
    "# You can prepare data in any way you want\n",
    "# Preprocess it the way you like and return what the model expects as (dict)\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        input_ids_s1 = [tokenizer.cls_token] + tokenizer.tokenize(f['hypothesis'])[: max_seq_length-2] + [tokenizer.sep_token] # -2 to add CLS and SEP\n",
    "        input_ids_s1 = tokenizer.convert_tokens_to_ids(input_ids_s1)\n",
    "        input_type_ids_s1 = [0] * len(input_ids_s1) # 0 for s1\n",
    "        \n",
    "        input_ids_s2 = tokenizer.tokenize(f['premise'])[: max_seq_length-1] + [tokenizer.sep_token] # -1 to add SEP\n",
    "        input_ids_s2 = tokenizer.convert_tokens_to_ids(input_ids_s2)\n",
    "        input_type_ids_s2 = [1] * len(input_ids_s2)\n",
    "        \n",
    "        input_ids =  input_ids_s1 + input_ids_s2\n",
    "        input_type_ids = input_type_ids_s1 + input_type_ids_s2\n",
    "        input_mask = [1] * len(input_ids) # 1 for s2\n",
    "        \n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        result['input_type_ids'] = input_type_ids\n",
    "        \n",
    "        result['labels'] = f['label']\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Schema is simple\n",
    "# \"var_len\" --> for Variable Length feature tf.data.VarLenFeature\n",
    "# \"fixed_len\" --> for fixed length feature tf.data.FixedLenFeature\n",
    "\n",
    "# Make sure yiu have schema for all keys in the dict you are returning from \"parse_function\"\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "    \"input_mask\": (\"var_len\", \"int\"),\n",
    "    \"input_type_ids\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "# Create TFRecords\n",
    "tfrecord_train_dir = '../../OFFICIAL_TFRECORDS/glue/alberta/mnli/train' # Directory\n",
    "tfrecord_filename = 'mnli' # Filename, not required by default\n",
    "\n",
    "# Auto shuffling if \"tag=train\"\n",
    "# Overwrite the directory if \"overwrite=True\"\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "\n",
    "# Pass your parse_function (generator)\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader\n",
    "\n",
    "* TFReader is a simple class, with which you can read any TFRECORD of any type (int. float, string)\n",
    "* TFReader expects a schema, schema will be automatically written as a json when creating TFRecords with TFWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "# Schema from TFRECORD directory\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "# Provide what keys you required in inputs and labels\n",
    "# If you dont want to separate keys, just dont pass \"x_keys\" and \"y_keys\"\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels']\n",
    "\n",
    "# Auto Batch :-) \n",
    "batch_size = 32\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "* Most of NLP downstream tasks like Classification, Span_Selection (QA), Token_Classification (NER) etc are defined by separate tasks models, independent of base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model Example\n",
    "\n",
    "from tf_transformers.tasks import Classification_Model, Span_Selection_Model, Token_Classification_Model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "classification_layer = Classification_Model(model=model,\n",
    "                                      num_classes=3,\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "classification_model = classification_layer.get_model()\n",
    "\n",
    "\n",
    "# Span_Selection_Model (QA) examples\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      is_training=True)\n",
    "span_selection_model = span_selection_layer.get_model()\n",
    "\n",
    "# Token Classification Model (NER) examples\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model_ner = Token_Classification_Model(model=model,\n",
    "                                      token_vocab_size=len(slot_map), # Vocab Size\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "model_ner = model_ner.get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Serialized model (Optimized and Fast)\n",
    "\n",
    "* Serialized Models are the crux of TF2.0 and most of the performance benefits of the model comes only from this\n",
    "* LegacyModel has an option called \"save_as_serialized_module\", which save a model as \"tf.saved_model\", without altering the output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
