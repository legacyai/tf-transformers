{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 + Squad for Span Selection (not Generation)\n",
    "\n",
    "* The purpose of this tutorial is to demonstrate that, how can we customize **decoder** for some other tasks\n",
    "* which it was actually not trained for . \n",
    "\n",
    "**Note** - T5 works well when we formulate most or all tasks as Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer, SPIECE_UNDERLINE\n",
    "from tf_transformers.utils import fast_sp_alignment\n",
    "from tf_transformers.data.squad_utils_sp import (\n",
    "    read_squad_examples,\n",
    "    post_clean_train_squad,\n",
    "    example_to_features_using_fast_sp_alignment_train,\n",
    "    example_to_features_using_fast_sp_alignment_test, \n",
    "    evaluate_v1\n",
    ")\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import T5Model\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.tasks import Span_Selection_Model\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "\n",
    "tokenizer.sep_token = tokenizer.cls_token = tokenizer.eos_token # We need it for squad internal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert train data to Features\n",
    "\n",
    "* using Fast Sentence Piece Alignment, we convert text to features (text -> list of sub words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Time taken 0.06583905220031738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.7573883533477783\n",
      "time taken 1.051743984222412 seconds\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/train-v1.1.json'\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 1. Read Examples\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    "      )\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "# 2.Postprocess (clean text to avoid some unwanted unicode charcaters)\n",
    "train_examples_processed, failed_examples = post_clean_train_squad(train_examples, basic_tokenizer, is_training=is_training)\n",
    "\n",
    "\n",
    "# 3.Convert question, context and answer to proper features (tokenized words) not word indices\n",
    "feature_generator = example_to_features_using_fast_sp_alignment_train(tokenizer, train_examples_processed, max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=SPIECE_UNDERLINE) \n",
    "\n",
    "all_features = []\n",
    "for feature in feature_generator:\n",
    "    all_features.append(feature)\n",
    "end_time = time.time()\n",
    "print(\"time taken {} seconds\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert features to TFRecords using TFWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 100\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in all_features:\n",
    "        \n",
    "        question_sep_index =  f['input_ids'].index(tokenizer.sep_token) # We dont want sep-token in the starting\n",
    "        question_ids = tokenizer.tokenize('question: ') + f['input_ids'][1:question_sep_index+1] # 1 to avoid CLS token\n",
    "        \n",
    "        passage_context_prompt = tokenizer.tokenize('context: ')\n",
    "        passage_ids = passage_context_prompt + f['input_ids'][question_sep_index +1 :] # Adding context as prompt\n",
    "        new_start_position  = (f['start_position'] - len(question_ids)) + len(passage_context_prompt) + 1\n",
    "        new_end_position  = (f['end_position'] - len(question_ids)) + len(passage_context_prompt) + 1\n",
    "        \n",
    "        assert(f['input_ids'][f['start_position']: f['end_position']] == passage_ids[new_start_position: new_end_position])\n",
    "\n",
    "        result['encoder_input_ids'] = tokenizer.convert_tokens_to_ids(question_ids)\n",
    "        result['encoder_input_mask'] = [1] * len(result['encoder_input_ids'])\n",
    "        \n",
    "        result['decoder_input_ids'] = tokenizer.convert_tokens_to_ids(passage_ids)\n",
    "        result['decoder_input_mask'] = [1] * len(result['decoder_input_ids'])\n",
    "        \n",
    "        result['start_position'] = new_start_position\n",
    "        result['end_position']   = new_end_position\n",
    "        yield result\n",
    "        \n",
    "\n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {'encoder_input_ids': (\"var_len\", \"int\"), \n",
    "         'encoder_input_mask': (\"var_len\", \"int\"), \n",
    "         'decoder_input_ids': (\"var_len\", \"int\"), \n",
    "         'decoder_input_mask': (\"var_len\", \"int\"),\n",
    "         'start_position': (\"var_len\", \"int\"), \n",
    "         'end_position': (\"var_len\", \"int\")}\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/squad/t5_span/train'\n",
    "tfrecord_filename = 'squad'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecords using TFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['encoder_input_ids', 'encoder_input_mask', 'decoder_input_ids', 'decoder_input_mask']\n",
    "y_keys = ['start_position', 'end_position']\n",
    "batch_size = 32\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset.take(1):\n",
    "    print(batch_inputs, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load t5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "model_layer, model, config = T5Model(model_name='t5-small', decoder_mask_mode='user_defined')\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/t5-small/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Span Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      is_training=True)\n",
    "span_selection_model = span_selection_layer.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete to save memory\n",
    "\n",
    "del model\n",
    "del model_layer\n",
    "del span_selection_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple.\n",
    "* labels: 1D (batch_size) # start or end positions\n",
    "* logits: 2D (batch_size x sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_loss(position, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.reshape(position, -1)))\n",
    "    return loss\n",
    "\n",
    "def span_loss(position, logits):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.squeeze(position)))\n",
    "    return loss\n",
    "\n",
    "def start_loss(y_true_dict, y_pred_dict):\n",
    "    return span_loss(y_true_dict['start_position'], y_pred_dict['start_logits'])\n",
    "\n",
    "def end_loss(y_true_dict, y_pred_dict):\n",
    "    return span_loss(y_true_dict['end_position'], y_pred_dict['end_logits'])\n",
    "\n",
    "\n",
    "def joint_loss(y_true_dict, y_pred_dict):\n",
    "    start_loss = span_loss(y_true_dict['start_position'], y_pred_dict['start_logits'])\n",
    "    end_loss = span_loss(y_true_dict['end_position'], y_pred_dict['end_logits'])\n",
    "    return (start_loss + end_loss)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 89000\n",
    "EPOCHS = 4\n",
    "optimizer = optimization.AdamWeightDecay(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Start logits (16, None)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits (16, None)\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/10 [=================>............] - ETA: 23s - loss: 10.2776 - end_logits_loss: 5.1788 - start_logits_loss: 5.0988WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 6/10 [=================>............] - 35s 6s/step - loss: 10.2776 - end_logits_loss: 5.1788 - start_logits_loss: 5.0988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90801d0c70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Fit\n",
    "\n",
    "keras_loss_fn = {'start_logits': start_loss, \n",
    "           'end_logits': end_loss}\n",
    "span_selection_model.compile2(optimizer=optimizer, \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn, \n",
    "                            run_eagerly=False)\n",
    "history = span_selection_model.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Global Steps 165\n",
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# Custom training\n",
    "history = SimpleTrainer(model = span_selection_model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = joint_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = '../OFFICIAL_MODELS/squad/t5_span_selection'\n",
    "span_selection_model.save_checkpoint(model_save_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse validation data\n",
    "\n",
    "We use ```TFProcessor``` to create validation data, because dev data is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.07536649703979492\n"
     ]
    }
   ],
   "source": [
    "dev_input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/dev-v1.1.json'\n",
    "\n",
    "is_training = False\n",
    "\n",
    "start_time = time.time()\n",
    "dev_examples = read_squad_examples(\n",
    "      input_file=dev_input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "dev_examples_cleaned = post_clean_train_squad(dev_examples, basic_tokenizer, is_training=False)\n",
    "qas_id_info, dev_features = example_to_features_using_fast_sp_alignment_test(tokenizer, dev_examples_cleaned,  max_seq_length = 384, \n",
    "                                                           max_query_length=64, doc_stride=128, SPECIAL_PIECE=SPIECE_UNDERLINE)\n",
    "\n",
    "\n",
    "def parse_dev():\n",
    "    result = {}\n",
    "    for f in dev_features:\n",
    "        question_sep_index =  f['input_ids'].index(tokenizer.sep_token) # We dont want sep-token in the starting\n",
    "        question_ids = tokenizer.tokenize('question: ') + f['input_ids'][1:question_sep_index+1] # 1 to avoid CLS token\n",
    "        \n",
    "        passage_context_prompt = tokenizer.tokenize('context: ')\n",
    "        passage_ids = passage_context_prompt + f['input_ids'][question_sep_index +1 :] # Adding context as prompt\n",
    "\n",
    "        result['encoder_input_ids'] = tokenizer.convert_tokens_to_ids(question_ids)\n",
    "        result['encoder_input_mask'] = [1] * len(result['encoder_input_ids'])\n",
    "        \n",
    "        result['decoder_input_ids'] = tokenizer.convert_tokens_to_ids(passage_ids)\n",
    "        result['decoder_input_mask'] = [1] * len(result['decoder_input_ids'])\n",
    "        \n",
    "        f['passage_ids'] = result['decoder_input_ids'] # indices\n",
    "        yield result\n",
    "        \n",
    "\n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Exact Match\n",
    "\n",
    "* Make Predictions\n",
    "* Extract Answers\n",
    "* Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_dict(dict_items, key):\n",
    "    holder = []\n",
    "    for item in dict_items:\n",
    "        holder.append(item[key])\n",
    "    return holder\n",
    "qas_id_list = extract_from_dict(dev_features, 'qas_id')\n",
    "doc_offset_list = extract_from_dict(dev_features, 'doc_offset')\n",
    "\n",
    "# Make batch predictions\n",
    "\n",
    "per_layer_start_logits = []\n",
    "per_layer_end_logits = []\n",
    "\n",
    "start_time = time.time()\n",
    "for (batch_inputs) in dev_dataset:\n",
    "    model_outputs = span_selection_model(batch_inputs)\n",
    "    per_layer_start_logits.append(model_outputs['start_logits'])\n",
    "    per_layer_end_logits.append(model_outputs['end_logits'])\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "# Make batch predictions\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n",
    "squad_dev_data = json.load(open(dev_input_file_path))['data']\n",
    "layer_results = []\n",
    "\n",
    "start_logits_unstcaked = []\n",
    "end_logits_unstacked = []\n",
    "for batch_start_logits in per_layer_start_logits:\n",
    "    start_logits_unstcaked.extend(tf.unstack(batch_start_logits))\n",
    "for batch_end_logits in per_layer_end_logits:\n",
    "    end_logits_unstacked.extend(tf.unstack(batch_end_logits))\n",
    "\n",
    "qas_id_logits = {}\n",
    "for i in range(len(qas_id_list)): #\n",
    "    qas_id = qas_id_list[i]\n",
    "    example = qas_id_info[qas_id]\n",
    "    feature = dev_features[i]\n",
    "    assert qas_id == feature['qas_id']\n",
    "    if qas_id not in qas_id_logits:\n",
    "        qas_id_logits[qas_id] = {\n",
    "                                            'feature_length': [len(feature['passage_ids'])],\n",
    "                                            'doc_offset': [doc_offset_list[i]],\n",
    "                                            'passage_start_pos': [0],\n",
    "                                            'start_logits': [start_logits_unstcaked[i]], \n",
    "                                            'end_logits': [end_logits_unstacked[i]]}\n",
    "\n",
    "    else:\n",
    "        qas_id_logits[qas_id]['start_logits'].append(start_logits_unstcaked[i])\n",
    "        qas_id_logits[qas_id]['end_logits'].append(end_logits_unstacked[i])\n",
    "        qas_id_logits[qas_id]['feature_length'].append(len(feature['passage_ids']))\n",
    "        qas_id_logits[qas_id]['doc_offset'].append(doc_offset_list[i])\n",
    "        qas_id_logits[qas_id]['passage_start_pos'].append(0)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "qas_id_answer = {}\n",
    "skipped = []\n",
    "skipped_null = []\n",
    "global_counter = 0\n",
    "p_texts = []\n",
    "for qas_id in qas_id_logits:\n",
    "\n",
    "    current_example = qas_id_logits[qas_id]\n",
    "\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"feature_index\", \"start_index\", \"end_index\",\n",
    "         \"start_log_prob\", \"end_log_prob\"])\n",
    "    prelim_predictions = []\n",
    "    example_features = []\n",
    "    for i in range(len( current_example['start_logits'])):\n",
    "        f = dev_features[global_counter]\n",
    "        assert f['qas_id'] == qas_id\n",
    "        example_features.append(f)\n",
    "        global_counter += 1\n",
    "        # passage_start_pos = current_example['passage_start_pos'][i]\n",
    "        passage_start_pos = 0\n",
    "        feature_length = current_example['feature_length'][i] # non-masked length\n",
    "\n",
    "        start_log_prob_list = current_example['start_logits'][i].numpy().tolist()[:feature_length]\n",
    "        end_log_prob_list = current_example['end_logits'][i].numpy().tolist()[:feature_length]\n",
    "        start_indexes = _get_best_indexes(start_log_prob_list, n_best_size)\n",
    "        end_indexes   = _get_best_indexes(end_log_prob_list, n_best_size)\n",
    "\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "              # We could hypothetically create invalid predictions, e.g., predict\n",
    "              # that the start of the span is in the question. We throw out all\n",
    "              # invalid predictions.\n",
    "              # if start_index < passage_start_pos or end_index < passage_start_pos:\n",
    "              #  continue\n",
    "              if end_index < start_index:\n",
    "                continue\n",
    "              length = end_index - start_index + 1\n",
    "              if length > max_answer_length:\n",
    "                continue\n",
    "              start_log_prob = start_log_prob_list[start_index]\n",
    "              end_log_prob = end_log_prob_list[end_index]\n",
    "              start_idx = start_index - passage_start_pos\n",
    "              end_idx = end_index - passage_start_pos\n",
    "\n",
    "              prelim_predictions.append(\n",
    "                        _PrelimPrediction(\n",
    "                            feature_index=i,\n",
    "                            start_index=start_idx,\n",
    "                            end_index=end_idx,\n",
    "                            start_log_prob=start_log_prob,\n",
    "                            end_log_prob=end_log_prob))\n",
    "\n",
    "\n",
    "\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "        reverse=True)\n",
    "    \n",
    "    if prelim_predictions:\n",
    "        best_index = prelim_predictions[0].feature_index\n",
    "        passage_ids = example_features[best_index]['passage_ids']\n",
    "        predicted_ids = passage_ids[prelim_predictions[best_index].start_index: prelim_predictions[best_index].end_index + 1]\n",
    "        predicted_text = tokenizer.decode(predicted_ids)\n",
    "        qas_id_answer[qas_id] = predicted_text\n",
    "        print(\"Predicted text\", predicted_text)\n",
    "        p_texts.append(predicted_text)\n",
    "\n",
    "    else:\n",
    "        qas_id_answer[qas_id] = \"\"\n",
    "        skipped_null.append(qas_id)\n",
    "        \n",
    "    \n",
    "    \n",
    "eval_results = evaluate_v1(squad_dev_data, qas_id_answer)\n",
    "\n",
    "# {'exact_match': 63.27341532639546, 'f1': 75.86691833684937}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Serialized version \n",
    "\n",
    "- Now we can use ```save_as_serialize_module``` to save a model directly to saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as optimized version\n",
    "span_selection_model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=True)\n",
    "\n",
    "# Load optimized version\n",
    "span_selection_model_serialized = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLite Conversion\n",
    "\n",
    "TFlite conversion requires:\n",
    "- static batch size\n",
    "- static sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence_length = 384\n",
    "# batch_size = 1\n",
    "\n",
    "# Lets convert it to a TFlite model\n",
    "\n",
    "model_layer, model, config = T5Model(model_name='t5-small', \n",
    "                                     decoder_mask_mode='user_defined', \n",
    "                                     batch_size=1, \n",
    "                                     encoder_sequence_length=64, \n",
    "                                     decoder_sequence_length=384\n",
    "                                     )\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "span_selection_layer = Span_Selection_Model(model=model,\n",
    "                                      is_training=False)\n",
    "span_selection_model = span_selection_layer.get_model()\n",
    "span_selection_model.load_checkpoint(model_save_dir)\n",
    "\n",
    "# Save to .pb format , we need it for tflite\n",
    "\n",
    "span_selection_model.save_as_serialize_module(\"{}/saved_model_for_tflite\".format(model_save_dir))\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}/saved_model_for_tflite\".format(model_save_dir)) # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"{}/converted_model.tflite\".format(model_save_dir), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **In production**\n",
    "\n",
    "- We can use either ```tf.keras.Model``` or ```saved_model```. I recommend saved_model, which is much much faster and no hassle of having architecture code\n",
    "\n",
    "**Note** - As of now pipeline cannot be overwridden. But you can write one by looking at existing pipelines,\n",
    "with minimal changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pipeline for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tf_transformers.data.squad_utils_sp import *\n",
    "from tf_transformers.data.squad_utils_sp import _get_best_indexes, _compute_softmax\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def extract_from_dict(dict_items, key):\n",
    "    holder = []\n",
    "    for item in dict_items:\n",
    "        holder.append(item[key])\n",
    "    return holder\n",
    "\n",
    "class Span_Extraction_Pipeline():\n",
    "\n",
    "    def __init__(self, model, \n",
    "                tokenizer, \n",
    "                tokenizer_fn, \n",
    "                SPECIAL_PIECE, \n",
    "                n_best_size, \n",
    "                n_best, \n",
    "                max_answer_length, \n",
    "                max_seq_length,\n",
    "                max_query_length, \n",
    "                doc_stride,\n",
    "                call_fn = None,\n",
    "                batch_size=32):\n",
    "\n",
    "        self.get_model_fn(model)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_fn = tokenizer_fn\n",
    "        self.SPECIAL_PIECE = SPECIAL_PIECE\n",
    "        self.n_best_size = n_best_size\n",
    "        self.n_best = n_best\n",
    "        self.max_answer_length = max_answer_length\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.doc_stride = doc_stride\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "\n",
    "    def get_model_fn(self, model):\n",
    "        self.model_fn = None\n",
    "        # keras Model\n",
    "        if isinstance(model, tf.keras.Model):\n",
    "            self.model_fn = model\n",
    "        else:\n",
    "            # saved model\n",
    "            if \"saved_model\" in str(type(self.model)):\n",
    "                # Extract signature\n",
    "                self.model_pb = model.signatures['serving_default']\n",
    "                def model_fn(x):\n",
    "                    return self.model_pb(**x)\n",
    "                self.model_fn = model_fn\n",
    "        if self.model_fn is None:\n",
    "            raise ValueError(\"Please check the type of your model\")\n",
    "    \n",
    "    def run(self, dataset):\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        for batch_inputs in dataset:\n",
    "            model_outputs = self.model_fn(batch_inputs)\n",
    "            start_logits.append(model_outputs['start_logits'])\n",
    "            end_logits.append(model_outputs['end_logits'])\n",
    "\n",
    "        # Unstack\n",
    "\n",
    "        start_logits_unstacked = []\n",
    "        end_logits_unstacked = []\n",
    "\n",
    "        for batch_logits in start_logits:\n",
    "            start_logits_unstacked.extend(tf.unstack(batch_logits))\n",
    "        for batch_logits in end_logits:\n",
    "            end_logits_unstacked.extend(tf.unstack(batch_logits))\n",
    "\n",
    "        return start_logits_unstacked, end_logits_unstacked\n",
    "    \n",
    "    def convert_to_features(self, dev_examples):\n",
    "        \"\"\"Convert examples to features\"\"\"\n",
    "        qas_id_examples = {ex['qas_id']: ex for ex in dev_examples} \n",
    "        # dev_examples_cleaned = post_clean_train_squad(dev_examples, self.basic_tokenizer, is_training=False)\n",
    "        qas_id_info, dev_features = example_to_features_using_fast_sp_alignment_test(self.tokenizer,\n",
    "            dev_examples, self.max_seq_length, self.max_query_length, self.doc_stride, self.SPECIAL_PIECE\n",
    "        )\n",
    "        return qas_id_examples, qas_id_info, dev_features\n",
    "        \n",
    "    def convert_features_to_dataset(self, dev_features):\n",
    "        \"\"\"Feaures to TF dataset\"\"\"\n",
    "        # for TFProcessor\n",
    "        def local_parser():\n",
    "            for f in dev_features:\n",
    "                result = tokenizer_fn(f)\n",
    "                f['passage_ids'] = result['decoder_input_ids']\n",
    "                yield result\n",
    "\n",
    "        # Create dataset\n",
    "        tf_processor = TFProcessor()\n",
    "        dev_dataset  = tf_processor.process(parse_fn=local_parser())\n",
    "        self.dev_dataset = dev_dataset  = tf_processor.auto_batch(dev_dataset, batch_size = self.batch_size)\n",
    "        return dev_dataset\n",
    "    \n",
    "    def post_process(self, qas_id_examples, dev_features, qas_id_info, start_logits_unstacked, end_logits_unstacked):\n",
    "        # List of qa_ids per feature\n",
    "        # List of doc_offset, for shifting when an example gets splitted due to length\n",
    "        qas_id_list = extract_from_dict(dev_features, 'qas_id')  \n",
    "        doc_offset_list = extract_from_dict(dev_features, 'doc_offset')\n",
    "\n",
    "        # Group by qas_id -> predictions , because multiple feature may come from\n",
    "        # single example :-)\n",
    "\n",
    "        qas_id_logits = {}\n",
    "        for i in range(len(qas_id_list)):\n",
    "            qas_id = qas_id_list[i]\n",
    "            example = qas_id_info[qas_id]\n",
    "            feature = dev_features[i]\n",
    "            assert qas_id == feature['qas_id']\n",
    "            if qas_id not in qas_id_logits:\n",
    "                qas_id_logits[qas_id] = {\n",
    "                                                    'feature_length': [len(feature['passage_ids'])],\n",
    "                                                    'doc_offset': [doc_offset_list[i]],\n",
    "                                                    'start_logits': [start_logits_unstacked[i]], \n",
    "                                                    'end_logits': [end_logits_unstacked[i]]}\n",
    "\n",
    "            else:\n",
    "                qas_id_logits[qas_id]['start_logits'].append(start_logits_unstacked[i])\n",
    "                qas_id_logits[qas_id]['end_logits'].append(end_logits_unstacked[i])\n",
    "                qas_id_logits[qas_id]['feature_length'].append(len(feature['passage_ids']))\n",
    "                qas_id_logits[qas_id]['doc_offset'].append(doc_offset_list[i])\n",
    "\n",
    "\n",
    "\n",
    "        qas_id_answer = {}\n",
    "        skipped = []\n",
    "        skipped_null = []\n",
    "        global_counter = 0\n",
    "        final_result = {}\n",
    "        for qas_id in qas_id_logits:\n",
    "\n",
    "            current_example = qas_id_logits[qas_id]\n",
    "\n",
    "            _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "                \"PrelimPrediction\",\n",
    "                [\"feature_index\", \"start_index\", \"end_index\",\n",
    "                \"start_log_prob\", \"end_log_prob\"])\n",
    "            prelim_predictions = []\n",
    "            example_features = []\n",
    "            for i in range(len( current_example['start_logits'])):\n",
    "                f = dev_features[global_counter]\n",
    "                assert f['qas_id'] == qas_id\n",
    "                example_features.append(f)\n",
    "                global_counter += 1\n",
    "                feature_length = current_example['feature_length'][i]\n",
    "\n",
    "                start_log_prob_list = current_example['start_logits'][i].numpy().tolist()[:feature_length]\n",
    "                end_log_prob_list = current_example['end_logits'][i].numpy().tolist()[:feature_length]\n",
    "                start_indexes = _get_best_indexes(start_log_prob_list, self.n_best_size)\n",
    "                end_indexes   = _get_best_indexes(end_log_prob_list, self.n_best_size)\n",
    "\n",
    "                for start_index in start_indexes:\n",
    "                    for end_index in end_indexes:\n",
    "                        # We could hypothetically create invalid predictions, e.g., predict\n",
    "                        # that the start of the span is in the question. We throw out all\n",
    "                        # invalid predictions.\n",
    "                        # if start_index < passage_start_pos or end_index < passage_start_pos:\n",
    "                        #    continue\n",
    "                        if end_index < start_index:\n",
    "                            continue\n",
    "                        length = end_index - start_index + 1\n",
    "                        if length > self.max_answer_length:\n",
    "                            continue\n",
    "                        start_log_prob = start_log_prob_list[start_index]\n",
    "                        end_log_prob = end_log_prob_list[end_index]\n",
    "                        start_idx = start_index \n",
    "                        end_idx = end_index \n",
    "                        \n",
    "                        prelim_predictions.append(\n",
    "                                    _PrelimPrediction(\n",
    "                                        feature_index=i,\n",
    "                                        start_index=start_idx,\n",
    "                                        end_index=end_idx,\n",
    "                                        start_log_prob=start_log_prob,\n",
    "                                        end_log_prob=end_log_prob))\n",
    "\n",
    "\n",
    "\n",
    "            prelim_predictions = sorted(\n",
    "                prelim_predictions,\n",
    "                key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "                reverse=True)\n",
    "\n",
    "            answer_dict = {}\n",
    "            answer_dict[qas_id] = []\n",
    "            total_scores = []\n",
    "            if prelim_predictions:\n",
    "                for top_n in range(self.n_best):\n",
    "                    best_index = prelim_predictions[top_n].feature_index\n",
    "                    passage_ids = example_features[best_index]['passage_ids']\n",
    "                    predicted_ids = passage_ids[prelim_predictions[top_n].start_index: \\\n",
    "                                                prelim_predictions[top_n].end_index + 1]\n",
    "                    predicted_text = tokenizer.decode(predicted_ids)\n",
    "                    qas_id_answer[qas_id] = predicted_text\n",
    "                    total_scores.append(prelim_predictions[top_n].start_log_prob + prelim_predictions[top_n].end_log_prob)\n",
    "                    answer_dict[qas_id].append({'text': predicted_text})\n",
    "\n",
    "                _probs = _compute_softmax(total_scores)\n",
    "\n",
    "                for top_n in range(self.n_best):\n",
    "                    answer_dict[qas_id][top_n]['probability'] = _probs[top_n]\n",
    "                final_result[qas_id] = qas_id_examples[qas_id]\n",
    "            else:\n",
    "                qas_id_answer[qas_id] = \"\"\n",
    "                skipped_null.append(qas_id)\n",
    "            final_result[qas_id]['answers'] = answer_dict\n",
    "        return final_result\n",
    "    \n",
    "    def __call__(self, questions, contexts, qas_ids=[]):\n",
    "        \n",
    "        # If qas_id is empty, we assign positions as id\n",
    "        if qas_ids == []:\n",
    "            qas_ids = [i for i in range(len(questions))]\n",
    "        # each question should have a context\n",
    "        assert(len(questions) == len(contexts) == len(qas_ids))\n",
    "\n",
    "        dev_examples = convert_question_context_to_standard_format(questions, contexts, qas_ids)\n",
    "        qas_id_examples, qas_id_info, dev_features = self.convert_to_features(dev_examples)\n",
    "        dev_dataset = self.convert_features_to_dataset(dev_features)\n",
    "        start_logits_unstacked, end_logits_unstacked = self.run(dev_dataset)\n",
    "        final_result = self.post_process(qas_id_examples, dev_features, qas_id_info, start_logits_unstacked, end_logits_unstacked)\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn(features):\n",
    "    result = {}\n",
    "    question_sep_index =  features['input_ids'].index(tokenizer.sep_token) # We dont want sep-token in the starting\n",
    "    question_ids = tokenizer.tokenize('question: ') + features['input_ids'][1:question_sep_index+1] # 1 to avoid CLS token\n",
    "\n",
    "    passage_context_prompt = tokenizer.tokenize('context: ')\n",
    "    passage_ids = passage_context_prompt + features['input_ids'][question_sep_index +1 :] # Adding context as prompt\n",
    "\n",
    "    result['encoder_input_ids'] = tokenizer.convert_tokens_to_ids(question_ids)\n",
    "    result['encoder_input_mask'] = [1] * len(result['encoder_input_ids'])\n",
    "\n",
    "    result['decoder_input_ids'] = tokenizer.convert_tokens_to_ids(passage_ids)\n",
    "    result['decoder_input_mask'] = [1] * len(result['decoder_input_ids'])\n",
    "\n",
    "    return result\n",
    "\n",
    "    pipeline = Span_Extraction_Pipeline(model = span_selection_model_serialized,\n",
    "                tokenizer = tokenizer, \n",
    "                tokenizer_fn = tokenizer_fn, \n",
    "                SPECIAL_PIECE = SPIECE_UNDERLINE, \n",
    "                n_best_size = 20, \n",
    "                n_best = 5, \n",
    "                max_answer_length = 30, \n",
    "                max_seq_length = 384, \n",
    "                max_query_length=64, \n",
    "                doc_stride=128)\n",
    "\n",
    "questions = ['When was Kerala formed?']\n",
    "questions = ['What was prominent in Kerala?']\n",
    "questions = ['How many districts are there in Kerala']\n",
    "questions = ['When was Kerala formed?']\n",
    "\n",
    "contexts = ['''Kerala (English: /ˈkɛrələ/; Malayalam: [ke:ɾɐɭɐm] About this soundlisten (help·info)) is a\n",
    "state on the southwestern Malabar Coast of India. It was formed on 1 November 1956, \n",
    "following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of \n",
    "the erstwhile states of Travancore-Cochin and Madras. \n",
    "Spread over 38,863 km2 (15,005 sq mi), Kerala is the twenty-first largest Indian state by area. \n",
    "It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea[14] to the west. With 33,387,677 inhabitants as per the 2011 Census, Kerala is the thirteenth-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.[15]\n",
    "\n",
    "The Chera Dynasty was the first prominent kingdom based in Kerala. The Ay kingdom in the deep south and the Ezhimala kingdom in the north formed the other kingdoms in the early years of the Common Era (CE). The region had been a prominent spice exporter since 3000 BCE. The region's prominence in trade was noted in the works of Pliny as well as the Periplus around 100 CE. In the 15th century, the spice trade attracted Portuguese traders to Kerala, and paved the way for European colonisation of India. At the time of Indian independence movement in the early 20th century, there were two major princely states in Kerala-Travancore State and the Kingdom of Cochin. They united to form the state of Thiru-Kochi in 1949. The Malabar region, in the northern part of Kerala, had been a part of the Madras province of British India, which later became a part of the Madras State post-independence. After the States Reorganisation Act, 1956, the modern-day state of Kerala was formed by merging the Malabar district of Madras State (excluding Gudalur taluk of Nilgiris district, Lakshadweep Islands, Topslip, the Attappadi Forest east of Anakatti), the state of Thiru-Kochi (excluding four southern taluks of Kanyakumari district, Shenkottai and Tenkasi taluks), and the taluk of Kasaragod (now Kasaragod District) in South Canara (Tulunad) which was a part of Madras State.''']\n",
    "\n",
    "result = pipeline(questions, contexts)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check TFlite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lets do a sanity check\n",
    "\n",
    "sample_inputs = {}\n",
    "encoder_input_ids = tf.random.uniform(minval=0, maxval=100, shape=(1, 64), dtype=tf.int32)\n",
    "decoder_input_ids = tf.random.uniform(minval=0, maxval=100, shape=(1, 384), dtype=tf.int32)\n",
    "\n",
    "sample_inputs['encoder_input_ids'] = encoder_input_ids\n",
    "sample_inputs['encoder_input_mask'] = tf.ones_like(sample_inputs['encoder_input_ids'])\n",
    "sample_inputs['decoder_input_ids'] = decoder_input_ids\n",
    "sample_inputs['decoder_input_mask'] = tf.ones_like(sample_inputs['decoder_input_ids'])\n",
    "\n",
    "model_outputs = span_selection_model(sample_inputs)\n",
    "\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(model_save_dir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_inputs['decoder_input_ids'])\n",
    "\n",
    "interpreter.set_tensor(input_details[1]['index'], sample_inputs['decoder_input_mask'])\n",
    "\n",
    "interpreter.set_tensor(input_details[2]['index'], sample_inputs['encoder_input_ids'])\n",
    "\n",
    "interpreter.set_tensor(input_details[3]['index'], sample_inputs['encoder_input_mask'])\n",
    "interpreter.invoke()\n",
    "\n",
    "end_logits = interpreter.get_tensor(output_details[0]['index'])\n",
    "start_logits   = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "print(\"Start logits\", tf.reduce_sum(model_outputs['start_logits']), tf.reduce_sum(start_logits))\n",
    "print(\"End logits\", tf.reduce_sum(model_outputs['end_logits']), tf.reduce_sum(end_logits))\n",
    "\n",
    "# We are good :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
