{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTy_I5RsT3to"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE1LHLVeU3qR"
   },
   "outputs": [],
   "source": [
    "# Install tf-transformers from github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qc2fkTiwT3tw"
   },
   "source": [
    "### SNIPS NLU\n",
    "\n",
    "#### credit -> https://colab.research.google.com/drive/1wgWdxUpKf3FWJgqA6ogBGDEzxAosjJMI\n",
    "\n",
    "Snips NLU consists of 2 tasks (Slot Filling and Classification)\n",
    "\n",
    "Slot filling can be formulated as NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfRs3DlUUGHC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaL8m4yyUDYB"
   },
   "source": [
    "# NER + Albert base using Fast Sentence Piece Alignment + Joint loss + TFlite Conversion\n",
    "\n",
    "This tutorial contains code to fine-tune an Albert Model for NER (Token Classification).\n",
    "\n",
    "In this notebook:\n",
    "- Load the data + create ```tf.data.Dataset``` using TFProcesor\n",
    "- Load Albert Model V2 and use it to create a Token Classification Model\n",
    "- Train using ```tf.keras.Model.fit``` and ```Custom Trainer``` \n",
    "- Minimze loss per layer to find optimal layer\n",
    "- Evaluate EXACT MATCH per layer\n",
    "- Convert TFlite\n",
    "- In production using faster ```tf.SavedModel``` + no architecture code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa-dOtkaT3tx"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lX5zHRPT3tx"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "SNIPS_DATA_BASE_URL = (\n",
    "    \"https://github.com/ogrisel/slot_filling_and_intent_detection_of_SLU/blob/\"\n",
    "    \"master/data/snips/\"\n",
    ")\n",
    "for filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n",
    "    path = Path(filename)\n",
    "    if not path.exists():\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eX9J6JTNT3ty"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8MIQngfWT3ty"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from tf_transformers.utils.tokenization import BasicTokenizer, SPIECE_UNDERLINE # Special Piece for Albert\n",
    "from tf_transformers.data.ner_utils_sp import fast_tokenize_and_align_sentence_for_ner\n",
    "\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.models import AlbertModel\n",
    "from tf_transformers.tasks import Token_Classification_Model\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.pipeline import Token_Classification_Pipeline\n",
    "\n",
    "from transformers import AlbertTokenizer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkkocYTzWIW5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mnULAzdZT3ty"
   },
   "outputs": [],
   "source": [
    "# Read SNIPS data to dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_line(line):\n",
    "    utterance_data, intent_label = line.split(\" <=> \")\n",
    "    items = utterance_data.split()\n",
    "    words = [item.rsplit(\":\", 1)[0] for item in items]\n",
    "    word_labels = [item.rsplit(\":\", 1)[1] for item in items]\n",
    "    return {\n",
    "        \"intent_label\": intent_label,\n",
    "        \"words\": \" \".join(words),\n",
    "        \"word_labels\": \" \".join(word_labels),\n",
    "        \"length\": len(words),\n",
    "    }\n",
    "\n",
    "lines_train = Path(\"train\").read_text().strip().splitlines()\n",
    "lines_valid = Path(\"valid\").read_text().strip().splitlines()\n",
    "lines_test  = Path(\"test\").read_text().strip().splitlines()\n",
    "\n",
    "df_train = pd.DataFrame([parse_line(line) for line in lines_train])\n",
    "df_valid = pd.DataFrame([parse_line(line) for line in lines_valid])\n",
    "df_test  = pd.DataFrame([parse_line(line) for line in lines_test])\n",
    "\n",
    "# Slot labels\n",
    "slot_names = [\"[PAD]\"]\n",
    "slot_names += Path(\"vocab.slot\").read_text().strip().splitlines()\n",
    "slot_map = {}\n",
    "for label in slot_names:\n",
    "    slot_map[label] = len(slot_map)\n",
    "    \n",
    "# id ->  labels\n",
    "slot_map_reverse = {v:k for k,v in slot_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaLF0TpwT3tz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-mcNcefWtxi"
   },
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "huEnTGgbT3tz"
   },
   "outputs": [],
   "source": [
    "# Load HuggingFace Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSTmXIx3Wz77"
   },
   "source": [
    "### Parse train data \n",
    "\n",
    "We will use a simple ```generator``` to iterate over the data and use ```TFProcessor``` to convert it to ```tf.data.Dataset``` in 2 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MEBK5zC9T3t1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processed  10000 examples so far\n",
      "INFO:absl:Total individual observations/examples written is 13073\n"
     ]
    }
   ],
   "source": [
    "is_training = True\n",
    "label_pad_token=\"[PAD]\"\n",
    "\n",
    "# Convert train examples to properly aligned examples\n",
    "ignored_examples_index = []\n",
    "\n",
    "all_flat_tokens = []\n",
    "all_flat_labels = []\n",
    "for index, row in df_train.iterrows():\n",
    "    sentence = row[\"words\"]\n",
    "    labels = row[\"word_labels\"]\n",
    "    word_tokens = sentence.split()\n",
    "    label_tokens = labels.split()\n",
    "    if len(word_tokens) != len(label_tokens):\n",
    "        ignored_examples_index.append(index)\n",
    "        continue\n",
    "    aligned_words, sub_words_mapped, flat_tokens, flat_labels = fast_tokenize_and_align_sentence_for_ner(\n",
    "        tokenizer, sentence, word_tokens, SPIECE_UNDERLINE, is_training, label_tokens, label_pad_token=label_pad_token)\n",
    "    all_flat_tokens.append(flat_tokens)\n",
    "    all_flat_labels.append(flat_labels)\n",
    "    \n",
    "\n",
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "def parse_train():\n",
    "    \"\"\"\n",
    "    convert text to inputs (ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(all_flat_tokens)):\n",
    "        \n",
    "        flat_tokens = all_flat_tokens[i]\n",
    "        flat_labels = all_flat_labels[i]\n",
    "        # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "        result = {}\n",
    "        result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  flat_tokens + [tokenizer.bos_token])\n",
    "        result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
    "        result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
    "        labels = [slot_map[token] for token in flat_labels]\n",
    "        labels = [slot_map[label_pad_token]] + labels + [slot_map[label_pad_token]]  # for [CLS] and [SEP]\n",
    "        \n",
    "        # We dont want label_pad_token (which is mostly because of multiple subowrds)\n",
    "        label_mask = []\n",
    "        for token in flat_labels:\n",
    "            if token == [label_pad_token]:\n",
    "                label_mask.append(0)\n",
    "                continue\n",
    "            label_mask.append(1)\n",
    "        label_mask = [0] + label_mask + [0]  # for [CLS] and [SEP]\n",
    "        \n",
    "        result[\"labels\"] = labels\n",
    "        result[\"label_mask\"] = label_mask\n",
    "        yield result\n",
    "\n",
    "\n",
    "# use TFProcessor only if your data is in range of 10k - 20k maximum\n",
    "# otherwise use TFWriter\n",
    "# This will create a (batch_inputs, batch_labels) tuple dataset\n",
    "batch_size = 32\n",
    "tf_processor = TFProcessor()\n",
    "train_dataset = tf_processor.process(parse_fn=parse_train())\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels', 'label_mask']\n",
    "train_dataset = tf_processor.auto_batch(train_dataset, batch_size=batch_size,\n",
    "                                        x_keys = x_keys,\n",
    "                                        y_keys = y_keys,\n",
    "                                        shuffle=True, \n",
    "                                        drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH_LJ-pGT3t2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJTxyR85XI5o"
   },
   "source": [
    "### Load Albert V2 Model \n",
    "\n",
    "We will use Albert Model and load the checkpoints.\n",
    "To convert Huggingface models to checkpoints, refer ```conversion``` notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "h6VhTeDgT3t2",
    "outputId": "a336eaff-374d-4726-a884-8650b0ba6f2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "# Lets load Albert Model\n",
    "tf.keras.backend.clear_session()\n",
    "model_layer, model, config = AlbertModel(model_name='albert_base_v2', \n",
    "                   is_training=True, \n",
    "                   use_dropout=False\n",
    "                   )\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/albert-base-v2/\")\n",
    "\n",
    "# model_layer -> Legacylayer inherited from tf.keras.Layer\n",
    "# model -> legacyModel inherited from tf.keras.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_4I9uuwYRcF"
   },
   "source": [
    "### Load Token Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WDugadEAT3t4"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_ner = Token_Classification_Model(model=model,\n",
    "                                      token_vocab_size=len(slot_map), # Vocab Size\n",
    "                                      use_all_layers=True, \n",
    "                                      is_training=True)\n",
    "model_ner = model_ner.get_model()\n",
    "\n",
    "# Delete models to save memory\n",
    "\n",
    "del model\n",
    "del model_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqPIxjMwT3t4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhazY1sBYxdB"
   },
   "source": [
    "### Define Loss\n",
    "\n",
    "Loss function is simple, softmax over NER token vocab ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jQzwBqj7T3t5"
   },
   "outputs": [],
   "source": [
    "def token_loss(y_true_dict, token_logits):\n",
    "    loss = cross_entropy_loss(\n",
    "        labels=y_true_dict[\"labels\"],\n",
    "        logits=token_logits,\n",
    "        label_weights=y_true_dict[\"label_mask\"],\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def token_loss_all_layers(y_true_dict, y_pred_dict):\n",
    "    layer_loss = []\n",
    "    for token_logits in y_pred_dict['token_logits']:\n",
    "        loss = token_loss(y_true_dict, token_logits)\n",
    "        layer_loss.append(loss)\n",
    "    return tf.reduce_mean(layer_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duxA4U8IY665"
   },
   "source": [
    "### Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y6hpWdQPT3t5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 13000\n",
    "learning_rate   = 2e-5\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 3\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer_type = 'adamw'\n",
    "optimizer, learning_rate_fn = optimization.create_optimizer(learning_rate,\n",
    "                                                steps_per_epoch * EPOCHS,\n",
    "                                                warmup_steps,\n",
    "                                                optimizer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnTnKmQfc-j0"
   },
   "source": [
    "### Train Using Keras :-)\n",
    "\n",
    "- ```compile2``` allows you to have directly use model outputs as well batch dataset outputs into the loss function, without any further complexity.\n",
    "\n",
    "Note: For ```compile2```, loss_fn must be None, and custom_loss_fn must be active. Metrics are not supprted for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "u_pF-RJuT3t6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/albert/pooler_transform/kernel:0', 'tf_transformers/albert/pooler_transform/bias:0', 'tf_transformers/albert/lower_embedding_projection/kernel:0', 'tf_transformers/albert/lower_embedding_projection/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 6s 589ms/step - loss: 5.5261 - token_logits_loss: 5.5261\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 7s 712ms/step - loss: 5.0989 - token_logits_loss: 5.0989\n"
     ]
    }
   ],
   "source": [
    "# # Compile\n",
    "loss_fn = {'token_logits': token_loss_all_layers}\n",
    "model_ner.compile2(optimizer=optimizer, \n",
    "                             loss=None, \n",
    "                             custom_loss=loss_fn)\n",
    "\n",
    "# Change steps per epoch to large value/ ignore it completely to train\n",
    "# on full dataset\n",
    "history = model_ner.fit(train_dataset, epochs=2, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnW1Cc6SdDTL"
   },
   "source": [
    "### Train using SimpleTrainer (part of tf-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVkGlR2wT3t6"
   },
   "outputs": [],
   "source": [
    "history = SimpleTrainer(model = model_ner,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = token_loss_all_layers,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C1BSIc4T3t6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZrIxd04dV1a"
   },
   "source": [
    "### Save Models \n",
    "\n",
    "You can save models as checkpoints using ```.save_checkpoint``` attribute, which is a part of all ```LegacyModels```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYuclU5AT3t6"
   },
   "outputs": [],
   "source": [
    "model_save_dir = \"../OFFICIAL_MODELS/snips/albert_base\"\n",
    "model_ner.save_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdmwBfDgeGF6"
   },
   "source": [
    "### Parse validation data\n",
    "\n",
    "Like before we use ```TFProcessor``` to create datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UruCrzwRT3t7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert train examples to properly aligned examples\n",
    "ignored_examples_index = []\n",
    "\n",
    "dev_flat_tokens = []\n",
    "dev_flat_labels = []\n",
    "for index, row in df_valid.iterrows():\n",
    "    sentence = row[\"words\"]\n",
    "    labels = row[\"word_labels\"]\n",
    "    word_tokens = sentence.split()\n",
    "    label_tokens = labels.split()\n",
    "    if len(word_tokens) != len(label_tokens):\n",
    "        ignored_examples_index.append(index)\n",
    "        continue\n",
    "    aligned_words, sub_words_mapped, flat_tokens, flat_labels = fast_tokenize_and_align_sentence_for_ner(\n",
    "        tokenizer, sentence, word_tokens, SPIECE_UNDERLINE, is_training, label_tokens, label_pad_token=\"[PAD]\")\n",
    "    dev_flat_tokens.append(flat_tokens)\n",
    "    dev_flat_labels.append(flat_labels)\n",
    "    \n",
    "\n",
    "# Convert tokens to id and add type_ids\n",
    "# input_mask etc\n",
    "# This is user specific/ tokenizer specific\n",
    "# Eg: Roberta has input_type_ids = 0, BERT has input_type_ids = [0, 1]\n",
    "\n",
    "label_pad_token=\"[PAD]\"\n",
    "def parse_dev():\n",
    "    \"\"\"\n",
    "    convert text to inputs (ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(dev_flat_tokens)):\n",
    "        \n",
    "        flat_tokens = dev_flat_tokens[i]\n",
    "        flat_labels = dev_flat_labels[i]\n",
    "        # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "        result = {}\n",
    "        result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  flat_tokens + [tokenizer.bos_token])\n",
    "        result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
    "        result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
    "        labels = [slot_map[token] for token in flat_labels]\n",
    "        labels = [slot_map[label_pad_token]] + labels + [slot_map[label_pad_token]]  # for [CLS] and [SEP]\n",
    "        \n",
    "        # We dont want label_pad_token (which is mostly because of multiple subowrds)\n",
    "        label_mask = []\n",
    "        for token in flat_labels:\n",
    "            if token == [label_pad_token]:\n",
    "                label_mask.append(0)\n",
    "                continue\n",
    "            label_mask.append(1)\n",
    "        label_mask = [0] + label_mask + [0]  # for [CLS] and [SEP]\n",
    "        result[\"labels\"] = labels\n",
    "        result[\"label_mask\"] = label_mask\n",
    "        yield result\n",
    "\n",
    "batch_size = 32\n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "x_keys = ['input_ids', 'input_type_ids', 'input_mask']\n",
    "y_keys = ['labels', 'label_mask']\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=batch_size,\n",
    "                                        x_keys = x_keys,\n",
    "                                        y_keys = y_keys,\n",
    "                                        shuffle=False, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP80kTL6ennW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bhQRVI1etcZ"
   },
   "source": [
    "### EXACT MATCH based evaluation\n",
    "Lets see our idea of jointly minimze loss will bring\n",
    "some benefits or not. If so, that will reduce the overall latency :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VAHMmBg7T3t7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 exact match 223 / 700\n",
      "Layer 1 exact match 428 / 700\n",
      "Layer 2 exact match 511 / 700\n",
      "Layer 3 exact match 558 / 700\n",
      "Layer 4 exact match 571 / 700\n",
      "Layer 5 exact match 576 / 700\n",
      "Layer 6 exact match 587 / 700\n",
      "Layer 7 exact match 594 / 700\n",
      "Layer 8 exact match 592 / 700\n",
      "Layer 9 exact match 591 / 700\n",
      "Layer 10 exact match 596 / 700\n",
      "Layer 11 exact match 586 / 700\n"
     ]
    }
   ],
   "source": [
    "# Exact Match Evaluation .\n",
    "\n",
    "num_layers = 12\n",
    "prediction_per_layer = {i:[] for i in range(num_layers)}\n",
    "original_labels = []\n",
    "for (batch_inputs, batch_labels) in dev_dataset:\n",
    "    results = model_ner(batch_inputs)\n",
    "    model_logits = results['token_logits'][-1]\n",
    "    \n",
    "    for i, model_logits in enumerate(results['token_logits']):\n",
    "    \n",
    "        # Iterate over each example\n",
    "        for index, per_example_logits in enumerate(model_logits):\n",
    "            per_example_length = tf.reduce_sum(batch_inputs['input_mask'][index])\n",
    "            per_example_label  = batch_labels['labels'][index][:per_example_length][1:-1] # we dont want pad positions and 1:-1 is to remove CLS and SEP\n",
    "            per_example_logits = per_example_logits[:per_example_length][1:-1] # 1:-1 CLS and SEP\n",
    "            per_example_preds  = tf.argmax(per_example_logits, axis=-1)\n",
    "            prediction_per_layer[i].append(per_example_preds)\n",
    "            \n",
    "            # We want the original label only once\n",
    "            if i == 0:\n",
    "                original_labels.append(per_example_label)\n",
    "    \n",
    "    \n",
    "# We have 700 examples\n",
    "for layer_iter in range(num_layers):\n",
    "    result = prediction_per_layer[layer_iter]\n",
    "    \n",
    "    pred_list = []\n",
    "    for i in range(700):\n",
    "        pred = list(result[i].numpy())\n",
    "        orig = list(original_labels[i].numpy())\n",
    "        if pred == orig:\n",
    "            pred_list.append(1)\n",
    "        else:\n",
    "            pred_list.append(0)\n",
    "    print(\"Layer {} exact match {} / 700\".format(layer_iter, sum(pred_list)))\n",
    "    \n",
    "    \n",
    "# Layer 0 exact match 223 / 700\n",
    "# Layer 1 exact match 428 / 700\n",
    "# Layer 2 exact match 511 / 700\n",
    "# Layer 3 exact match 558 / 700\n",
    "# Layer 4 exact match 571 / 700\n",
    "# Layer 5 exact match 576 / 700\n",
    "# Layer 6 exact match 587 / 700\n",
    "# Layer 7 exact match 594 / 700\n",
    "# Layer 8 exact match 592 / 700\n",
    "# Layer 9 exact match 591 / 700\n",
    "# Layer 10 exact match 596 / 700\n",
    "# Layer 11 exact match 586 / 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocejDeOjioJF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KswX5gokiqF7"
   },
   "source": [
    "### Save as Serialized version \n",
    "\n",
    "- Now we can use ```save_as_serialize_module``` to save a model directly to saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "C0lNb4TRioRL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwride num_hidden_layers with 5\n",
      "INFO:absl:We are overwriding `is_training` is False to                         `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../OFFICIAL_MODELS/snips/albert_base/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../OFFICIAL_MODELS/snips/albert_base/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Reload with only 5 layers \n",
    "model_layer, model, config = AlbertModel(model_name='albert-base-v2', \n",
    "                                     num_hidden_layers=5, \n",
    "                                     is_training=False\n",
    "                                     )\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model_ner = Token_Classification_Model(model=model,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=False)\n",
    "model_ner = model_ner.get_model()\n",
    "model_ner.load_checkpoint(model_save_dir)\n",
    "\n",
    "model_ner.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnlgeVSYe3Ux"
   },
   "source": [
    "### TFLite Conversion\n",
    "\n",
    "TFlite conversion requires:\n",
    "- static batch size\n",
    "- static sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Mx1Vxp9fT3t7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44829296"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, layer 5 itself is giving 576/700 \n",
    "# Thats great. Lets finalize the model with 5 hidden layers (instead of 12)\n",
    "\n",
    "model_layer, model, config = AlbertModel(model_name='albert-base-v2', \n",
    "                                     batch_size=1, \n",
    "                                     sequence_length=45,  # 45 is enough for SNIP \n",
    "                                     num_hidden_layers=5, \n",
    "                                     is_training=False\n",
    "                                     )\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model_ner = Token_Classification_Model(model=model,\n",
    "                                      token_vocab_size=len(slot_map),\n",
    "                                      use_all_layers=False, \n",
    "                                      is_training=False)\n",
    "model_ner = model_ner.get_model()\n",
    "model_ner.load_checkpoint(model_save_dir)\n",
    "\n",
    "# Save to .pb format , we need it for tflite\n",
    "\n",
    "model_ner.save_as_serialize_module(\"{}/saved_model_for_tflite\".format(model_save_dir))\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}/saved_model_for_tflite\".format(model_save_dir)) # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(\"{}/converted_model.tflite\".format(model_save_dir), \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iTXqsVR4iSSn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nhP-DYdh0XR"
   },
   "source": [
    "### **In production**\n",
    "\n",
    "- We can use either ```tf.keras.Model``` or ```saved_model```. I recommend saved_model, which is much much faster and no hassle of having architecture code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF53k89jT3t8"
   },
   "outputs": [],
   "source": [
    "def tokenizer_fn(feature):\n",
    "    \"\"\"\n",
    "    feature: tokenized text (tokenizer.tokenize)\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    result[\"input_ids\"] = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] +  feature['input_ids'] + [tokenizer.bos_token])\n",
    "    result[\"input_mask\"] = [1] * len(result[\"input_ids\"])\n",
    "    result[\"input_type_ids\"] = [0] * len(result[\"input_ids\"])\n",
    "    return result\n",
    "\n",
    "# load serialized model\n",
    "model_ner = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))\n",
    "slot_map_reverse = {v:k for k,v in slot_map.items()}\n",
    "pipeline = Token_Classification_Pipeline( model = model_ner, \n",
    "                tokenizer = tokenizer, \n",
    "                tokenizer_fn = tokenizer_fn, \n",
    "                SPECIAL_PIECE = SPIECE_UNDERLINE,\n",
    "                label_map = slot_map_reverse,\n",
    "                max_seq_length = 128,\n",
    "                batch_size=32)\n",
    "\n",
    "sentences = ['I would love to listen to Carnatic music by Yesudas', \n",
    "            'Play Carnatic Fusion by Various Artists', \n",
    "            'Please book 2 tickets from Bangalore to Kerala']\n",
    "result = pipeline(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "vvHetDqRhlVT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'I would love to listen to Carnatic music by Yesudas',\n",
       "  'original_words': ['I',\n",
       "   'would',\n",
       "   'love',\n",
       "   'to',\n",
       "   'listen',\n",
       "   'to',\n",
       "   'Carnatic',\n",
       "   'music',\n",
       "   'by',\n",
       "   'Yesudas'],\n",
       "  'predicted_labels': ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-album',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-artist'],\n",
       "  'predicted_probs': array([2.1276660e-01, 5.5502349e-01, 8.1709892e-02, 3.0246880e-02,\n",
       "         4.6255462e-02, 4.7849871e-02, 8.4232342e-06, 2.8101532e-03,\n",
       "         2.2861654e-02, 4.6765100e-04], dtype=float32)},\n",
       " {'sentence': 'Play Carnatic Fusion by Various Artists',\n",
       "  'original_words': ['Play', 'Carnatic', 'Fusion', 'by', 'Various', 'Artists'],\n",
       "  'predicted_labels': ['O', 'B-album', 'I-album', 'O', 'O', 'O'],\n",
       "  'predicted_probs': array([0.57828164, 0.00286884, 0.00589877, 0.31501585, 0.03300228,\n",
       "         0.06493266], dtype=float32)},\n",
       " {'sentence': 'Please book 2 tickets from Bangalore to Kerala',\n",
       "  'original_words': ['Please',\n",
       "   'book',\n",
       "   '2',\n",
       "   'tickets',\n",
       "   'from',\n",
       "   'Bangalore',\n",
       "   'to',\n",
       "   'Kerala'],\n",
       "  'predicted_labels': ['O',\n",
       "   'O',\n",
       "   'B-party_size_number',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-facility',\n",
       "   'O',\n",
       "   'B-country'],\n",
       "  'predicted_probs': array([4.6688071e-01, 2.4216440e-01, 2.4932856e-03, 4.6911496e-03,\n",
       "         1.7782636e-01, 1.0358456e-04, 1.0325107e-01, 2.5894439e-03],\n",
       "        dtype=float32)}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzdvcZyBhnHb"
   },
   "source": [
    "### Sanity Check for TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ORoD0UXET3t8"
   },
   "outputs": [],
   "source": [
    "# Check same model with tflite\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(model_save_dir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "tflite_seq_length = 45\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "sample_inputs = {}\n",
    "sample_inputs['input_ids'] = tf.random.uniform(minval=0, maxval=100, \n",
    "                                                                    shape=(1, tflite_seq_length), dtype=tf.int32)\n",
    "sample_inputs['input_type_ids'] = tf.zeros_like(sample_inputs['input_ids'])\n",
    "sample_inputs['input_mask'] = tf.ones_like(sample_inputs['input_ids'])\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_inputs['input_ids'])\n",
    "interpreter.set_tensor(input_details[1]['index'],  sample_inputs['input_mask'])\n",
    "interpreter.set_tensor(input_details[2]['index'], sample_inputs['input_type_ids'])\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "model_output = model_ner(**sample_inputs) # Why ** ? because it is a saved model\n",
    "\n",
    "# Check tf.reduce_sum(tflite_output), tf.reduce_sum(model_output['token_logits'])\n",
    "# Both matches :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ner_albert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
