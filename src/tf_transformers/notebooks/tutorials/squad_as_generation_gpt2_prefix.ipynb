{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/home/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from tf_transformers.models import GPT2Model\n",
    "from transformers import GPT2Tokenizer\n",
    "from tf_transformers.data.squad_utils_sp import (\n",
    "    read_squad_examples)\n",
    "from tf_transformers.data import TFWriter, TFReader, TFProcessor\n",
    "from tf_transformers.losses import cross_entropy_loss_fast\n",
    "from tf_transformers.core import optimization, SimpleTrainer\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 0.8381352424621582\n"
     ]
    }
   ],
   "source": [
    "input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/train-v1.1.json'\n",
    "\n",
    "is_training = True\n",
    "\n",
    "# 1. Read Examples\n",
    "start_time = time.time()\n",
    "train_examples = read_squad_examples(\n",
    "      input_file=input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    "      )\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Wrote 1000 tfrecods\n",
      "INFO:absl:Wrote 2000 tfrecods\n",
      "INFO:absl:Wrote 3000 tfrecods\n",
      "INFO:absl:Wrote 4000 tfrecods\n",
      "INFO:absl:Wrote 5000 tfrecods\n",
      "INFO:absl:Wrote 6000 tfrecods\n",
      "INFO:absl:Wrote 7000 tfrecods\n",
      "INFO:absl:Wrote 8000 tfrecods\n",
      "INFO:absl:Wrote 9000 tfrecods\n",
      "INFO:absl:Wrote 10000 tfrecods\n",
      "INFO:absl:Wrote 11000 tfrecods\n",
      "INFO:absl:Wrote 12000 tfrecods\n",
      "INFO:absl:Wrote 13000 tfrecods\n",
      "INFO:absl:Wrote 14000 tfrecods\n",
      "INFO:absl:Wrote 15000 tfrecods\n",
      "INFO:absl:Wrote 16000 tfrecods\n",
      "INFO:absl:Wrote 17000 tfrecods\n",
      "INFO:absl:Wrote 18000 tfrecods\n",
      "INFO:absl:Wrote 19000 tfrecods\n",
      "INFO:absl:Wrote 20000 tfrecods\n",
      "INFO:absl:Wrote 21000 tfrecods\n",
      "INFO:absl:Wrote 22000 tfrecods\n",
      "INFO:absl:Wrote 23000 tfrecods\n",
      "INFO:absl:Wrote 24000 tfrecods\n",
      "INFO:absl:Wrote 25000 tfrecods\n",
      "INFO:absl:Wrote 26000 tfrecods\n",
      "INFO:absl:Wrote 27000 tfrecods\n",
      "INFO:absl:Wrote 28000 tfrecods\n",
      "INFO:absl:Wrote 29000 tfrecods\n",
      "INFO:absl:Wrote 30000 tfrecods\n",
      "INFO:absl:Wrote 31000 tfrecods\n",
      "INFO:absl:Wrote 32000 tfrecods\n",
      "INFO:absl:Wrote 33000 tfrecods\n",
      "INFO:absl:Wrote 34000 tfrecods\n",
      "INFO:absl:Wrote 35000 tfrecods\n",
      "INFO:absl:Wrote 36000 tfrecods\n",
      "INFO:absl:Wrote 37000 tfrecods\n",
      "INFO:absl:Wrote 38000 tfrecods\n",
      "INFO:absl:Wrote 39000 tfrecods\n",
      "INFO:absl:Wrote 40000 tfrecods\n",
      "INFO:absl:Wrote 41000 tfrecods\n",
      "INFO:absl:Wrote 42000 tfrecods\n",
      "INFO:absl:Wrote 43000 tfrecods\n",
      "INFO:absl:Wrote 44000 tfrecods\n",
      "INFO:absl:Wrote 45000 tfrecods\n",
      "INFO:absl:Wrote 46000 tfrecods\n",
      "INFO:absl:Wrote 47000 tfrecods\n",
      "INFO:absl:Wrote 48000 tfrecods\n",
      "INFO:absl:Wrote 49000 tfrecods\n",
      "INFO:absl:Wrote 50000 tfrecods\n",
      "INFO:absl:Wrote 51000 tfrecods\n",
      "INFO:absl:Wrote 52000 tfrecods\n",
      "INFO:absl:Wrote 53000 tfrecods\n",
      "INFO:absl:Wrote 54000 tfrecods\n",
      "INFO:absl:Wrote 55000 tfrecods\n",
      "INFO:absl:Wrote 56000 tfrecods\n",
      "INFO:absl:Wrote 57000 tfrecods\n",
      "INFO:absl:Wrote 58000 tfrecods\n",
      "INFO:absl:Wrote 59000 tfrecods\n",
      "INFO:absl:Wrote 60000 tfrecods\n",
      "INFO:absl:Wrote 61000 tfrecods\n",
      "INFO:absl:Wrote 62000 tfrecods\n",
      "INFO:absl:Wrote 63000 tfrecods\n",
      "INFO:absl:Wrote 64000 tfrecods\n",
      "INFO:absl:Wrote 65000 tfrecods\n",
      "INFO:absl:Wrote 66000 tfrecods\n",
      "INFO:absl:Wrote 67000 tfrecods\n",
      "INFO:absl:Wrote 68000 tfrecods\n",
      "INFO:absl:Wrote 69000 tfrecods\n",
      "INFO:absl:Wrote 70000 tfrecods\n",
      "INFO:absl:Wrote 71000 tfrecods\n",
      "INFO:absl:Wrote 72000 tfrecods\n",
      "INFO:absl:Wrote 73000 tfrecods\n",
      "INFO:absl:Wrote 74000 tfrecods\n",
      "INFO:absl:Wrote 75000 tfrecods\n",
      "INFO:absl:Wrote 76000 tfrecods\n",
      "INFO:absl:Wrote 77000 tfrecods\n",
      "INFO:absl:Wrote 78000 tfrecods\n",
      "INFO:absl:Wrote 79000 tfrecods\n",
      "INFO:absl:Wrote 80000 tfrecods\n",
      "INFO:absl:Wrote 81000 tfrecods\n",
      "INFO:absl:Wrote 82000 tfrecods\n",
      "INFO:absl:Wrote 83000 tfrecods\n",
      "INFO:absl:Wrote 84000 tfrecods\n",
      "INFO:absl:Wrote 85000 tfrecods\n",
      "INFO:absl:Wrote 86000 tfrecods\n",
      "INFO:absl:Wrote 87000 tfrecods\n",
      "INFO:absl:Wrote 88000 tfrecods\n",
      "INFO:absl:Wrote 89000 tfrecods\n",
      "INFO:absl:Wrote 90000 tfrecods\n",
      "INFO:absl:Wrote 91000 tfrecods\n",
      "INFO:absl:Wrote 92000 tfrecods\n",
      "INFO:absl:Wrote 93000 tfrecods\n",
      "INFO:absl:Wrote 94000 tfrecods\n",
      "INFO:absl:Wrote 95000 tfrecods\n",
      "INFO:absl:Wrote 96000 tfrecods\n",
      "INFO:absl:Wrote 97000 tfrecods\n",
      "INFO:absl:Wrote 98000 tfrecods\n",
      "INFO:absl:Wrote 99000 tfrecods\n",
      "INFO:absl:Wrote 100000 tfrecods\n",
      "INFO:absl:Wrote 101000 tfrecods\n",
      "INFO:absl:Wrote 102000 tfrecods\n",
      "INFO:absl:Wrote 103000 tfrecods\n",
      "INFO:absl:Wrote 104000 tfrecods\n",
      "INFO:absl:Wrote 105000 tfrecods\n",
      "INFO:absl:Wrote 106000 tfrecods\n",
      "INFO:absl:Wrote 107000 tfrecods\n",
      "INFO:absl:Wrote 108000 tfrecods\n",
      "INFO:absl:Wrote 109000 tfrecods\n",
      "INFO:absl:Wrote 110000 tfrecods\n",
      "INFO:absl:Wrote 111000 tfrecods\n",
      "INFO:absl:Wrote 112000 tfrecods\n",
      "INFO:absl:Wrote 113000 tfrecods\n",
      "INFO:absl:Wrote 114000 tfrecods\n",
      "INFO:absl:Wrote 115000 tfrecods\n",
      "INFO:absl:Wrote 116000 tfrecods\n",
      "INFO:absl:Wrote 117000 tfrecods\n",
      "INFO:absl:Wrote 118000 tfrecods\n",
      "INFO:absl:Wrote 119000 tfrecods\n",
      "INFO:absl:Wrote 120000 tfrecods\n",
      "INFO:absl:Wrote 121000 tfrecods\n",
      "INFO:absl:Wrote 122000 tfrecods\n",
      "INFO:absl:Wrote 123000 tfrecods\n",
      "INFO:absl:Wrote 124000 tfrecods\n",
      "INFO:absl:Wrote 125000 tfrecods\n",
      "INFO:absl:Wrote 126000 tfrecods\n",
      "INFO:absl:Wrote 127000 tfrecods\n",
      "INFO:absl:Wrote 128000 tfrecods\n",
      "INFO:absl:Wrote 129000 tfrecods\n",
      "INFO:absl:Wrote 130000 tfrecods\n",
      "INFO:absl:Wrote 131000 tfrecods\n",
      "INFO:absl:Wrote 132000 tfrecods\n",
      "INFO:absl:Wrote 133000 tfrecods\n",
      "INFO:absl:Wrote 134000 tfrecods\n",
      "INFO:absl:Wrote 135000 tfrecods\n",
      "INFO:absl:Wrote 136000 tfrecods\n",
      "INFO:absl:Wrote 137000 tfrecods\n",
      "INFO:absl:Wrote 138000 tfrecods\n",
      "INFO:absl:Wrote 139000 tfrecods\n",
      "INFO:absl:Wrote 140000 tfrecods\n",
      "INFO:absl:Wrote 141000 tfrecods\n",
      "INFO:absl:Wrote 142000 tfrecods\n",
      "INFO:absl:Wrote 143000 tfrecods\n",
      "INFO:absl:Wrote 144000 tfrecods\n",
      "INFO:absl:Wrote 145000 tfrecods\n",
      "INFO:absl:Wrote 146000 tfrecods\n",
      "INFO:absl:Wrote 147000 tfrecods\n",
      "INFO:absl:Wrote 148000 tfrecods\n",
      "INFO:absl:Wrote 149000 tfrecods\n",
      "INFO:absl:Wrote 150000 tfrecods\n",
      "INFO:absl:Wrote 151000 tfrecods\n",
      "INFO:absl:Wrote 152000 tfrecods\n",
      "INFO:absl:Wrote 153000 tfrecods\n",
      "INFO:absl:Wrote 154000 tfrecods\n",
      "INFO:absl:Wrote 155000 tfrecods\n",
      "INFO:absl:Wrote 156000 tfrecods\n",
      "INFO:absl:Wrote 157000 tfrecods\n",
      "INFO:absl:Wrote 158000 tfrecods\n",
      "INFO:absl:Wrote 159000 tfrecods\n",
      "INFO:absl:Wrote 160000 tfrecods\n",
      "INFO:absl:Wrote 161000 tfrecods\n",
      "INFO:absl:Wrote 162000 tfrecods\n",
      "INFO:absl:Wrote 163000 tfrecods\n",
      "INFO:absl:Wrote 164000 tfrecods\n",
      "INFO:absl:Wrote 165000 tfrecods\n",
      "INFO:absl:Wrote 166000 tfrecods\n",
      "INFO:absl:Wrote 167000 tfrecods\n",
      "INFO:absl:Wrote 168000 tfrecods\n",
      "INFO:absl:Wrote 169000 tfrecods\n",
      "INFO:absl:Wrote 170000 tfrecods\n",
      "INFO:absl:Wrote 171000 tfrecods\n",
      "INFO:absl:Wrote 172000 tfrecods\n",
      "INFO:absl:Wrote 173000 tfrecods\n",
      "INFO:absl:Wrote 174000 tfrecods\n",
      "INFO:absl:Wrote 175000 tfrecods\n",
      "INFO:absl:Wrote 176000 tfrecods\n",
      "INFO:absl:Wrote 177000 tfrecods\n",
      "INFO:absl:Wrote 178000 tfrecods\n",
      "INFO:absl:Wrote 179000 tfrecods\n",
      "INFO:absl:Wrote 180000 tfrecods\n",
      "INFO:absl:Wrote 181000 tfrecods\n",
      "INFO:absl:Wrote 182000 tfrecods\n",
      "INFO:absl:Wrote 183000 tfrecods\n",
      "INFO:absl:Wrote 184000 tfrecods\n",
      "INFO:absl:Wrote 185000 tfrecods\n",
      "INFO:absl:Wrote 186000 tfrecods\n",
      "INFO:absl:Wrote 187000 tfrecods\n",
      "INFO:absl:Wrote 188000 tfrecods\n",
      "INFO:absl:Wrote 189000 tfrecods\n",
      "INFO:absl:Wrote 190000 tfrecods\n",
      "INFO:absl:Wrote 191000 tfrecods\n",
      "INFO:absl:Wrote 192000 tfrecods\n",
      "INFO:absl:Wrote 193000 tfrecods\n",
      "INFO:absl:Wrote 194000 tfrecods\n",
      "INFO:absl:Wrote 195000 tfrecods\n",
      "INFO:absl:Wrote 196000 tfrecods\n",
      "INFO:absl:Wrote 197000 tfrecods\n",
      "INFO:absl:Wrote 198000 tfrecods\n",
      "INFO:absl:Wrote 199000 tfrecods\n",
      "INFO:absl:Wrote 200000 tfrecods\n",
      "INFO:absl:Wrote 201000 tfrecods\n",
      "INFO:absl:Wrote 202000 tfrecods\n",
      "INFO:absl:Wrote 203000 tfrecods\n",
      "INFO:absl:Wrote 204000 tfrecods\n",
      "INFO:absl:Total individual observations/examples written is 204045\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "max_passage_length = 384\n",
    "max_question_length = 64\n",
    "max_answer_length = 40\n",
    "\n",
    "def parse_train():\n",
    "    result = {}\n",
    "    for f in train_examples:\n",
    "        question_input_ids =  tokenizer.tokenize('question: ' + f['question_text'])[: max_question_length] \n",
    "        passage_input_ids  =  tokenizer.tokenize('context: '  + f['paragraph_text'])[: max_passage_length -1]  + [tokenizer.bos_token] # -1 to add </s>\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(question_input_ids + passage_input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        labels_mask = [0] * len(input_ids)\n",
    "        answer_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(f['orig_answer_text'])[: max_answer_length-1] + \n",
    "                                                     [tokenizer.bos_token])\n",
    "        input_ids = input_ids + answer_ids\n",
    "        input_mask = input_mask + [0] * len(answer_ids)\n",
    "        labels_mask = labels_mask + [1] * len(answer_ids)\n",
    "        \n",
    "        labels = input_ids[1:]\n",
    "        labels_mask = labels_mask[1:]\n",
    "        \n",
    "        input_ids = input_ids[:-1]\n",
    "        input_mask = input_mask[:-1]\n",
    "\n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "        \n",
    "        result['labels'] = labels\n",
    "        result['labels_mask'] = labels_mask\n",
    "        \n",
    "        # Decoder doesnt need input_mask because by default decoder has causal mask mode\n",
    "\n",
    "        yield result\n",
    "        \n",
    "# Lets write using TF Writer\n",
    "# Use TFProcessor for smalled data\n",
    "\n",
    "schema = {\n",
    "    \"input_ids\": (\"var_len\", \"int\"),\n",
    "    \"input_mask\": (\"var_len\", \"int\"),\n",
    "    \"labels\": (\"var_len\", \"int\"),\n",
    "    \"labels_mask\": (\"var_len\", \"int\"),\n",
    "}\n",
    "\n",
    "tfrecord_train_dir = '../OFFICIAL_TFRECORDS/squad_as_generation/gpt2/train'\n",
    "tfrecord_filename = 'squad'\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "tfwriter.process(parse_fn=parse_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "\n",
    "\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "all_files = glob.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids', 'input_mask']\n",
    "y_keys = ['labels', 'labels_mask']\n",
    "batch_size = 16\n",
    "train_dataset = tf_reader.read_record(auto_batch=True, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   y_keys = y_keys,\n",
    "                                   shuffle=True, \n",
    "                                   drop_remainder=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_input_ids': <tf.Tensor: shape=(8, 41), dtype=int32, numpy=\n",
      "array([[    0,   250,   693,    18,   809,    34,    57,   303,  1025,\n",
      "           10, 29957,   790,    11, 16612, 33897,     4,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [    0, 18187,   677,  8743,  1638,  4834,    40,   310,  3350,\n",
      "          118, 20486,   967,  6249,    11,   395,    18,  2561,  2117,\n",
      "          507,    71,   258,   439,   149,    11,  1359,  3880,     4,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [    0,  9497,  3219,    10,  2997,   891,    18,  3257,    11,\n",
      "            5,  3497,     9,  2487,    32,  8959,     5,   693,    18,\n",
      "          744,    25,  1900,     8,  3219,     5,  3302,    14,    79,\n",
      "           21,     5,  1802,     9,    10,  1900,    12,  9228, 15772,\n",
      "            4,     0,     0,     0,     0],\n",
      "       [    0, 32828,  4127,  3475,  1270,  3429, 11740,   468, 14282,\n",
      "           11,    78,  1524,    23,     5, 12696,  2374, 11761,    25,\n",
      "          344, 13919, 28137,  1550,   501,   212,    15,    39,   671,\n",
      "            7,   274,   134,     4,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [    0,  1106,    47,   348,   655,   770,     7,   310,  5919,\n",
      "           53,    57,   342,   160,   142,    24,    18,   350,  2933,\n",
      "            6,    50,   350,  7727,    50,    47,    64,    75,  1040,\n",
      "           10,   461,     6,   244,   111,    24,  1302,   111,    16,\n",
      "           23,   865,     4,     0,     0],\n",
      "       [    0,   717, 23228, 17791,  8417,  1573, 16788,  5460,    18,\n",
      "          614,  2116,    25, 14056,  1008,   628,    11,  1356,    12,\n",
      "          958,     7,  3002,   815,  1596,  4980,   459, 10416,    11,\n",
      "            5,   381,  7613,   968,     4,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [    0, 25826, 47248,    34,  1507,    10,   158,  6316,  2729,\n",
      "        11103,    19,     5,   796,  1332,     8,     5,  1016, 12387,\n",
      "         2896,    36,  3755,   597,    43,    11,    41,  2120,     7,\n",
      "         1690,  4097,   160,     5,  6277,     9,    63,  3454,  1293,\n",
      "            8,     5,  6012,   866,     4],\n",
      "       [    0,   133,  3862, 19061,  1924,     9,    20, 10880,    34,\n",
      "          174,   491,  3431,    37,  1017,   657,     7,  4279,   594,\n",
      "           19,  2344,   264,   254,   260,    11,     5,   499,     4,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]], dtype=int32)>, 'decoder_input_type_ids': <tf.Tensor: shape=(8, 41), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>, 'encoder_input_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[    0, 27696,   668, ...,     0,     0,     0],\n",
      "       [    0, 28498, 35472, ...,   127,   275,     2],\n",
      "       [    0,   133,  1763, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [    0,   487, 14963, ...,  4680,    43,     2],\n",
      "       [    0,  2264,    34, ...,   533,     7,     2],\n",
      "       [    0,   401,   587, ...,     0,     0,     0]], dtype=int32)>, 'encoder_input_mask': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'encoder_input_type_ids': <tf.Tensor: shape=(8, 512), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>} {'labels': <tf.Tensor: shape=(8, 41), dtype=int32, numpy=\n",
      "array([[  250,   693,    18,   809,    34,    57,   303,  1025,    10,\n",
      "        29957,   790,    11, 16612, 33897,     4,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [18187,   677,  8743,  1638,  4834,    40,   310,  3350,   118,\n",
      "        20486,   967,  6249,    11,   395,    18,  2561,  2117,   507,\n",
      "           71,   258,   439,   149,    11,  1359,  3880,     4,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [ 9497,  3219,    10,  2997,   891,    18,  3257,    11,     5,\n",
      "         3497,     9,  2487,    32,  8959,     5,   693,    18,   744,\n",
      "           25,  1900,     8,  3219,     5,  3302,    14,    79,    21,\n",
      "            5,  1802,     9,    10,  1900,    12,  9228, 15772,     4,\n",
      "            2,     0,     0,     0,     0],\n",
      "       [32828,  4127,  3475,  1270,  3429, 11740,   468, 14282,    11,\n",
      "           78,  1524,    23,     5, 12696,  2374, 11761,    25,   344,\n",
      "        13919, 28137,  1550,   501,   212,    15,    39,   671,     7,\n",
      "          274,   134,     4,     2,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [ 1106,    47,   348,   655,   770,     7,   310,  5919,    53,\n",
      "           57,   342,   160,   142,    24,    18,   350,  2933,     6,\n",
      "           50,   350,  7727,    50,    47,    64,    75,  1040,    10,\n",
      "          461,     6,   244,   111,    24,  1302,   111,    16,    23,\n",
      "          865,     4,     2,     0,     0],\n",
      "       [  717, 23228, 17791,  8417,  1573, 16788,  5460,    18,   614,\n",
      "         2116,    25, 14056,  1008,   628,    11,  1356,    12,   958,\n",
      "            7,  3002,   815,  1596,  4980,   459, 10416,    11,     5,\n",
      "          381,  7613,   968,     4,     2,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0],\n",
      "       [25826, 47248,    34,  1507,    10,   158,  6316,  2729, 11103,\n",
      "           19,     5,   796,  1332,     8,     5,  1016, 12387,  2896,\n",
      "           36,  3755,   597,    43,    11,    41,  2120,     7,  1690,\n",
      "         4097,   160,     5,  6277,     9,    63,  3454,  1293,     8,\n",
      "            5,  6012,   866,     4,     2],\n",
      "       [  133,  3862, 19061,  1924,     9,    20, 10880,    34,   174,\n",
      "          491,  3431,    37,  1017,   657,     7,  4279,   594,    19,\n",
      "         2344,   264,   254,   260,    11,     5,   499,     4,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]], dtype=int32)>, 'labels_mask': <tf.Tensor: shape=(8, 41), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "      dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for (batch_inputs, batch_labels) in train_dataset:\n",
    "    print(batch_inputs, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwride mask_mode with user_defined\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Overwride mask_mode with causal\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Succesful: Model checkpoints matched\n",
      "INFO:absl:Encoder loaded succesfully from /mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/roberta-base/\n",
      "INFO:absl:Warm started decoder 197/202 variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:encoder_input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:encoder_input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_ids ---> Tensor(\"decoder_input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:decoder_input_type_ids ---> Tensor(\"decoder_input_type_ids:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_layer, model, config = GPT2Model(model_name='gpt2', mask_mode = 'prefix', return_all_layer_token_embeddings=False)\n",
    "model.load_checkpoint(\"/mnt/home/PRE_MODELS/LegacyAI_models/checkpoints/gpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = model(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_label_smoothing(labels, logits, smoothing=0.1, label_weights=None):\n",
    "    \"\"\"\n",
    "    logits: (.. , vocab_size)\n",
    "    labels: (.. ) rank should be less than logits\n",
    "    label_weights: labels shape\n",
    "\n",
    "    Faster than above implementation\n",
    "    \"\"\"\n",
    "    confidence = 1.0 - smoothing\n",
    "    vocab_size = tf.shape(logits)[-1]\n",
    "    vocab_float = tf.cast(vocab_size - 1, tf.float32)\n",
    "    low_confidence = (1.0 - confidence) / vocab_float\n",
    "    soft_targets = tf.one_hot(\n",
    "        labels,\n",
    "        depth=vocab_size,\n",
    "        on_value=confidence,\n",
    "        off_value=low_confidence)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=soft_targets)\n",
    "    # Calculate the best (lowest) possible value of cross entropy, and\n",
    "    # subtract from the cross entropy loss.\n",
    "    normalizing_constant = -(\n",
    "        confidence * tf.math.log(confidence) + vocab_float *\n",
    "        low_confidence * tf.math.log(low_confidence + 1e-20))\n",
    "    xentropy -= normalizing_constant\n",
    "    if label_weights is None:\n",
    "        label_weights = tf.ones_like(labels)\n",
    "    per_example_loss = xentropy * tf.cast(label_weights, xentropy.dtype)\n",
    "    numerator = tf.reduce_sum(per_example_loss)\n",
    "    denominator = tf.cast(tf.reduce_sum(label_weights), numerator.dtype)\n",
    "    denominator = tf.reduce_sum(label_weights)\n",
    "    loss = tf.math.divide_no_nan(numerator, tf.cast(denominator, numerator.dtype))\n",
    "    return loss\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict, smoothing=0.1):\n",
    "    \n",
    "    return cross_entropy_loss_label_smoothing(labels=y_true_dict['labels'], \n",
    "                                   logits=y_pred_dict['token_logits'],\n",
    "                                   smoothing=smoothing,\n",
    "                                      label_weights=y_true_dict['labels_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Adamw optimizer\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimization.AdamWeightDecay(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_transformers/roberta/pooler_transform/kernel:0', 'tf_transformers/roberta/pooler_transform/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 36s 7s/step - loss: 16.4347 - token_logits_loss: 16.4347\n",
      "Epoch 2/2\n",
      "5/5 [==============================] - 36s 7s/step - loss: 11.4058 - token_logits_loss: 11.4058\n"
     ]
    }
   ],
   "source": [
    "# Keras Fit\n",
    "\n",
    "keras_loss_fn = {'token_logits': lm_loss\n",
    "                }\n",
    "model.compile2(optimizer=optimizer, \n",
    "                            loss=None, \n",
    "                            custom_loss=keras_loss_fn, \n",
    "                            run_eagerly=False)\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = 87000\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "EPOCHS = 4\n",
    "\n",
    "# Custom training\n",
    "history2 = SimpleTrainer(model = model,\n",
    "             optimizer = optimizer,\n",
    "             loss_fn = lm_loss,\n",
    "             dataset = train_dataset.repeat(EPOCHS+1), # This is important\n",
    "             epochs = EPOCHS, \n",
    "             num_train_examples = train_data_size, \n",
    "             batch_size = batch_size, \n",
    "             steps_per_call=100)\n",
    "model_save_dir = \"../OFFICIAL_MODELS/squad_as_generation/gpt2_prefix\"\n",
    "model.save_checkpoint(model_save_dir, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = 0\n",
    "top_32_results = []\n",
    "for batch_inputs in dev_dataset:\n",
    "    predicted_ids = []\n",
    "    for i in range(30):\n",
    "        result = model(batch_inputs)\n",
    "        p_ids = tf.cast(tf.argmax(result['last_token_logits'][0]), tf.int32)\n",
    "        predicted_ids.append(p_ids.numpy())\n",
    "        batch_inputs['input_ids'] = tf.concat([batch_inputs['input_ids'], [[p_ids]]], axis=1)\n",
    "        #batch_inputs['input_mask'] = tf.concat([batch_inputs['input_mask'], [[1]]], axis=1)\n",
    "        if p_ids.numpy() == tokenizer.bos_token_id:\n",
    "            print(\"Found\")\n",
    "            break\n",
    "    top_32_results.append(tokenizer.decode(predicted_ids, skip_special_tokens=True))\n",
    "    batch_counter += 1\n",
    "    if batch_counter == 32:\n",
    "        break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model by disabling dropout and add pipeline_mode = 'auto-regressive'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model_layer, model, config = GPT2Model(model_name='gpt2',\n",
    "                                       mask_mode='prefix',\n",
    "                                      return_all_layer_token_embeddings=False,\n",
    "                                      is_training=False, pipeline_mode='auto-regressive')\n",
    "model.load_checkpoint(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_as_serialize_module(\"{}/saved_model\".format(model_save_dir), overwrite=True)\n",
    "loaded = tf.saved_model.load(\"{}/saved_model\".format(model_save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_input_file_path = '/mnt/home/PRE_MODELS/HuggingFace_models/datasets/squadv1.1/dev-v1.1.json'\n",
    "\n",
    "is_training = False\n",
    "\n",
    "start_time = time.time()\n",
    "dev_examples = read_squad_examples(\n",
    "      input_file=dev_input_file_path,\n",
    "      is_training=is_training,\n",
    "      version_2_with_negative=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print('Time taken {}'.format(end_time-start_time))\n",
    "\n",
    "def parse_dev():\n",
    "    for f in dev_examples:\n",
    "        question_input_ids =  tokenizer.tokenize('question: ' + f['question_text'])[: max_question_length] \n",
    "        passage_input_ids  =  tokenizer.tokenize('context: '  + f['paragraph_text'])[: max_passage_length -1]  + [tokenizer.bos_token] # -1 to add </s>\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(question_input_ids + passage_input_ids)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        \n",
    "        result = {}\n",
    "        result['input_ids'] = input_ids\n",
    "        result['input_mask'] = input_mask\n",
    "       \n",
    "        yield result\n",
    "        \n",
    "tf_processor = TFProcessor()\n",
    "dev_dataset = tf_processor.process(parse_fn=parse_dev())\n",
    "dev_dataset = tf_processor.auto_batch(dev_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(predicted_ids, eos_id):\n",
    "    all_ids = []\n",
    "    for per_example_id in predicted_ids:\n",
    "        try:\n",
    "            index = per_example_id.index(eos_id)\n",
    "        except:\n",
    "            index = -1\n",
    "        sliced_ids = per_example_id[:index]\n",
    "        all_ids.append(sliced_ids)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tf_transformers.text import TextDecoder\n",
    "\n",
    "decoder = TextDecoder(model=loaded, \n",
    "                      input_mask_ids=1,\n",
    "                            )\n",
    "# OR if keras.model\n",
    "\n",
    "decoder = TextDecoder(model=model, \n",
    "                            input_mask_ids=1\n",
    "                            )\n",
    "\n",
    "batch_counter = 0\n",
    "start_time = time.time()\n",
    "predicted_answers = []\n",
    "for batch_inputs in dev_dataset:\n",
    "    padded_mask = tf.cast(tf.equal(batch_inputs['input_mask'], 0), tf.int32) * -1\n",
    "    batch_inputs['input_ids'] = batch_inputs['input_ids'] + padded_mask # we need -1 (not 0) for padded positions\n",
    "    model_outputs = decoder.decode(batch_inputs, \n",
    "                   mode='greedy', \n",
    "                   max_iterations=40,\n",
    "                   do_sample=False,\n",
    "                   eos_id=tokenizer.bos_token_id)\n",
    "\n",
    "    predicted_ids = model_outputs['predicted_ids'][:, 0, :].numpy().tolist()\n",
    "    predicted_ids_sliced = split_by_id(predicted_ids, tokenizer.bos_token_id)\n",
    "    predicted_text = [tokenizer.decode(p_ids, skip_special_tokens=True) for p_ids in predicted_ids_sliced]\n",
    "        \n",
    "    \n",
    "    predicted_answers.extend(predicted_text)\n",
    "    batch_counter += 1\n",
    "    print(\"batch {}/{}\".format(batch_counter, len(dev_examples)//32))\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "\n",
    "squad_dev_data = json.load(open(dev_input_file_path))['data']\n",
    "qas_id_answer  = {item['qas_id']: predicted_answers[i] for(i, item) in enumerate(dev_examples)}\n",
    "eval_results = evaluate_v1(squad_dev_data, qas_id_answer)\n",
    "\n",
    "# {'exact_match': 47.52128666035951, 'f1': 63.22016537129672}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
