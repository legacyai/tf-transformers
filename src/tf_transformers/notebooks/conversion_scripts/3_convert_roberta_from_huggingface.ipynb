{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/PRVATE/Documents/tf_transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel\n",
    "from tf_transformers.models import ROBERTAEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "from tf_transformers.utils import convert_roberta_hf_to_tf_transformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /Users/PRVATE/HUggingFace_Models/roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at /Users/PRVATE/HUggingFace_Models/roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load HF model\n",
    "\n",
    "# Always do this\n",
    "\n",
    "\n",
    "model_hf_location = '/Users/PRVATE/HUggingFace_Models/roberta-base'\n",
    "model_hf = TFRobertaModel.from_pretrained(model_hf_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training`                     to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask_1:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids_1:0\", shape=(None, None), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Load tf_transformers model\n",
    "# Most config we will be providing\n",
    "\n",
    "# Default configs for the model\n",
    "model_config_dir = '/Users/PRVATE/Documents/tf_transformers/model_configs/'\n",
    "model_name = 'roberta_base'\n",
    "config_location = os.path.join(model_config_dir, model_name, 'config.json')\n",
    "config = json.load(open(config_location))\n",
    "# Always do this\n",
    "\n",
    "\n",
    "# tf_transformers Layer (an extension of Keras Layer)\n",
    "# This is not Keras model, but extension of keras Layer\n",
    "\n",
    "model_layer = ROBERTAEncoder(config=config,\n",
    "                      name='roberta',\n",
    "                      mask_mode=config['mask_mode'],\n",
    "                      is_training=False, \n",
    "                      use_dropout=False,\n",
    "                      )\n",
    "# model_dir = None, because we have not initialized the model with proper variable values\n",
    "model_tf_transformers = model_layer.get_and_load_model(model_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Deleteing huggingface model for saving memory\n",
      "INFO:absl:We slice Positional Embeddings from 514 to 512\n",
      "INFO:absl:Done assigning variables weights . Total 199\n"
     ]
    }
   ],
   "source": [
    "convert_roberta_hf_to_tf_transformers(model_hf, model_tf_transformers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_output --> tf.Tensor(9.341352, shape=(), dtype=float32) --> (2, 768)\n",
      "token_embeddings --> tf.Tensor(149.64793, shape=(), dtype=float32) --> (2, 5, 768)\n",
      "token_logits --> tf.Tensor(-385577.8, shape=(), dtype=float32) --> (2, 5, 50265)\n",
      "last_token_logits --> tf.Tensor(-85478.8, shape=(), dtype=float32) --> (2, 50265)\n"
     ]
    }
   ],
   "source": [
    "# Please have a look at tf_transformers/extra/*.py for reference values\n",
    "\n",
    "input_ids  = tf.constant([[1, 9, 10, 11, 23], \n",
    "                         [1, 22, 234, 432, 2349]])\n",
    "\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.zeros_like(input_ids)\n",
    "\n",
    "inputs = {'input_ids': input_ids, \n",
    "          'input_mask': input_mask, \n",
    "          'input_type_ids': input_type_ids}\n",
    "\n",
    "results_tf_transformers   = model_tf_transformers(inputs)\n",
    "for k, r in results_tf_transformers.items():\n",
    "    if isinstance(r, list):\n",
    "        continue\n",
    "    print(k, '-->', tf.reduce_sum(r), '-->', r.shape)\n",
    "    \n",
    "    \n",
    "# For Roberta Base \n",
    "\n",
    "# cls_output --> tf.Tensor(9.341034, shape=(), dtype=float32) --> (2, 768)\n",
    "# token_embeddings --> tf.Tensor(149.65, shape=(), dtype=float32) --> (2, 5, 768)\n",
    "# token_logits --> tf.Tensor(-385563.88, shape=(), dtype=float32) --> (2, 5, 50265)\n",
    "# last_token_logits --> tf.Tensor(-85481.09, shape=(), dtype=float32) --> (2, 50265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(144.91971, shape=(), dtype=float32) --> (2, 5, 768)\n",
      "tf.Tensor(7.443221, shape=(), dtype=float32) --> (2, 768)\n"
     ]
    }
   ],
   "source": [
    "# Huggingface Model\n",
    "input_ids  = tf.constant([[1, 9, 10, 11, 23], \n",
    "                         [1, 22, 234, 432, 2349]])\n",
    "\n",
    "input_ids  = tf.constant([[1, 2, 3, 4, 5], \n",
    "                         [1, 2, 3, 4, 5]])\n",
    "\n",
    "input_mask = tf.ones_like(input_ids)\n",
    "input_type_ids = tf.zeros_like(input_ids)\n",
    "\n",
    "results_hf = model_hf([input_ids, input_mask, input_type_ids])\n",
    "for k in results_hf:\n",
    "    print(tf.reduce_sum(k), '-->', k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf_transformers.save_checkpoint('/Users/PRVATE/tf_transformers_models/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Succesful: Model checkpoints matched\n"
     ]
    }
   ],
   "source": [
    "model_tf_transformers.load_checkpoint('/Users/PRVATE/tf_transformers_models/roberta-base/')\n",
    "# model_name = 'roberta-base'\n",
    "# model_loc  = '/Users/PRVATE/tf_transformers_models/'\n",
    "# checkpoint_dir = os.path.join(model_loc, model_name)\n",
    "# if os.path.exists(checkpoint_dir):\n",
    "#     raise FileExistsError()\n",
    "\n",
    "# # If you want to save the model as checkpoints\n",
    "# checkpoint = tf.train.Checkpoint(model=model_tf_transformers)\n",
    "# manager = tf.train.CheckpointManager(\n",
    "#     checkpoint, directory=checkpoint_dir, max_to_keep=1\n",
    "# )\n",
    "# manager.save()\n",
    "# print(\"Saved at {}\".format(manager.latest_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/PRVATE/anaconda3/envs/venv_tf_transformers/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_pb/assets\n"
     ]
    }
   ],
   "source": [
    "model_tf_transformers.save(\"model_pb\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
