{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tf_transformers.models import GPT2Encoder\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "config  = json.load(open(\"../model_directory/gpt2_base/gpt2_config.json\"))\n",
    "\n",
    "model_layer = GPT2Encoder(config = config, \n",
    "                 name='gpt2',\n",
    "                 mask_mode='causal', \n",
    "                 is_training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_probs_dropout_prob': 0.1,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'embedding_size': 768,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 3072,\n",
       " 'max_position_embeddings': 1024,\n",
       " 'num_attention_heads': 12,\n",
       " 'num_hidden_layers': 12,\n",
       " 'type_vocab_size': 2,\n",
       " 'vocab_size': 50257,\n",
       " 'layer_norm_epsilon': 1e-05}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0-rc0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:GPT2 weights restored\n"
     ]
    }
   ],
   "source": [
    "# Load checkpointa from GPT2 TF1 model and assign it to the TF2 model and save it\n",
    "\n",
    "ckpt = tf.train.load_checkpoint(\"/Users/PRVATE/gpt-2-master/models/117M/\")\n",
    "gpt2_vars = tf.train.list_variables(\"/Users/PRVATE/gpt-2-master/models/117M/\")\n",
    "\n",
    "mapping_dict = {\n",
    "    'attn/c_attn/b' : 'self_attention/qkv/bias',\n",
    "    'attn/c_attn/w' : 'self_attention/qkv/kernel',\n",
    "    'attn/c_proj/b' : 'self_attention_output/bias',\n",
    "    'attn/c_proj/w' : 'self_attention_output/kernel',\n",
    "    'ln_1/b'        : 'ln_1/layer_norm/beta',\n",
    "    'ln_1/g'        : 'ln_1/layer_norm/gamma',\n",
    "    'ln_2/b'        : 'self_attention_layer_norm/beta',\n",
    "    'ln_2/g'        : 'self_attention_layer_norm/gamma',\n",
    "    'mlp/c_fc/b'    : 'intermediate/bias',\n",
    "    'mlp/c_fc/w'    : 'intermediate/kernel',\n",
    "    'mlp/c_proj/b'  : 'output/bias',\n",
    "    'mlp/c_proj/w'  : 'output/kernel',\n",
    "    'model/wpe'     : 'tf_transformers/gpt2/positional_embeddings/embeddings',\n",
    "    'model/wte'     : 'tf_transformers/gpt2/word_embeddings/embeddings',\n",
    "    'ln_f/b'        : 'tf_transformers/gpt2/ln_f/layer_norm/beta',\n",
    "    'ln_f/g'        : 'tf_transformers/gpt2/ln_f/layer_norm/gamma'\n",
    "    \n",
    "}\n",
    "\n",
    "def tf1_to_tf2(model):\n",
    "    gpt2_var_index = {var.name.split(':')[0]:index for index,var in enumerate(model.variables)}\n",
    "\n",
    "    for var in gpt2_vars:\n",
    "        var_name  = var[0]\n",
    "        var_shape = var[1]\n",
    "        hidden_layer = var_name.split('/')[1]\n",
    "\n",
    "        if hidden_layer.startswith('h'):\n",
    "            layer_number = hidden_layer[1:]\n",
    "            gpt2_layer_name = 'layer_{}'.format(layer_number)\n",
    "\n",
    "            for original_var in mapping_dict:\n",
    "                if original_var in var_name:\n",
    "                    w = ckpt.get_tensor(var_name)\n",
    "                    # In GPT2, most variables are starting with an extra dimension\n",
    "                    # Squeeze it if it is\n",
    "                    if tf.shape(w)[0] == 1:\n",
    "                        w = tf.squeeze(w, 0)\n",
    "                    value = mapping_dict[original_var]\n",
    "                    value = 'tf_transformers/gpt2/{}/{}/{}'.format('transformer',gpt2_layer_name, value)\n",
    "                    value_index = gpt2_var_index[value]\n",
    "                    model.variables[value_index].assign(w)\n",
    "        else:\n",
    "            for original_var in ['model/wpe', 'model/wte', 'ln_f/b', 'ln_f/g']:\n",
    "                if original_var in var_name:\n",
    "                    w = ckpt.get_tensor(var_name)\n",
    "                    # In GPT2, most variables are starting with an extra dimension\n",
    "                    # Squeeze it if it is\n",
    "                    if tf.shape(w)[0] == 1:\n",
    "                        w = tf.squeeze(w, 0)\n",
    "                    value = mapping_dict[original_var]\n",
    "                    value_index = gpt2_var_index[value]\n",
    "                    model.variables[value_index].assign(w)\n",
    "    logging.info(\"GPT2 weights restored\")\n",
    "    \n",
    "tf1_to_tf2(model_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11, 50257), dtype=float32, numpy=\n",
       "array([[[ -32.175262,  -31.924515,  -34.43251 , ...,  -40.16725 ,\n",
       "          -39.505302,  -32.213375],\n",
       "        [ -64.75787 ,  -64.33038 ,  -67.42167 , ...,  -75.922905,\n",
       "          -72.15867 ,  -64.45026 ],\n",
       "        [ -67.90772 ,  -65.774635,  -71.97972 , ...,  -74.40051 ,\n",
       "          -72.69093 ,  -65.50628 ],\n",
       "        ...,\n",
       "        [ -77.36691 ,  -75.05343 ,  -80.40854 , ...,  -83.03256 ,\n",
       "          -83.34796 ,  -78.000595],\n",
       "        [-114.61692 , -111.259094, -117.29202 , ..., -114.363396,\n",
       "         -117.813736, -112.561356],\n",
       "        [-118.39931 , -118.12673 , -122.41326 , ..., -122.978035,\n",
       "         -125.60019 , -119.76474 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_ids = tf.constant([[   50,   620,   259, 48664, 12171,   283,   318,   530,   286,\n",
    "          262, 18822]])\n",
    "results = model_layer({'input_ids': input_ids})\n",
    "\n",
    "\n",
    "# <tf.Tensor: shape=(1, 11, 50257), dtype=float32, numpy=\n",
    "# array([[[ -32.175262,  -31.924515,  -34.43251 , ...,  -40.16725 ,\n",
    "#           -39.505302,  -32.213375],\n",
    "#         [ -64.75787 ,  -64.33038 ,  -67.42167 , ...,  -75.922905,\n",
    "#           -72.15867 ,  -64.45026 ],\n",
    "#         [ -67.90772 ,  -65.774635,  -71.97972 , ...,  -74.40051 ,\n",
    "#           -72.69093 ,  -65.50628 ],\n",
    "#         ...,\n",
    "#         [ -77.36691 ,  -75.05343 ,  -80.40854 , ...,  -83.03256 ,\n",
    "#           -83.34796 ,  -78.000595],\n",
    "#         [-114.61692 , -111.259094, -117.29202 , ..., -114.363396,\n",
    "#          -117.813736, -112.561356],\n",
    "#         [-118.39931 , -118.12673 , -122.41326 , ..., -122.978035,\n",
    "#          -125.60019 , -119.76474 ]]], dtype=float32)>\n",
    "\n",
    "\n",
    "results['token_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "checkpoint_dir = '../model_directory/gpt2_base'\n",
    "ckpt    = tf.train.Checkpoint(model=model_layer)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=1)\n",
    "save_path = manager.save()\n",
    "\n",
    "\n",
    "with open('../model_directory/gpt2_base/gpt2_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_directory/gpt2_base/ckpt-1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
