{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "logging.set_verbosity(\"INFO\")\n",
    "import tensorflow as tf\n",
    "from tf_transformers.models import UNILMEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:We are overwriding `is_training` is False to `is_training` to True with `use_dropout` is False, no effects on your inference pipeline\n",
      "INFO:absl:Inputs -->\n",
      "INFO:absl:input_ids ---> Tensor(\"input_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_mask ---> Tensor(\"input_mask:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:input_type_ids ---> Tensor(\"input_type_ids:0\", shape=(None, None), dtype=int32)\n",
      "INFO:absl:Initialized Variables\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "config  = json.load(open(\"../model_directory/bert_base/bert_config.json\"))\n",
    "\n",
    "config['embedding_size'] = 1024\n",
    "config['vocab_size'] = 28996\n",
    "config['num_hidden_layers'] = 24\n",
    "config['type_vocab_size'] = 6\n",
    "config['max_position_embeddings'] = 768\n",
    "config['num_attention_heads'] = 16 # 1024/16 = 64\n",
    "config['intermediate_size'] = 4096\n",
    "config['layer_norm_epsilon'] = 1e-5\n",
    "\n",
    "\n",
    "# UniLM\n",
    "unilm_layer = UNILMEncoder(config=config,\n",
    "                  name='unilm',\n",
    "                  mask_mode='prefix',\n",
    "                  is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_transformers/last_logits_bias:0 --> (28996,)\n",
      "tf_transformers/unilm/word_embeddings/embeddings:0 --> (28996, 1024)\n",
      "tf_transformers/unilm/type_embeddings/embeddings:0 --> (6, 1024)\n",
      "tf_transformers/unilm/positional_embeddings/embeddings:0 --> (768, 1024)\n",
      "tf_transformers/unilm/embeddings/layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/embeddings/layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_0/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_0/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_0/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_0/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_1/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_1/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_1/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_1/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_2/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_2/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_2/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_2/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_3/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_3/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_3/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_3/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_4/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_4/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_4/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_4/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_5/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_5/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_5/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_5/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_6/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_6/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_6/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_6/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_7/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_7/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_7/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_7/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_8/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_8/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_8/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_8/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_9/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_9/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_9/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_9/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_10/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_10/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_10/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_10/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_11/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_11/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_11/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_11/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_12/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_12/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_12/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_12/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_13/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_13/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_13/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_13/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_14/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_14/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_14/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_14/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_15/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_15/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_15/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_15/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_16/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_16/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_16/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_16/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_17/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_17/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_17/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_17/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_18/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_18/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_18/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_18/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_19/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_19/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_19/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_19/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_20/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_20/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_20/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_20/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_21/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_21/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_21/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_21/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_22/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_22/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_22/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_22/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/query/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/query/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/key/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/key/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/value/kernel:0 --> (1024, 16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention/value/bias:0 --> (16, 64)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention_output/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention_output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/self_attention_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/intermediate/kernel:0 --> (1024, 4096)\n",
      "tf_transformers/unilm/transformer/layer_23/intermediate/bias:0 --> (4096,)\n",
      "tf_transformers/unilm/transformer/layer_23/output/kernel:0 --> (4096, 1024)\n",
      "tf_transformers/unilm/transformer/layer_23/output/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/output_layer_norm/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/transformer/layer_23/output_layer_norm/beta:0 --> (1024,)\n",
      "tf_transformers/unilm/pooler_transform/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/pooler_transform/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/mlm_layer/dense/kernel:0 --> (1024, 1024)\n",
      "tf_transformers/unilm/mlm_layer/dense/bias:0 --> (1024,)\n",
      "tf_transformers/unilm/mlm_layer/layer_normalization/gamma:0 --> (1024,)\n",
      "tf_transformers/unilm/mlm_layer/layer_normalization/beta:0 --> (1024,)\n"
     ]
    }
   ],
   "source": [
    "for var in unilm_layer.variables:\n",
    "    print(var.name , '-->', var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNNDM model from UNILM \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_recover_path = '../../../Projects/unilm/src/data/cnndm_model/cnndm_model.bin'\n",
    "model_recover = torch.load(model_recover_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Original is in float16, we need float32\n",
    "model_recover_float32 = {k: item.numpy().astype(np.float32) for k, item in model_recover.items()}\n",
    "\n",
    "# Save space\n",
    "del model_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_model_vars = ['bert.encoder.layer.{}.attention.self.query.weight',\n",
    "'bert.encoder.layer.{}.attention.self.query.bias',\n",
    "'bert.encoder.layer.{}.attention.self.key.weight',\n",
    "'bert.encoder.layer.{}.attention.self.key.bias',\n",
    "'bert.encoder.layer.{}.attention.self.value.weight',\n",
    "'bert.encoder.layer.{}.attention.self.value.bias',\n",
    "'bert.encoder.layer.{}.attention.output.dense.weight',\n",
    "'bert.encoder.layer.{}.attention.output.dense.bias',\n",
    "'bert.encoder.layer.{}.attention.output.LayerNorm.weight',\n",
    "'bert.encoder.layer.{}.attention.output.LayerNorm.bias',\n",
    "'bert.encoder.layer.{}.intermediate.dense.weight',\n",
    "'bert.encoder.layer.{}.intermediate.dense.bias',\n",
    "'bert.encoder.layer.{}.output.dense.weight',\n",
    "'bert.encoder.layer.{}.output.dense.bias',\n",
    "'bert.encoder.layer.{}.output.LayerNorm.weight',\n",
    "'bert.encoder.layer.{}.output.LayerNorm.bias'\n",
    "                  ]\n",
    "\n",
    "to_model_vars = ['tf_transformers/unilm/transformer/layer_{}/self_attention/query/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention/query/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention/key/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention/key/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention/value/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention/value/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention_output/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention_output/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention_layer_norm/gamma:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/self_attention_layer_norm/beta:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/intermediate/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/intermediate/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/output/kernel:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/output/bias:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/output_layer_norm/gamma:0',\n",
    "'tf_transformers/unilm/transformer/layer_{}/output_layer_norm/beta:0']\n",
    "\n",
    "\n",
    "assert(len(from_model_vars) == len(to_model_vars))\n",
    "mapping_dict = {}\n",
    "\n",
    "for index in range(len(from_model_vars)):\n",
    "    for i in range(config['num_hidden_layers']):\n",
    "        mapping_dict[from_model_vars[index].format(i)] = to_model_vars[index].format(i)\n",
    "    \n",
    "# Word Embeddings   \n",
    "mapping_dict['bert.embeddings.word_embeddings.weight'] = 'tf_transformers/unilm/word_embeddings/embeddings:0'\n",
    "# Positional Embedding\n",
    "mapping_dict['bert.embeddings.position_embeddings.weight'] = 'tf_transformers/unilm/positional_embeddings/embeddings:0'\n",
    "# Type Embeddings\n",
    "mapping_dict['bert.embeddings.token_type_embeddings.weight'] = 'tf_transformers/unilm/type_embeddings/embeddings:0'\n",
    "mapping_dict['bert.embeddings.LayerNorm.weight']  = 'tf_transformers/unilm/embeddings/layer_norm/gamma:0'\n",
    "mapping_dict['bert.embeddings.LayerNorm.bias']   = 'tf_transformers/unilm/embeddings/layer_norm/beta:0'\n",
    "\n",
    "# Pooler Layer\n",
    "mapping_dict['bert.pooler.dense.weight'] = 'tf_transformers/unilm/pooler_transform/kernel:0'\n",
    "mapping_dict['bert.pooler.dense.bias']   = 'tf_transformers/unilm/pooler_transform/bias:0'\n",
    "\n",
    "#Extra layers\n",
    "mapping_dict['cls.predictions.bias'] = 'tf_transformers/last_logits_bias:0'\n",
    "mapping_dict['cls.predictions.transform.dense.weight'] = 'tf_transformers/unilm/mlm_layer/dense/kernel:0'\n",
    "mapping_dict['cls.predictions.transform.dense.bias'] = 'tf_transformers/unilm/mlm_layer/dense/bias:0'\n",
    "mapping_dict['cls.predictions.transform.LayerNorm.weight'] = 'tf_transformers/unilm/mlm_layer/layer_normalization/gamma:0'\n",
    "mapping_dict['cls.predictions.transform.LayerNorm.bias'] = 'tf_transformers/unilm/mlm_layer/layer_normalization/beta:0'\n",
    "\n",
    "# mapping_dict['cls.predictions.transform.dense.weight'] = 'tf_transformers/unilm/extra_layer/dense/kernel:0'\n",
    "# mapping_dict['cls.predictions.transform.dense.bias'] = 'tf_transformers/unilm/extra_layer/dense/bias:0'\n",
    "# mapping_dict['cls.predictions.transform.LayerNorm.weight'] = 'tf_transformers/unilm/extra_layer/layer_normalization/gamma:0'\n",
    "# mapping_dict['cls.predictions.transform.LayerNorm.bias'] = 'tf_transformers/unilm/extra_layer/layer_normalization/beta:0'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
      "UNILM requires Transpose \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done assigning variables weights\n"
     ]
    }
   ],
   "source": [
    "tf_transformers_model_index_dict = {}\n",
    "for index, var in enumerate(unilm_layer.variables):\n",
    "    tf_transformers_model_index_dict[var.name] = index\n",
    "    \n",
    "# legacy_ai <-- HuggingFace\n",
    "assigned_map = []\n",
    "assigned_map_values = []\n",
    "print(1000 * \"*\")\n",
    "print(\"UNILM requires Transpose \")\n",
    "for original_var, legacy_var in mapping_dict.items():\n",
    "\n",
    "    index = tf_transformers_model_index_dict[legacy_var]        \n",
    "    # Transpose UNILM weigths also (No idea why)\n",
    "    if 'query/kernel:0' in legacy_var or 'key/kernel:0' in legacy_var or 'value/kernel:0' in legacy_var:\n",
    "        # huggingface (2D) to tf_transformers (3D)\n",
    "        unilm_layer.variables[index].assign(tf.reshape(model_recover_float32[original_var].transpose(), (1024, 16, 64)))\n",
    "        assigned_map.append((original_var, legacy_var))\n",
    "        assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    if 'query/bias:0' in legacy_var or 'key/bias:0' in legacy_var or 'value/bias:0' in legacy_var:\n",
    "        # huggingface (2D) to tf_transformers (3D)\n",
    "        unilm_layer.variables[index].assign(tf.reshape(model_recover_float32[original_var], (16, 64)))\n",
    "        assigned_map.append((original_var, legacy_var))\n",
    "        assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "        continue\n",
    "    # Transpose UniLM variables here\n",
    "    if 'intermediate/kernel:0' in legacy_var or 'output/kernel:0' in legacy_var:\n",
    "        # huggingface (2D) to tf_transformers (3D)\n",
    "        unilm_layer.variables[index].assign(model_recover_float32[original_var].transpose())\n",
    "        assigned_map.append((original_var, legacy_var))\n",
    "        assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "        continue\n",
    "    # Positional Embeddings\n",
    "    if 'positional_embeddings' in legacy_var:\n",
    "        unilm_layer.variables[index].assign(model_recover_float32[original_var])\n",
    "        assigned_map.append((original_var, legacy_var))\n",
    "        assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "        continue\n",
    "     \n",
    "    # Transpose here (No idea why)\n",
    "    if 'mlm_layer/dense/kernel' in legacy_var:\n",
    "        # huggingface (2D) to tf_transformers (3D)\n",
    "        unilm_layer.variables[index].assign(model_recover_float32[original_var].transpose())\n",
    "        assigned_map.append((original_var, legacy_var))\n",
    "        assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "        continue\n",
    "        \n",
    "    unilm_layer.variables[index].assign(model_recover_float32[original_var])\n",
    "    assigned_map.append((original_var, legacy_var))\n",
    "    assigned_map_values.append((tf.reduce_sum(model_recover_float32[original_var]).numpy(), tf.reduce_sum(unilm_layer.variables[index]).numpy()))\n",
    "\n",
    "    \n",
    "logging.info(\"Done assigning variables weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ids tf.Tensor(1497, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.constant([[  101, 17851,   117,  1699,   113, 13597,   114,  1109,  1497, 16810,\n",
    "          2020,  1126,  4449,  1154,  1103,  5683,  1104,  1528,  7635,  1116,\n",
    "          6945,  4573, 17600,  6744,  9031,  1115,  1119,  1108,  1136,  4484,\n",
    "          1104,  1251,  1888,  8145,  1121,  1113,  2313,  1103,  4261,   119,\n",
    "             2, 17851, 16810,   139, 10835,  5981,  1500, 13597,  1115,   107,\n",
    "          1177,  1677,  1185,  6581,  1127,  1215,  1107,  1103,  5683,  4449,\n",
    "           119,   107,     3,  1124,  1896,   117,   107,   138,  1825,  1150,\n",
    "          1144,  1216,   170,  1888,  2993,  1106,  2411,  1660,  1122,  1106,\n",
    "          1103, 17718,   119,   107,     4,  5981,   112,   188,  7640,  2812,\n",
    "          3711,  1118,  1160,  6959,   117,  1528,  3828,   139,  2723,  1181,\n",
    "          1105,  1497,  2123, 11492,   117,  1104,   170,  2765,  2179,  1888,\n",
    "          4000,  1103,  5871,  8674,  1158,  1509,  3071,  1121,  1113,  2313,\n",
    "          1528,  7635,  1116,  6945,  4573, 17600,  1112,  1122,  7573,  1154,\n",
    "          1103,  1497, 14316,   119,  1398,  4214,  1113,  2313,  1127,  1841,\n",
    "           119,     5,  2123, 11492,  1105,   139,  2723,  1181,  2103,  1115,\n",
    "          1103,  1888,  1108,  6203,  1121,   170,  2179,  1120,  1103, 24069,\n",
    "          1751,   119,     6,  1109,  1160,  5873,  1758,  1103,  3155,  1888,\n",
    "           117,  1133,  1225,  1136,  2112,  1122,  1113,  1147, 12045,   119,\n",
    "          1109,  5873,  1163,  1115,  1152,  2542,  1103,  1888,   117,   102,\n",
    "           103]])\n",
    "\n",
    "input_type_ids = tf.constant([[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "         4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5]])\n",
    "    \n",
    "input_mask = tf.ones_like(input_type_ids)[0][:-1]\n",
    "input_mask = tf.expand_dims(tf.concat([input_mask, [0]], axis=0), 0)\n",
    "\n",
    "\n",
    "inputs_summarisation = {'input_ids':  input_ids, \n",
    "          'input_mask': input_mask, \n",
    "          'input_type_ids': input_type_ids}\n",
    "\n",
    "outputs = unilm_layer(inputs_summarisation)\n",
    "\n",
    "print(\"Predicted ids\", tf.argmax(outputs['last_token_logits'][0]))\n",
    "# Reference is 1497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "checkpoint_dir = '../model_directory/unilm_cnndm/'\n",
    "ckpt    = tf.train.Checkpoint(model=unilm_layer)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=1)\n",
    "save_path = manager.save()\n",
    "\n",
    "with open(\"../model_directory/unilm_cnndm/unilm_config.json\", \"w\") as f:\n",
    "    json.dump(config,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
