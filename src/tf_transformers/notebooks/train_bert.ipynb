{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe87f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from tf_transformers.core import TPUTrainer\n",
    "from tf_transformers.data import TFReader\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from transformers import AlbertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62a8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dynamic_masking_from_features(\n",
    "                            max_seq_len,\n",
    "                            max_predictions_per_batch,\n",
    "                            vocab_size,\n",
    "                            cls_id,\n",
    "                            sep_id,\n",
    "                            unk_id,\n",
    "                            pad_id,\n",
    "                            mask_id\n",
    "                            ):\n",
    "    \n",
    "    \"\"\"Dynamic Masking from input_ids (saved as tfrecord)\"\"\"\n",
    "    # Truncate inputs to a maximum length.\n",
    "    trimmer = tf_text.RoundRobinTrimmer(max_seq_length=max_seq_len)\n",
    "\n",
    "    # Random Selector\n",
    "    random_selector = tf_text.RandomItemSelector(\n",
    "        max_selections_per_batch=max_predictions_per_batch,\n",
    "        selection_rate=0.2,\n",
    "        unselectable_ids=[cls_id, sep_id, unk_id, pad_id]\n",
    "    )\n",
    "\n",
    "    # Mask Value chooser (Encapsulates the BERT MLM token selection logic)\n",
    "    mask_values_chooser = tf_text.MaskValuesChooser(vocab_size, mask_id, 0.8)\n",
    "    \n",
    "    \n",
    "    def map_mlm(item):\n",
    "        \n",
    "        segments = item['input_ids']\n",
    "        trimmed_segments = trimmer.trim([segments])\n",
    "\n",
    "        # We replace trimmer with slice [:_MAX_SEQ_LEN-2] operation # 2 to add CLS and SEP\n",
    "        # input_ids = item['input_ids'][:_MAX_SEQ_LEN-2]\n",
    "\n",
    "        # Combine segments, get segment ids and add special tokens.\n",
    "        segments_combined, segment_ids = tf_text.combine_segments(\n",
    "              trimmed_segments,\n",
    "              start_of_sequence_id=cls_id,\n",
    "              end_of_segment_id=sep_id)\n",
    "\n",
    "        # We replace segment with concat\n",
    "        # input_ids = tf.concat([[_START_TOKEN], input_ids, [_END_TOKEN]], axis=0)\n",
    "\n",
    "        # Apply dynamic masking\n",
    "        masked_token_ids, masked_pos, masked_lm_ids = tf_text.mask_language_model(\n",
    "          segments_combined,\n",
    "          item_selector=random_selector,\n",
    "          mask_values_chooser=mask_values_chooser)\n",
    "\n",
    "        # Prepare and pad combined segment inputs\n",
    "        input_word_ids, input_mask = tf_text.pad_model_inputs(\n",
    "            masked_token_ids, max_seq_length=max_seq_len)\n",
    "        input_type_ids, _ = tf_text.pad_model_inputs(\n",
    "            segment_ids, max_seq_length=max_seq_len)\n",
    "\n",
    "        # Prepare and pad masking task inputs\n",
    "        # Masked lm weights will mask the weights\n",
    "        masked_lm_positions, masked_lm_weights = tf_text.pad_model_inputs(\n",
    "          masked_pos, max_seq_length=max_predictions_per_batch)\n",
    "        masked_lm_ids, _ = tf_text.pad_model_inputs(\n",
    "          masked_lm_ids, max_seq_length=max_predictions_per_batch)\n",
    "\n",
    "        inputs = {}\n",
    "        inputs['input_ids'] = input_word_ids\n",
    "        inputs['input_type_ids'] = input_type_ids\n",
    "        inputs['input_mask'] = input_mask\n",
    "        inputs['masked_lm_positions'] = masked_lm_positions\n",
    "\n",
    "        labels = {}\n",
    "        labels['masked_lm_labels'] = masked_lm_ids\n",
    "        labels['masked_lm_weights']   = masked_lm_weights # Mask\n",
    "\n",
    "        return (inputs, labels)\n",
    "    \n",
    "    return map_mlm\n",
    "\n",
    "    \n",
    "    \n",
    "def get_tfdataset_from_tfrecords(tfrecord_path_list):\n",
    "    \"\"\"Get tf dataset from tfrecords\"\"\"\n",
    "    all_files = []\n",
    "    for tfrecord_path in tfrecord_path_list:\n",
    "        all_files.extend(glob.glob(\"{}/*.tfrecord\".format(tfrecord_path)))\n",
    "    schema    = json.load(open(\"{}/schema.json\".format(tfrecord_path)))\n",
    "    tf_reader = TFReader(schema=schema, \n",
    "                        tfrecord_files=all_files)\n",
    "    train_dataset = tf_reader.read_record(\n",
    "                                      )\n",
    "    return train_dataset\n",
    "\n",
    "def filter_by_length(x, min_sen_len):\n",
    "    \"\"\"Filter by minimum sentence length (subwords)\"\"\"\n",
    "    return tf.squeeze(tf.greater_equal(tf.shape(x['input_ids']) ,tf.constant(min_sen_len)), axis=0)\n",
    "\n",
    "def filter_by_batch(x, y, batch_size):\n",
    "    \"\"\"Filter by batch size\"\"\"\n",
    "    x_batch = tf.shape(x['input_ids'])[0]\n",
    "    return tf.equal(x_batch, tf.constant(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63823e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Model\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"attention_probs_dropout_prob\": 0.1,\n",
    "        \"hidden_act\": \"gelu\",\n",
    "        \"intermediate_act\": \"gelu\",\n",
    "        \"hidden_dropout_prob\": 0.1,\n",
    "        \"embedding_size\": 768,\n",
    "        \"initializer_range\": 0.02,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"max_position_embeddings\": 512,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"attention_head_size\": 64,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"type_vocab_size\": 2,\n",
    "        \"vocab_size\": tokenizer.vocab_size,\n",
    "        \"layer_norm_epsilon\": 1e-12\n",
    "    }\n",
    "    \n",
    "    from tf_transformers.models import BertModel\n",
    "    model = BertModel.from_config(config,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  use_masked_lm_positions=True, # Add batch_size to avoid dynamic shapes\n",
    "                                  return_all_layer_outputs=True) \n",
    "\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "    \"\"\"Optimizer\"\"\"\n",
    "    LEARNING_RATE = 1e-04\n",
    "    NUM_TRAIN_STEPS = 200000\n",
    "    NUM_WARMUP_STEPS = 30000\n",
    "    OPTIMIZER_TYPE = \"lamb\"\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=LEARNING_RATE, \n",
    "                                                 num_train_steps=NUM_TRAIN_STEPS,\n",
    "                                                 num_warmup_steps=NUM_WARMUP_STEPS, \n",
    "                                                 optimizer_type=OPTIMIZER_TYPE)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    \"\"\"Joint loss over all layers\"\"\"    \n",
    "    loss_dict = {}\n",
    "    loss_holder = []\n",
    "    for layer_count, per_layer_output in enumerate(y_pred_dict['all_layer_token_logits']):\n",
    "        \n",
    "        loss = cross_entropy_loss(labels=y_true_dict['masked_lm_labels'], \n",
    "                                logits=per_layer_output, \n",
    "                                label_weights=y_true_dict['masked_lm_weights'])\n",
    "        loss_dict['loss_{}'.format(layer_count+1)] = loss\n",
    "        loss_holder.append(loss)\n",
    "    loss_dict['loss'] = tf.reduce_mean(loss_holder, axis=0)\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a801949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "class MLMCallback():\n",
    "    \"\"\"Simple MLM Callback to check progress of the training\"\"\"\n",
    "    def __init__(self, tokenizer, validation_sentences, top_k=10):\n",
    "        \"\"\"Init\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.validation_sentences = validation_sentences\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def get_inputs(self):\n",
    "        \"\"\"Text to features\"\"\"\n",
    "        inputs = self.tokenizer(self.validation_sentences, padding=True, return_tensors=\"tf\")\n",
    "        inputs_tf = {}\n",
    "        inputs_tf[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "        inputs_tf[\"input_type_ids\"] = inputs[\"token_type_ids\"]\n",
    "        inputs_tf[\"input_mask\"] = inputs[\"attention_mask\"]\n",
    "        \n",
    "        seq_length = tf.shape(inputs_tf['input_ids'])[1]\n",
    "        inputs_tf['masked_lm_positions'] = tf.zeros_like(inputs_tf[\"input_ids\"]) + tf.range(seq_length)\n",
    "\n",
    "        return inputs_tf\n",
    "    \n",
    "    \n",
    "    def __call__(self, trainer_params):\n",
    "        \"\"\"Main Call\"\"\"\n",
    "        model = trainer_params['model']\n",
    "        inputs_tf = self.get_inputs()\n",
    "        outputs_tf = model(inputs_tf)\n",
    "        \n",
    "        # Get masked positions from each sentence\n",
    "        masked_positions = tf.argmax(tf.equal(inputs_tf[\"input_ids\"], self.tokenizer.mask_token_id), axis=1)\n",
    "        for layer_count,layer_logits in enumerate(outputs_tf['all_layer_token_logits']):\n",
    "            print(\"Layer {}\".format(layer_count+1))\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            for i,logits in enumerate(layer_logits):\n",
    "                mask_token_logits = logits[masked_positions[i]]\n",
    "                # 0 for probs and 1 for indexes from tf.nn.top_k\n",
    "                top_words = tokenizer.decode(tf.nn.top_k(mask_token_logits, k = self.top_k)[1].numpy())\n",
    "                print(\"Input ----> {}\".format(self.validation_sentences[i]))\n",
    "                print(\"Predicted words ----> {}\".format(top_words.split()))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51f223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938fcb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f189766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define Constants\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "MAX_PREDICTIONS_PER_BATCH = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TFRECORDS_PATH = ['home/sidhu/Datasets/TFRECORD_BOOKCORPUS',\n",
    "                  '/home/sidhu/Datasets/TFRECORD_WIKI']\n",
    "TPU_ADDRESS = 'local'\n",
    "DTYPE = 'bf16'\n",
    "\n",
    "MODEL_DIR  ='bert_joint'\n",
    "EPOCHS = 3\n",
    "STEPS_PER_EPOCH = 50000\n",
    "CALLBACK_STEPS = 5000\n",
    "TRAINING_LOSS_NAMES = ['loss_1',\n",
    " 'loss_2',\n",
    " 'loss_3',\n",
    " 'loss_4',\n",
    " 'loss_5',\n",
    " 'loss_6',\n",
    " 'loss_7',\n",
    " 'loss_8',\n",
    " 'loss_9',\n",
    " 'loss_10',\n",
    " 'loss_11',\n",
    " 'loss_12']\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "validation_sentences = [\n",
    "\n",
    "    'Read the rest of this [MASK] to understand things in more detail.',\n",
    "    'I want to buy the [MASK] because it is so cheap.', \n",
    "    'The [MASK] was amazing.',\n",
    "    'Sachin Tendulkar is one of the [MASK] palyers in the world.',\n",
    "    '[MASK] is the capital of France.', \n",
    "    'Machine Learning requires [MASK]', \n",
    "    'He is working as a [MASK]', \n",
    "    'She is working as a [MASK]'\n",
    "\n",
    "]\n",
    "mlm_callback = MLMCallback(tokenizer, validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f08da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TF dataset\n",
    "\n",
    "dynamic_mlm_fn = dynamic_masking_from_features(\n",
    "                            MAX_SEQ_LEN,\n",
    "                            MAX_PREDICTIONS_PER_BATCH,\n",
    "                            tokenizer.vocab_size,\n",
    "                            tokenizer.cls_token_id,\n",
    "                            tokenizer.sep_token_id,\n",
    "                            tokenizer.unk_token_id,\n",
    "                            tokenizer.pad_token_id,\n",
    "                            tokenizer.mask_token_id\n",
    "                            )\n",
    "\n",
    "train_dataset = get_tfdataset_from_tfrecords(TFRECORDS_PATH)\n",
    "train_dataset = train_dataset.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=BATCH_SIZE))\n",
    "train_dataset = train_dataset.map(dynamic_mlm_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.filter(lambda x, y: filter_by_batch(x, y, BATCH_SIZE))\n",
    "train_dataset = train_dataset.shuffle(100)\n",
    "train_dataset = train_dataset.prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60fb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Trainer\n",
    "trainer =  TPUTrainer(\n",
    "    tpu_address=TPU_ADDRESS,\n",
    "    dtype=DTYPE\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "\n",
    "trainer.run(\n",
    "    model_fn=get_model,\n",
    "    optimizer_fn=get_optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    train_loss_fn=lm_loss,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    model_checkpoint_dir=MODEL_DIR, # gs://tft_free/\n",
    "    batch_size=BATCH_SIZE,\n",
    "    training_loss_names=TRAINING_LOSS_NAMES,\n",
    "    validation_loss_names=None,\n",
    "    validation_dataset=None,\n",
    "    validation_loss_fn=None,\n",
    "    validation_interval_steps=None,\n",
    "    steps_per_call=100,\n",
    "    enable_xla=False,\n",
    "    callbacks=[mlm_callback],\n",
    "    callbacks_interval_steps=[CALLBACK_STEPS],\n",
    "    overwrite_checkpoint_dir=True,\n",
    "    max_number_of_models=10,\n",
    "    model_save_interval_steps=None,\n",
    "    repeat_dataset=True\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "os.system(\"gsutil -m cp -R {} gs://tft_free/{}\".format(MODEL_DIR, MODEL_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
