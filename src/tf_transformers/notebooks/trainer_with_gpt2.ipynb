{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08540c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This Notebook is intented to build a trainer\n",
    "### using GPT2 and CNN daily mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bbafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a92817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 30 03:35:51 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.126.02   Driver Version: 418.126.02   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    45W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    44W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacd7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/USER/TF_NEW/tf-transformers/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a53cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import functools\n",
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5e848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "from tf_transformers.data.utils import separate_x_y, auto_batch\n",
    "from tf_transformers.losses import cross_entropy_loss\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.models import GPT2Model as Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc8088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405c2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'take_sample': True, 'name': 'cnn_dailymail', 'batch_size': 32, 'max_seq_length': 512, 'max_target_length': 64, 'train_columns': ['input_ids', 'labels', 'labels_mask'], 'val_columns': ['input_ids', 'labels', 'labels_mask', 'highlights'], 'x_keys': ['input_ids'], 'y_keys': ['labels', 'labels_mask', 'highlights'], 'src_column_name': 'article', 'target_column_name': 'highlights'}, 'model': {'name': 'gpt2'}, 'text_generation': {'max_iterations': 64, 'mode': 'greedy'}}\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path=\"confs\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd419c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9107d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80578f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load GPT2 Model\n",
    "\n",
    "# model , model_config = GPT2Model.from_pretrained(model_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a186a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load CNN daily mail dataset\n",
    "dataset = load_from_disk(\"/home/jovyan/PRE_MODELS/HuggingFace_models/datasets/cnn_dailymail/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2964cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_tokenizer(e,\n",
    "                  src_column_name,\n",
    "                  target_column_name,\n",
    "                  max_seq_length, \n",
    "                  max_target_length):\n",
    "    \"\"\"Convert src text and target text to input_ids\"\"\"\n",
    "    inputs  = tokenizer(e[src_column_name], truncation=True, padding=False, max_length=max_seq_length)\n",
    "    targets = tokenizer(e[target_column_name], truncation=True, padding=False, max_length=max_target_length)\n",
    "    e['input_ids'] = inputs['input_ids']\n",
    "    e['target_input_ids'] = targets['input_ids']\n",
    "    return e\n",
    "\n",
    "def get_dataset(dataset, tokenizer, mode, cfg):\n",
    "    \n",
    "    take_sample = cfg.dataset.take_sample\n",
    "    batch_size = cfg.dataset.batch_size\n",
    "    x_keys = cfg.dataset.x_keys\n",
    "    y_keys = cfg.dataset.y_keys\n",
    "    src_column_name = cfg.dataset.src_column_name\n",
    "    target_column_name = cfg.dataset.target_column_name\n",
    "    max_seq_length = cfg.dataset.max_seq_length\n",
    "    max_target_length = cfg.dataset.max_target_length\n",
    "    \n",
    "    if mode in ['train']:\n",
    "        COLUMNS = cfg.dataset.train_columns\n",
    "        shuffle = True\n",
    "    if mode in ['val']:\n",
    "        COLUMNS = cfg.dataset.val_columns\n",
    "        shuffle = False\n",
    "        \n",
    "    if take_sample:\n",
    "        dataset = dataset.select(range(5000))\n",
    "    \n",
    "    # Batched is better here\n",
    "    fn_kwargs = { 'src_column_name': src_column_name, \n",
    "                  'target_column_name': target_column_name,\n",
    "                  'max_seq_length': max_seq_length, \n",
    "                  'max_target_length': max_target_length}\n",
    "    dataset = dataset.map(map_tokenizer,batched=True, fn_kwargs=fn_kwargs)\n",
    "    # Merge it together (for Encoder only models)\n",
    "    dataset = dataset.map(lambda x: {\"input_ids\": x[\"input_ids\"] + x[\"target_input_ids\"]})\n",
    "    dataset = dataset.map(lambda x: {\"input_ids\": x[\"input_ids\"][:-1], \n",
    "                                                  \"labels\": x[\"input_ids\"][1:], \n",
    "                                                  \"labels_mask\": [1] * len(x[\"input_ids\"][1:])})\n",
    "    # HF dataset to tf dataset\n",
    "    dataset.set_format(type=\"tensorflow\", columns=COLUMNS)\n",
    "    features = {}\n",
    "    for x in COLUMNS:\n",
    "        if isinstance(dataset[x], tf.RaggedTensor):\n",
    "            if dataset[x].dtype in [tf.int32, tf.int64]:\n",
    "                if x == 'labels_mask': # labels_mask should be 0\n",
    "                    features[x] = tf.cast(dataset[x], dtype=tf.int32).to_tensor(default_value=0,\n",
    "                                                               shape=[None, max_seq_length+max_target_length])\n",
    "                    continue\n",
    "                features[x] = tf.cast(dataset[x], dtype=tf.int32).to_tensor(default_value=tokenizer.pad_token_id,\n",
    "                                                               shape=[None, max_seq_length+max_target_length])\n",
    "        else:\n",
    "            features[x] = dataset[x]\n",
    "    tfdataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "    tfdataset = auto_batch(tfdataset, \n",
    "                           batch_size, \n",
    "                           shuffle=shuffle, \n",
    "                           x_keys=x_keys, \n",
    "                           y_keys=y_keys)\n",
    "    return tfdataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8841ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jovyan/PRE_MODELS/HuggingFace_models/datasets/cnn_dailymail/train/cache-958823b5499194f8.arrow\n",
      "Loading cached processed dataset at /home/jovyan/PRE_MODELS/HuggingFace_models/datasets/cnn_dailymail/train/cache-39cffd77994cbea9.arrow\n",
      "Loading cached processed dataset at /home/jovyan/PRE_MODELS/HuggingFace_models/datasets/cnn_dailymail/train/cache-cd7112db9e70814b.arrow\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4bf2e4aad040baa3bb56d53600eee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f096bd7e19fc4f07bd61aa060f6209ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871d44cdc5e64be196d3a227b92d5304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tfdataset =  get_dataset( dataset['train'], tokenizer, \"train\", cfg)\n",
    "tfdataset_validation =  get_dataset( dataset['validation'], tokenizer, \"val\", cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "915b3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model , model_config = Model.from_pretrained(\"gpt2\", return_layer=False)\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "    optimizer, learning_rate_fn = create_optimizer(init_lr=2e-05, \n",
    "                                                  num_train_steps=1000,\n",
    "                                                  num_warmup_steps=100)\n",
    "    return optimizer\n",
    "\n",
    "def lm_loss(y_true_dict, y_pred_dict):\n",
    "    loss = cross_entropy_loss(labels=y_true_dict['labels'], \n",
    "                             logits=y_pred_dict['token_logits'], \n",
    "                             label_weights=y_true_dict['labels_mask'])\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1897ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bd041fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from absl import logging\n",
    "\n",
    "from tf_transformers.core import keras_utils\n",
    "from tf_transformers.core.distribute_utils import get_distribution_strategy\n",
    "from tf_transformers.core.performance_utils import (\n",
    "    configure_optimizer,\n",
    "    get_tf_dtype,\n",
    "    is_float16,\n",
    "    set_mixed_precision_policy,\n",
    ")\n",
    "\n",
    "logging.get_absl_logger().name = \"trainer\"\n",
    "\n",
    "\n",
    "def flat_metric_dict(metric_dict):\n",
    "    \"\"\"Flatten the dict\"\"\"\n",
    "    dict_flatten = {}\n",
    "    dict_flatten['steps'] = list(metric_dict.keys())\n",
    "    for _key, value in metric_dict.items():\n",
    "        for sub_key, sub_value in value.items():\n",
    "            if sub_key not in dict_flatten:\n",
    "                dict_flatten[sub_key] = [sub_value]\n",
    "            else:\n",
    "                dict_flatten[sub_key].append(sub_value)\n",
    "    return dict_flatten\n",
    "\n",
    "\n",
    "def save_model_checkpoints(model, overwrite_checkpoint_dir, model_checkpoint_dir, max_number_of_models):\n",
    "    # Model checkpoint\n",
    "    if not overwrite_checkpoint_dir:\n",
    "        import os\n",
    "\n",
    "        if os.path.exists(model_checkpoint_dir):\n",
    "            raise FileExistsError(\"Model directory exists\")\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory=model_checkpoint_dir, max_to_keep=max_number_of_models)\n",
    "    return manager\n",
    "\n",
    "\n",
    "def get_loss_metric_dict(model, dataset, loss_fn, validation_dataset, validation_loss_fn):\n",
    "    for (batch_inputs, batch_labels) in dataset.take(1):\n",
    "        model_outputs = model(batch_inputs)\n",
    "        train_loss_dict = loss_fn(batch_labels, model_outputs)\n",
    "        training_loss_dict_metric = {name: tf.keras.metrics.Mean(name, dtype=tf.float32) for name in train_loss_dict}\n",
    "\n",
    "    training_loss_dict_metric[\"learning_rate\"] = tf.keras.metrics.Mean(\n",
    "        \"learning_rate\", dtype=tf.float32\n",
    "    )  # We store learning rate here and reset after every global steps\n",
    "\n",
    "    validation_loss_dict_metric = {}\n",
    "    if validation_dataset and validation_loss_fn:\n",
    "        for (batch_inputs, batch_labels) in dataset.take(1):\n",
    "            model_outputs = model(batch_inputs)\n",
    "            valid_loss_dict = validation_loss_fn(batch_labels, model_outputs)\n",
    "            validation_loss_dict_metric = {\n",
    "                name: tf.keras.metrics.Mean(name, dtype=tf.float32) for name in valid_loss_dict\n",
    "            }\n",
    "\n",
    "    return training_loss_dict_metric, validation_loss_dict_metric\n",
    "\n",
    "\n",
    "def get_and_reset_metric_from_dict(metric_dict):\n",
    "    if not metric_dict:\n",
    "        return {}\n",
    "    metric_result = {name: metric.result().numpy() for name, metric in metric_dict.items()}\n",
    "    for _name, metric in metric_dict.items():\n",
    "        metric.reset_states()\n",
    "    return metric_result\n",
    "\n",
    "\n",
    "def get_tensorboard_writers(model_checkpoint_dir):\n",
    "    train_log_dir = model_checkpoint_dir + \"/logs/train\"\n",
    "    test_log_dir = model_checkpoint_dir + \"/logs/dev\"\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    return train_summary_writer, test_summary_writer\n",
    "\n",
    "\n",
    "def write_metrics(metric_dict, writer, step):\n",
    "    with writer.as_default():\n",
    "        for name, result in metric_dict.items():\n",
    "            tf.summary.scalar(name, result, step=step)\n",
    "\n",
    "\n",
    "def train_and_eval(\n",
    "    model,\n",
    "    optimizer,\n",
    "    strategy,\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    steps_per_call,\n",
    "    train_dataset_iter,\n",
    "    train_loss_fn,\n",
    "    training_loss_dict_metric,\n",
    "    validation_dataset_distributed,\n",
    "    validation_loss_fn,\n",
    "    validation_loss_dict_metric,\n",
    "    validation_interval_steps,\n",
    "    mixed_precision,\n",
    "    callbacks,\n",
    "    callbacks_interval_steps,\n",
    "    trainer_kwargs,\n",
    "    checkpoint_manager,\n",
    "    model_checkpoint_dir,\n",
    "    model_save_interval_steps,\n",
    "):\n",
    "    def save_model(epoch_end=False):\n",
    "        if not epoch_end:\n",
    "            if model_save_interval_steps:\n",
    "                if global_step % model_save_interval_steps == 0:\n",
    "                    checkpoint_manager.save()\n",
    "                    logging.info(\"Model saved at step {}\".format(global_step))\n",
    "        else:\n",
    "            checkpoint_manager.save()\n",
    "            logging.info(\"Model saved at epoch {}\".format(epoch))\n",
    "\n",
    "    # Train Functions\n",
    "    @tf.function\n",
    "    def do_train(iterator):\n",
    "        \"\"\"The step function for one training step\"\"\"\n",
    "\n",
    "        def train_step(dist_inputs):\n",
    "            \"\"\"The computation to run on each device.\"\"\"\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            with tf.GradientTape() as tape:\n",
    "                model_outputs = model(batch_inputs)\n",
    "                loss = train_loss_fn(batch_labels, model_outputs)\n",
    "                if isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                    loss_scaled = {name: optimizer.get_scaled_loss(loss_value) for name, loss_value in loss.items()}\n",
    "                # TODO\n",
    "                # Scales down the loss for gradients to be invariant from replicas.\n",
    "                # loss = loss / strategy.num_replicas_in_sync\n",
    "            if mixed_precision:\n",
    "                scaled_gradients = tape.gradient(loss_scaled[\"loss\"], model.trainable_variables)\n",
    "                grads = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "            else:\n",
    "                grads = tape.gradient(loss[\"loss\"], model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
    "            return loss\n",
    "\n",
    "        for _ in tf.range(tf.convert_to_tensor(steps_per_call)):\n",
    "            dist_inputs = next(iterator)\n",
    "            loss = strategy.run(train_step, args=(dist_inputs,))\n",
    "            # strategy reduce\n",
    "            loss = {\n",
    "                name: strategy.reduce(tf.distribute.ReduceOp.MEAN, loss_value, axis=None)\n",
    "                for name, loss_value in loss.items()\n",
    "            }\n",
    "            for name, loss_value in loss.items():\n",
    "                training_loss = training_loss_dict_metric[name]\n",
    "                training_loss.update_state(loss_value)\n",
    "            # Get current learning rate\n",
    "            if isinstance(optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                current_lr = optimizer._optimizer._decayed_lr(tf.float32)\n",
    "            else:\n",
    "                current_lr = optimizer._decayed_lr(tf.float32)\n",
    "            training_loss_dict_metric[\"learning_rate\"].update_state(current_lr)\n",
    "            # training_result = get_and_reset_metric_from_dict(training_loss_dict_metric)\n",
    "\n",
    "    # do validation\n",
    "    def do_validation(validation_dataset_distributed):\n",
    "        \"\"\"Validation step\"\"\"\n",
    "\n",
    "        @tf.function\n",
    "        def _validate_step(dist_inputs):\n",
    "\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            model_outputs = model(batch_inputs)\n",
    "            loss = validation_loss_fn(batch_labels, model_outputs)\n",
    "            return loss\n",
    "\n",
    "        if not epoch_end:\n",
    "            if (\n",
    "                validation_dataset_distributed\n",
    "                and validation_loss_fn\n",
    "                and validation_interval_steps\n",
    "                and (global_step % validation_interval_steps == 0)\n",
    "            ):\n",
    "                logging.info(\"Validation in progress at step {} . . . .\".format(global_step))\n",
    "                with tqdm.tqdm(validation_dataset_distributed, unit=\" Val batch \") as val_batches:\n",
    "                    for dist_inputs in val_batches:\n",
    "                        loss = strategy.run(_validate_step, args=(dist_inputs,))\n",
    "                        for name, loss_value in loss.items():\n",
    "                            loss_value = strategy.reduce(tf.distribute.ReduceOp.MEAN, loss_value, axis=None)\n",
    "                            validation_loss = validation_loss_dict_metric[name]\n",
    "                            validation_loss.update_state(loss_value)\n",
    "\n",
    "                validation_result = get_and_reset_metric_from_dict(validation_loss_dict_metric)\n",
    "                validation_history[global_step] = validation_result\n",
    "                write_metrics(validation_result, val_summary_writer, global_step)\n",
    "                logging.info(\"Validation result at step {}\".format(validation_result))\n",
    "                print(\"\\n\")\n",
    "        else:\n",
    "            if validation_dataset_distributed and validation_loss_fn:\n",
    "                logging.info(\"Validation in progress at epoch end {} . . . .\".format(epoch))\n",
    "                with tqdm.tqdm(validation_dataset_distributed, unit=\" Val batch \") as val_batches:\n",
    "                    for dist_inputs in val_batches:\n",
    "                        loss = strategy.run(_validate_step, args=(dist_inputs,))\n",
    "                        for name, loss_value in loss.items():\n",
    "                            loss_value = strategy.reduce(tf.distribute.ReduceOp.MEAN, loss_value, axis=None)\n",
    "                            validation_loss = validation_loss_dict_metric[name]\n",
    "                            validation_loss.update_state(loss_value)\n",
    "\n",
    "                validation_result = get_and_reset_metric_from_dict(validation_loss_dict_metric)\n",
    "                write_metrics(validation_result, val_summary_writer, global_step)\n",
    "                # validation_history[global_step] = validation_result\n",
    "                logging.info(\"Validation result at epoch {} is {}\".format(epoch, validation_result))\n",
    "                print(\"\\n\")\n",
    "\n",
    "    def do_callbacks(callbacks):\n",
    "        \"\"\"Call callbacks\"\"\"\n",
    "        if not epoch_end:\n",
    "            callback_scores = None\n",
    "            if callbacks and callbacks_interval_steps:\n",
    "                logging.info(\"Callbacks in progress at step {} . . . .\".format(global_step))\n",
    "                callback_scores = []\n",
    "                for callback, callback_steps in zip(callbacks, callbacks_interval_steps):\n",
    "                    if callback_steps and (global_step % callback_steps == 0):\n",
    "                        score = callback(trainer_kwargs)\n",
    "                        callback_scores.append(score)\n",
    "                    else:\n",
    "                        callback_scores.append(None)\n",
    "            return callback_scores\n",
    "        else:\n",
    "            callback_scores = None\n",
    "            if callbacks:\n",
    "                logging.info(\"Callbacks in progress at epoch end {} . . . .\".format(epoch))\n",
    "                callback_scores = []\n",
    "                for callback in callbacks:\n",
    "                    score = callback(trainer_kwargs)\n",
    "                    callback_scores.append(score)\n",
    "            return callback_scores\n",
    "\n",
    "    # Loop starts here\n",
    "    # Get Tensorboard writers\n",
    "    train_summary_writer, val_summary_writer = get_tensorboard_writers(model_checkpoint_dir)\n",
    "    validation_history = {}\n",
    "    training_history = {}\n",
    "    global_step = 0\n",
    "    epoch_end = False\n",
    "    STEPS = steps_per_epoch // steps_per_call\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # start_epoch_time = time.time()\n",
    "        with tqdm.trange(STEPS, unit=\"batch \") as tepoch:\n",
    "            for step in tepoch:\n",
    "                steps_covered = (step + 1) * steps_per_call\n",
    "                global_step += steps_per_call\n",
    "                tepoch.set_description(\n",
    "                    \"Epoch {}/{} --- Step {}/{} --- \".format(epoch, epochs, steps_covered, steps_per_epoch)\n",
    "                )\n",
    "                # Call Train\n",
    "                do_train(train_dataset_iter)\n",
    "\n",
    "                # Call Validation\n",
    "                do_validation(validation_dataset_distributed)\n",
    "\n",
    "                # Call Callbacks\n",
    "                callback_scores = do_callbacks(callbacks)\n",
    "\n",
    "                # Train Metrics\n",
    "                training_result = get_and_reset_metric_from_dict(training_loss_dict_metric)\n",
    "                training_history[global_step] = training_result\n",
    "                write_metrics(training_result, train_summary_writer, global_step)\n",
    "                # training_result[\"learning_rate\"] = learning_rate_holder.result().numpy()\n",
    "                # learning_rate_holder.reset_states()\n",
    "                tepoch.set_postfix(**training_result)\n",
    "\n",
    "                # Save model\n",
    "                save_model()\n",
    "\n",
    "        # Do after every epoch\n",
    "        epoch_end = True\n",
    "        save_model(epoch_end)\n",
    "        do_validation(validation_dataset_distributed)\n",
    "        callback_scores = do_callbacks(callbacks)\n",
    "        epoch_end = False\n",
    "\n",
    "    # Flatten the results\n",
    "    training_history = flat_metric_dict(training_history)\n",
    "    validation_history = flat_metric_dict(validation_history)\n",
    "    return training_history, validation_history, callback_scores\n",
    "\n",
    "\n",
    "class TrainerNew:\n",
    "    def __init__(\n",
    "        self,\n",
    "        distribution_strategy,\n",
    "        num_gpus=0,\n",
    "        all_reduce_alg=None,\n",
    "        num_packs=1,\n",
    "        tpu_address=None,\n",
    "        dtype='fp32',\n",
    "        loss_scale='dynamic',\n",
    "    ):\n",
    "\n",
    "        self.distribution_strategy = get_distribution_strategy(\n",
    "            distribution_strategy=distribution_strategy,\n",
    "            num_gpus=num_gpus,\n",
    "            all_reduce_alg=all_reduce_alg,\n",
    "            num_packs=num_packs,\n",
    "            tpu_address=tpu_address,\n",
    "        )\n",
    "\n",
    "        self.num_replicas = self.distribution_strategy.num_replicas_in_sync\n",
    "        self._dtype = get_tf_dtype(dtype)\n",
    "\n",
    "        # Setting dtype policy\n",
    "        set_mixed_precision_policy(self._dtype)\n",
    "        self.use_float16 = is_float16(self._dtype)\n",
    "        self.loss_scale = loss_scale\n",
    "\n",
    "        # # TODO\n",
    "        # if self.use_tpu:\n",
    "        # params[\"num_replicas\"] = self.distribution_strategy.num_replicas_in_sync\n",
    "        # else:\n",
    "        # logging.info(\"Running transformer with num_gpus = %d\", num_gpus)\n",
    "\n",
    "        # Add keras utils threads\n",
    "\n",
    "    @property\n",
    "    def use_tpu(self):\n",
    "        if self.distribution_strategy:\n",
    "            return isinstance(self.distribution_strategy, tf.distribute.TPUStrategy)\n",
    "        return False\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        model_fn,\n",
    "        optimizer_fn,\n",
    "        train_dataset,\n",
    "        train_loss_fn,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        model_checkpoint_dir,\n",
    "        validation_dataset=None,\n",
    "        validation_loss_fn=None,\n",
    "        validation_interval_steps=None,\n",
    "        steps_per_call=100,\n",
    "        enable_xla=True,\n",
    "        callbacks=None,\n",
    "        callbacks_interval_steps=None,\n",
    "        overwrite_checkpoint_dir=False,\n",
    "        max_number_of_models=10,\n",
    "        model_save_interval_steps=None,\n",
    "    ):\n",
    "\n",
    "        if steps_per_epoch:\n",
    "            logging.info(\"Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\")\n",
    "        if callbacks:\n",
    "            assert len(callbacks) == len(callbacks_interval_steps)\n",
    "\n",
    "        # Enable XLA\n",
    "        keras_utils.set_session_config(enable_xla=enable_xla)\n",
    "        logging.info(\"Policy: ----> {}\".format(keras_utils.get_policy_name()))\n",
    "        logging.info(\"Strategy: ---> {}\".format(self.distribution_strategy))\n",
    "        if self.use_tpu:\n",
    "            logging.info(\"Num TPU Devices: ---> {}\".format(self.distribution_strategy.num_replicas_in_sync))\n",
    "        else:\n",
    "            logging.info(\"Num GPU Devices: ---> {}\".format(self.distribution_strategy.num_replicas_in_sync))\n",
    "\n",
    "        # Under Strategy Scope\n",
    "        with self.distribution_strategy.scope():\n",
    "            # Model\n",
    "            model = model_fn()\n",
    "\n",
    "            # Optimizer\n",
    "            optimizer = optimizer_fn()\n",
    "            optimizer = configure_optimizer(optimizer, use_float16=self.use_float16, loss_scale=self.loss_scale)\n",
    "\n",
    "        # Checkpoint manager\n",
    "        checkpoint_manager = save_model_checkpoints(\n",
    "            model, overwrite_checkpoint_dir, model_checkpoint_dir, max_number_of_models\n",
    "        )\n",
    "\n",
    "        # Get metric dicts before distributing the dataset\n",
    "        # ddistributed datasets has no attribute .take\n",
    "        logging.info(\"Inferring metric shapes . . . . .\")\n",
    "        training_loss_dict_metric, validation_loss_dict_metric = get_loss_metric_dict(\n",
    "            model, train_dataset, train_loss_fn, validation_dataset, validation_loss_fn\n",
    "        )\n",
    "        # Distribute dataset\n",
    "        train_dataset_distributed = self.distribution_strategy.experimental_distribute_dataset(\n",
    "            train_dataset.repeat(epochs + 1)\n",
    "        )\n",
    "        validation_dataset_distributed = None\n",
    "        if validation_dataset:\n",
    "            validation_dataset_distributed = self.distribution_strategy.experimental_distribute_dataset(\n",
    "                validation_dataset\n",
    "            )\n",
    "\n",
    "        # Make train dataset iterator\n",
    "        train_dataset_distributed = iter(train_dataset_distributed)\n",
    "\n",
    "        history = {}\n",
    "        training_history, validation_history, callback_scores = train_and_eval(\n",
    "            model,\n",
    "            optimizer,\n",
    "            self.distribution_strategy,\n",
    "            epochs,\n",
    "            steps_per_epoch,\n",
    "            steps_per_call,\n",
    "            train_dataset_distributed,\n",
    "            train_loss_fn,\n",
    "            training_loss_dict_metric,\n",
    "            validation_dataset_distributed,\n",
    "            validation_loss_fn,\n",
    "            validation_loss_dict_metric,\n",
    "            validation_interval_steps,\n",
    "            self.use_float16,\n",
    "            callbacks,\n",
    "            callbacks_interval_steps,\n",
    "            locals(),\n",
    "            checkpoint_manager,\n",
    "            model_checkpoint_dir,\n",
    "            model_save_interval_steps,\n",
    "        )\n",
    "        history['training_history'] = training_history\n",
    "        history['validation_hsitory'] = validation_history\n",
    "        history['callbacks'] = callback_scores\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cbe99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8e40c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCallback():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, trainer_kwargs):\n",
    "        import pprint\n",
    "        pprint.pprint(trainer_kwargs)\n",
    "        \n",
    "simple_callback = SimpleCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e93abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerNew(\n",
    "            distribution_strategy=\"mirrored\", \n",
    "            num_gpus=2, \n",
    "            all_reduce_alg='nccl', \n",
    "            num_packs=1, \n",
    "            dtype=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b42548a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tempdir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51220034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp dir /tmp/tmp14vkffu3\n"
     ]
    }
   ],
   "source": [
    "tempdir = \"/tmp/tmp14vkffu3\"\n",
    "print(\"Temp dir\", tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b3bfd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:trainer:Policy: ----> mixed_float16\n",
      "INFO:trainer:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fe29d2ec490>\n",
      "INFO:trainer:Num GPU Devices: ---> 2\n",
      "You are using a model of type gpt2 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:trainer:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/gpt2\n",
      "INFO:trainer:Using Adamw optimizer\n",
      "INFO:trainer:Inferring metric shapes . . . . .\n",
      "Epoch 1/1 --- Step 10/21 --- :   0%|          | 0/2 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:trainer:Callbacks in progress at step 10 . . . .\n",
      "Epoch 1/1 --- Step 20/21 --- :  50%|█████     | 1/2 [00:55<00:55, 55.74s/batch , learning_rate=1.1e-6, loss=10.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callbacks': [<__main__.SimpleCallback object at 0x7fe29d2ec820>],\n",
      " 'callbacks_interval_steps': [10],\n",
      " 'checkpoint_manager': <tensorflow.python.training.checkpoint_management.CheckpointManager object at 0x7fe2428311f0>,\n",
      " 'enable_xla': False,\n",
      " 'epochs': 1,\n",
      " 'history': {},\n",
      " 'max_number_of_models': 10,\n",
      " 'model': <tf_transformers.core.legacy_model.LegacyModel object at 0x7fe234040e80>,\n",
      " 'model_checkpoint_dir': '/tmp/tmp14vkffu3',\n",
      " 'model_fn': <function get_model at 0x7fe2427e6dc0>,\n",
      " 'model_save_interval_steps': None,\n",
      " 'optimizer': <tensorflow.python.keras.mixed_precision.loss_scale_optimizer.LossScaleOptimizer object at 0x7fdf8c57e220>,\n",
      " 'optimizer_fn': <function get_optimizer at 0x7fe2427a00d0>,\n",
      " 'overwrite_checkpoint_dir': True,\n",
      " 'self': <__main__.TrainerNew object at 0x7fe242ab3550>,\n",
      " 'steps_per_call': 10,\n",
      " 'steps_per_epoch': 21,\n",
      " 'train_dataset': <PrefetchDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32})>,\n",
      " 'train_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7fe0701ad4f0>,\n",
      " 'train_loss_fn': <function lm_loss at 0x7fe1101c7e50>,\n",
      " 'training_loss_dict_metric': {'learning_rate': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546f7c40>,\n",
      "                               'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546a25e0>},\n",
      " 'validation_dataset': <TakeDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None), highlights: (None,)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32, highlights: tf.string})>,\n",
      " 'validation_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedDataset object at 0x7fe0701ad040>,\n",
      " 'validation_interval_steps': None,\n",
      " 'validation_loss_dict_metric': {'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fe08866d040>},\n",
      " 'validation_loss_fn': <function lm_loss at 0x7fe1101c7e50>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer:Callbacks in progress at step 20 . . . .\n",
      "Epoch 1/1 --- Step 20/21 --- : 100%|██████████| 2/2 [01:03<00:00, 31.67s/batch , learning_rate=3.1e-6, loss=10.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callbacks': [<__main__.SimpleCallback object at 0x7fe29d2ec820>],\n",
      " 'callbacks_interval_steps': [10],\n",
      " 'checkpoint_manager': <tensorflow.python.training.checkpoint_management.CheckpointManager object at 0x7fe2428311f0>,\n",
      " 'enable_xla': False,\n",
      " 'epochs': 1,\n",
      " 'history': {},\n",
      " 'max_number_of_models': 10,\n",
      " 'model': <tf_transformers.core.legacy_model.LegacyModel object at 0x7fe234040e80>,\n",
      " 'model_checkpoint_dir': '/tmp/tmp14vkffu3',\n",
      " 'model_fn': <function get_model at 0x7fe2427e6dc0>,\n",
      " 'model_save_interval_steps': None,\n",
      " 'optimizer': <tensorflow.python.keras.mixed_precision.loss_scale_optimizer.LossScaleOptimizer object at 0x7fdf8c57e220>,\n",
      " 'optimizer_fn': <function get_optimizer at 0x7fe2427a00d0>,\n",
      " 'overwrite_checkpoint_dir': True,\n",
      " 'self': <__main__.TrainerNew object at 0x7fe242ab3550>,\n",
      " 'steps_per_call': 10,\n",
      " 'steps_per_epoch': 21,\n",
      " 'train_dataset': <PrefetchDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32})>,\n",
      " 'train_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7fe0701ad4f0>,\n",
      " 'train_loss_fn': <function lm_loss at 0x7fe1101c7e50>,\n",
      " 'training_loss_dict_metric': {'learning_rate': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546f7c40>,\n",
      "                               'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546a25e0>},\n",
      " 'validation_dataset': <TakeDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None), highlights: (None,)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32, highlights: tf.string})>,\n",
      " 'validation_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedDataset object at 0x7fe0701ad040>,\n",
      " 'validation_interval_steps': None,\n",
      " 'validation_loss_dict_metric': {'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fe08866d040>},\n",
      " 'validation_loss_fn': <function lm_loss at 0x7fe1101c7e50>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trainer:Model saved at epoch 1\n",
      "INFO:trainer:Validation in progress at epoch end 1 . . . .\n",
      "10 Val batch  [00:06,  1.50 Val batch /s]\n",
      "INFO:trainer:Validation result at epoch 1 is {'loss': 10.51753}\n",
      "INFO:trainer:Callbacks in progress at epoch end 1 . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'callbacks': [<__main__.SimpleCallback object at 0x7fe29d2ec820>],\n",
      " 'callbacks_interval_steps': [10],\n",
      " 'checkpoint_manager': <tensorflow.python.training.checkpoint_management.CheckpointManager object at 0x7fe2428311f0>,\n",
      " 'enable_xla': False,\n",
      " 'epochs': 1,\n",
      " 'history': {},\n",
      " 'max_number_of_models': 10,\n",
      " 'model': <tf_transformers.core.legacy_model.LegacyModel object at 0x7fe234040e80>,\n",
      " 'model_checkpoint_dir': '/tmp/tmp14vkffu3',\n",
      " 'model_fn': <function get_model at 0x7fe2427e6dc0>,\n",
      " 'model_save_interval_steps': None,\n",
      " 'optimizer': <tensorflow.python.keras.mixed_precision.loss_scale_optimizer.LossScaleOptimizer object at 0x7fdf8c57e220>,\n",
      " 'optimizer_fn': <function get_optimizer at 0x7fe2427a00d0>,\n",
      " 'overwrite_checkpoint_dir': True,\n",
      " 'self': <__main__.TrainerNew object at 0x7fe242ab3550>,\n",
      " 'steps_per_call': 10,\n",
      " 'steps_per_epoch': 21,\n",
      " 'train_dataset': <PrefetchDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32})>,\n",
      " 'train_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedIterator object at 0x7fe0701ad4f0>,\n",
      " 'train_loss_fn': <function lm_loss at 0x7fe1101c7e50>,\n",
      " 'training_loss_dict_metric': {'learning_rate': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546f7c40>,\n",
      "                               'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fdf546a25e0>},\n",
      " 'validation_dataset': <TakeDataset shapes: ({input_ids: (None, None)}, {labels: (None, None), labels_mask: (None, None), highlights: (None,)}), types: ({input_ids: tf.int32}, {labels: tf.int32, labels_mask: tf.int32, highlights: tf.string})>,\n",
      " 'validation_dataset_distributed': <tensorflow.python.distribute.input_lib.DistributedDataset object at 0x7fe0701ad040>,\n",
      " 'validation_interval_steps': None,\n",
      " 'validation_loss_dict_metric': {'loss': <tensorflow.python.keras.metrics.Mean object at 0x7fe08866d040>},\n",
      " 'validation_loss_fn': <function lm_loss at 0x7fe1101c7e50>}\n"
     ]
    }
   ],
   "source": [
    "history = trainer.run(\n",
    "        model_fn = get_model,\n",
    "        optimizer_fn = get_optimizer,\n",
    "        train_dataset = tfdataset,\n",
    "        train_loss_fn = lm_loss,\n",
    "        epochs = 1,\n",
    "        steps_per_epoch = 21,\n",
    "        model_checkpoint_dir= tempdir,\n",
    "        validation_dataset =  tfdataset_validation.take(10),  # tfdataset_validation.take(10)\n",
    "        validation_loss_fn=lm_loss,\n",
    "        validation_interval_steps=None,\n",
    "        steps_per_call=10,\n",
    "        enable_xla=False,\n",
    "        callbacks=[simple_callback],\n",
    "        callbacks_interval_steps=[10],\n",
    "        overwrite_checkpoint_dir=True,\n",
    "        max_number_of_models=10,\n",
    "        model_save_interval_steps=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9e3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcfdd076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  ckpt-1.data-00000-of-00001\tckpt-1.index  logs\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/tmp14vkffu3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f500b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1624965412.tfnewgpu-0.4005.12076.v2\r\n",
      "events.out.tfevents.1624965766.tfnewgpu-0.4005.30017.v2\r\n",
      "events.out.tfevents.1624965974.tfnewgpu-0.4469.12065.v2\r\n",
      "events.out.tfevents.1624966428.tfnewgpu-0.4911.12065.v2\r\n",
      "events.out.tfevents.1624966509.tfnewgpu-0.4911.30006.v2\r\n",
      "events.out.tfevents.1624974375.tfnewgpu-0.4911.125679.v2\r\n",
      "events.out.tfevents.1624977036.tfnewgpu-0.5707.12065.v2\r\n",
      "events.out.tfevents.1624977618.tfnewgpu-0.5707.48989.v2\r\n",
      "events.out.tfevents.1624978018.tfnewgpu-0.5707.125427.v2\r\n",
      "events.out.tfevents.1624981946.tfnewgpu-0.5707.367256.v2\r\n",
      "events.out.tfevents.1624982247.tfnewgpu-0.5707.437411.v2\r\n",
      "events.out.tfevents.1625025212.tfnewgpu-0.6603.52046.v2\r\n",
      "events.out.tfevents.1625028255.tfnewgpu-0.6603.121581.v2\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/tmp14vkffu3/logs/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38732829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
