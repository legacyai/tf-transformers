{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68c8c1a",
   "metadata": {},
   "source": [
    "# CLIP Benchmark\n",
    "\n",
    "* This notebook measures the performance of CLIP Image Encoder on Imagenetv2 dataset using ```Pytorch``` and ```Tensorflow```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8067c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ebddc85",
   "metadata": {},
   "source": [
    "### PyTorch CLIP Model\n",
    "\n",
    "* With ```batch_size=64```, model takes ```54 seconds``` to process ```~10k``` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7221ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff44db2e70b840dcb70cb292d515e905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load Model and preprocess for images\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "! pip install git+https://github.com/modestyachts/ImageNetV2_pytorch\n",
    "\n",
    "from imagenetv2_pytorch import ImageNetV2Dataset\n",
    "images = ImageNetV2Dataset(transform=preprocess)\n",
    "loader = torch.utils.data.DataLoader(images, batch_size=64, num_workers=2)\n",
    "\n",
    "all_data = []\n",
    "with torch.no_grad():\n",
    "    for i, (images, target) in enumerate(tqdm(loader)):\n",
    "        images = images.cuda()\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d065ab93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "297af325",
   "metadata": {},
   "source": [
    "### Load CLIP Model tf-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f57100fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful ✅✅: Model checkpoints matched and loaded from /home/jovyan/.cache/huggingface/hub/tftransformers__clip-vit-base-patch32.main.c812ceb87d98962d632933234a5edd4b69edf574/ckpt-1\n",
      "INFO:absl:Successful ✅: Loaded model from tftransformers/clip-vit-base-patch32\n"
     ]
    }
   ],
   "source": [
    "from tf_transformers.models.clip import CLIPModel, CLIPFeatureExtractorTF\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "# Model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", return_layer=True)\n",
    "# Get text and image encoder out\n",
    "text_encoder = model.text_encoder\n",
    "image_encoder = model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856da6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8352fde8",
   "metadata": {},
   "source": [
    "### TF with clip preprocess (pt to tf data)\n",
    "\n",
    "* With ```batch_size=64```, model takes ```54 seconds``` to process ```~10k``` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00ae02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:54,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "images = ImageNetV2Dataset(transform=preprocess)\n",
    "loader = torch.utils.data.DataLoader(images, batch_size=64, num_workers=2)\n",
    "\n",
    "for i, (images, target) in tqdm.tqdm(enumerate(loader)):\n",
    "    images = {'input_pixels': tf.transpose(tf.convert_to_tensor(images.numpy()), [0, 2, 3, 1] )}\n",
    "    outputs = image_encoder(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472def3",
   "metadata": {},
   "source": [
    "### TF with tf.io preprocess (Preprocess on the fly)\n",
    "\n",
    "* With ```batch_size=64```, model takes ```17 seconds``` to process ```~10k``` images. This is ```3x``` times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88108e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:17,  9.13it/s]\n"
     ]
    }
   ],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "rescaler = tf.keras.layers.Rescaling(scale=1.0/255.0)\n",
    "mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "variance = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "def standardize(image_data):\n",
    "    image_data -= tf.constant([mean])\n",
    "    image_data /= tf.constant([variance])\n",
    "    return image_data\n",
    "\n",
    "def read_process_resize(image_path: str):\n",
    "    \"\"\"Read, decode and process\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    result = {}\n",
    "    result['image_path'] = image_path\n",
    "    result['input_pixels'] = standardize(rescaler(img))\n",
    "    result['label'] = tf.strings.split(image_path, '/')[2] # string\n",
    "    return result\n",
    "\n",
    "image_files = tf.constant(tf.io.gfile.glob(\"Imagenet/imagenetv2-matched-frequency-format-val/*/*.jpeg\"))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(image_files)\n",
    "image_dataset = image_dataset.map(read_process_resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "batch_size = 64\n",
    "image_dataset = image_dataset.batch(batch_size, drop_remainder=False)\n",
    "\n",
    "for (index, item) in tqdm.tqdm(enumerate(image_dataset)):\n",
    "    image_features = image_encoder(item)['cls_output']\n",
    "    image_features = tf.nn.l2_normalize(image_features, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3fa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
