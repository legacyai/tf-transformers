{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/a642163/Projects/tf-transformers/src/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#\n",
      "# macOS Notice\n",
      "#\n",
      "# This file is not consulted for DNS hostname resolution, address\n",
      "# resolution, or the DNS query routing mechanism used by most\n",
      "# processes on this system.\n",
      "#\n",
      "# To view the DNS configuration used by this system, use:\n",
      "#   scutil --dns\n",
      "#\n",
      "# SEE ALSO\n",
      "#   dns-sd(1), scutil(8)\n",
      "#\n",
      "# This file is automatically generated.\n",
      "#\n",
      "search fmr.com fmrco.com im.fmrco.com fi.fmrco.com\n",
      "nameserver 10.165.9.56\n",
      "nameserver 10.168.208.20\n",
      "cat: /Users/a642163/.wgetrcb: No such file or directory\n",
      "10.239.228.20:8000\n",
      "10.239.228.20:8000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This notebook is intented to test, some of the\n",
    "# results validation of GPT2 model \n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_transformers.models import  GPT2Model \n",
    "from transformers import GPT2Tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Globals\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d88951420641ddb2a3e44160f52b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0693fe893da4c2a9e02c7b5512d6ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca92f533d6441608d2d2b10bc6e0049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e90aedd3cfb432b9b5b69cfa5196eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Check TF Conversion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Check TF conversion\n",
    "\n",
    "# !rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model = GPT2Model.from_pretrained(model_name=model_name, convert_fn_type='tf')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "WARNING:absl:Expected `transformers` version `4.6.0`, but found version `4.10.2`.        The conversion might or might not work.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5417b9e0d2940d08159589690c3915d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Successful ✅: Converted model using TF HF\n",
      "INFO:absl:Successful: Saved model at /var/folders/9v/54jzg8fs4v91m_50ydft_1yx7cmk7x/T/tf_transformers_cache/gpt2/ckpt-1\n",
      "INFO:absl:Successful ✅: Asserted and Converted `gpt2` from HF and saved it in cache folder /var/folders/9v/54jzg8fs4v91m_50ydft_1yx7cmk7x/T/tf_transformers_cache/gpt2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Check PT Conversion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Check PT conversion\n",
    "\n",
    "# !rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model = GPT2Model.from_pretrained(model_name=model_name, convert_fn_type='pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:absl:Successful: Model checkpoints matched and loaded from /var/folders/9v/54jzg8fs4v91m_50ydft_1yx7cmk7x/T/tf_transformers_cache/gpt2/ckpt-1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Load model auto regressive\n",
    "model_ar, config = GPT2Model.from_pretrained(model_name=model_name,\n",
    "                                       use_auto_regressive = True,\n",
    "                                       return_config=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.gpt2.GPT2Encoder object at 0x7f21cf27f6d0> and <keras.engine.input_layer.InputLayer object at 0x7f21cc4700a0>).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.gpt2.gpt2.GPT2Encoder object at 0x7f21cf27f6d0> and <keras.engine.input_layer.InputLayer object at 0x7f21cc4700a0>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/gpt2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Test GPT2 with and without caching (Greedy)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# T5 text generation without caching\n",
    "text = \"I would love to walk with my cat because\"\n",
    "\n",
    "# Create inputs\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_hf['input_ids']\n",
    "\n",
    "# Iterate\n",
    "predictions_non_auto_regressive = []\n",
    "predictions_prob_non_auto_regressive = []\n",
    "\n",
    "for i in range(20):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"input_ids\"] = tf.concat([inputs[\"input_ids\"], predicted_ids], axis=1)\n",
    "    predictions_non_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_non_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_non_auto_regressive = tf.concat(predictions_non_auto_regressive, axis=1)\n",
    "predictions_prob_non_auto_regressive = tf.concat(predictions_prob_non_auto_regressive, axis=1)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "# Text generation with cache\n",
    "encoder_hidden_dim = config['embedding_size']\n",
    "num_hidden_layers  = config['num_hidden_layers']\n",
    "num_attention_heads = config['num_attention_heads']\n",
    "attention_head_size = config['attention_head_size']\n",
    "\n",
    "# Inputs\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "input_ids = inputs_hf['input_ids']\n",
    "\n",
    "batch_size = tf.shape(input_ids)[0]\n",
    "seq_length = tf.shape(input_ids)[1]\n",
    "\n",
    "all_cache_key = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "all_cahce_value = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "\n",
    "\n",
    "inputs = {}\n",
    "inputs['input_ids'] = input_ids\n",
    "inputs['all_cache_key'] = all_cache_key\n",
    "inputs['all_cache_value'] = all_cahce_value\n",
    "inputs[\"past_length\"] = tf.zeros(shape=(1, batch_size), dtype=tf.int32)\n",
    "\n",
    "# Iterate\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "past_lengths = []\n",
    "for i in range(20):\n",
    "    outputs = model_ar(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    \n",
    "    # This is useful for variable batch decoding\n",
    "    if i == 0:\n",
    "        masks = tf.cast(tf.not_equal(input_ids, -1), tf.float32)\n",
    "        masks = tf.reshape(\n",
    "            masks,\n",
    "            (1, batch_size, 1, seq_length, 1),\n",
    "        )\n",
    "        outputs[\"all_cache_key\"]   = outputs[\"all_cache_key\"]   * masks\n",
    "        outputs[\"all_cache_value\"] = outputs[\"all_cache_value\"] * masks\n",
    "        \n",
    "    inputs[\"input_ids\"] = predicted_ids\n",
    "    inputs[\"all_cache_key\"] = outputs[\"all_cache_key\"]\n",
    "    inputs[\"all_cache_value\"] = outputs[\"all_cache_value\"]\n",
    "    inputs[\"past_length\"] = outputs[\"past_length\"]\n",
    "    past_lengths.append(inputs[\"past_length\"])\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n",
    "\n",
    "#----------------------------------------------------------------------------------------#\n",
    "expected_outputs = [[ 314, 1842,  607,  523,  881,   13,  314, 1842,  607,  523,  881,\n",
    "          13,  314, 1842,  607,  523,  881,   13,  314, 1842]]\n",
    "tf.assert_equal(predictions_non_auto_regressive, predictions_auto_regressive)\n",
    "assert(np.allclose(predictions_prob_non_auto_regressive.numpy(), \n",
    "            predictions_prob_auto_regressive.numpy(),expected_outputs) == True)\n",
    "print(\"Success\")\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Test GPT2 with TextDecoder Saved Model (Greedy)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Text generation using saved_model with TextDecoder\n",
    "\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# from tf_transformers.text import TextDecoder\n",
    "# text = [\"I would love to walk with my cat because\", \n",
    "#         \"I love stars because\"]\n",
    "\n",
    "# # Save as saved model\n",
    "# saved_model_dir = tempfile.mkdtemp()\n",
    "# model_ar.save_as_serialize_module(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load saved model\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoder(\n",
    "    model = loaded\n",
    ")\n",
    "\n",
    "# Inputs\n",
    "# Batching using -1 is must\n",
    "input_ids = tf.ragged.constant(tokenizer(text)['input_ids']).to_tensor(-1)\n",
    "inputs = {}\n",
    "inputs['input_ids'] = input_ids\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=13, \n",
    "               eos_id=1)\n",
    "\n",
    "expected_outputs = [[[314, 1842, 607, 523, 881, 13, 314, 1842, 607, 523, 881, 13, 314]],\n",
    " [[484, 821, 523, 881, 517, 621, 655, 257, 3491, 13, 1119, 821, 257]]]\n",
    "assert(decoder_results['predicted_ids'].numpy().tolist()[0] == expected_outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Test GPT2 with TextDecoder Saved Model (Beam)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Beam check\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam',\n",
    "               num_beams=3,\n",
    "               max_iterations=13, \n",
    "               eos_id=-100)\n",
    "top_prediction = decoder_results['predicted_ids'].numpy().tolist()[0][0]\n",
    "assert([top_prediction] == expected_outputs[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Test GPT2 with TextDecoder Saved Model (Top K top P)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p',\n",
    "               num_return_sequences=1,\n",
    "                                 top_k=100,\n",
    "                                 top_p=0.6,\n",
    "               max_iterations=13, \n",
    "               eos_id=-100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Test GPT2 with TextDecoderSerializable (Greedy)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    max_iterations=20,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=-100,\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"I would love to walk with my cat because\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_hf['input_ids']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "expected_outputs = [[ 314, 1842,  607,  523,  881,   13,  314, 1842,  607,  523,  881,\n",
    "          13,  314, 1842,  607,  523,  881,   13,  314, 1842]]\n",
    "assert(decoder_results_serialized['predicted_ids'].numpy().tolist()[0] == expected_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as word_embeddings_layer_call_and_return_conditional_losses, word_embeddings_layer_call_fn, positional_embeddings_layer_call_and_return_conditional_losses, positional_embeddings_layer_call_fn, dropout_2_layer_call_and_return_conditional_losses while saving (showing 5 of 740). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0cvx8lmc/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0cvx8lmc/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Test GPT2 with TextDecoderSerializable (Beam)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    max_iterations=20,\n",
    "    num_beams=3,\n",
    "    mode=\"beam\",\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"I would love to walk with my cat because\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_hf['input_ids']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "top_prediction = decoder_results_serialized['predicted_ids'].numpy().tolist()[0][0]\n",
    "assert([top_prediction] == expected_outputs)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. Test GPT2 with TextDecoderSerializable (Top K top P)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "# loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    max_iterations=20,\n",
    "    top_k=100,\n",
    "    top_p=0.7,\n",
    "    mode=\"top_k_top_p\",\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"I would love to walk with my cat because\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['input_ids'] = inputs_hf['input_ids']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10. Test GPT2 tf lite\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model.from_pretrained(model_name=MODEL_NAME, batch_size=1, sequence_length=32,)\n",
    "\n",
    "tempdir = tempfile.mkdtemp()\n",
    "model.save_serialized(tempdir, overwrite=True)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}\".format(tempdir))  # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open(\"{}/converted_model.tflite\".format(tempdir), \"wb\").write(tflite_model)\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(tempdir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Get result\n",
    "# encoder input_ids\n",
    "interpreter.set_tensor(\n",
    "    input_details[0]['index'],\n",
    "    tf.random.uniform(input_details[0]['shape'], minval=0, maxval=100, dtype=tf.int32),\n",
    ")\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[-1]['index'])\n",
    "print(\"Tflite output shape\", tflite_output.shape)\n",
    "# tf.debugging.assert_equal(tflite_output.shape, (1, 32, 32128))\n",
    "logging.info(\"Test: TFlite Conversion. ✅\")\n",
    "shutil.rmtree(tempdir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}