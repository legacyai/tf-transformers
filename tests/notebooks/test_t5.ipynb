{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d7e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is intented to test, some of the\n",
    "# results validation of T5 model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5eac58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_transformers.models import  T5Model\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84386e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992e0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "model_name = 't5-small'\n",
    "DECODER_START_ID = 0\n",
    "DECODER_EOS_ID = 1\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660977a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4271b682",
   "metadata": {},
   "source": [
    "### 1. Check TF Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49166bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "WARNING:absl:Expected `transformers` version `4.6.0`, but found version `4.9.2`.        The conversion might or might not work.\n",
      "INFO:absl:Successful ✅: Converted model using TF HF\n",
      "INFO:absl:Successful: Saved model at /tmp/tf_transformers_cache/t5-small/ckpt-1\n",
      "INFO:absl:Successful ✅: Asserted and Converted `t5-small` from HF and saved it in cache folder /tmp/tf_transformers_cache/t5-small\n"
     ]
    }
   ],
   "source": [
    "# Check TF conversion\n",
    "\n",
    "# !rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model = T5Model.from_pretrained(model_name=model_name, convert_fn_type='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2db07d",
   "metadata": {},
   "source": [
    "### 2.Check PT Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024ceeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/t5-small\n"
     ]
    }
   ],
   "source": [
    "# Check PT conversion\n",
    "\n",
    "# !rm -rf /tmp/tf_transformers_cache/t5-base\n",
    "\n",
    "model = T5Model.from_pretrained(model_name=model_name, convert_fn_type='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102f679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fd6af666430> and <keras.engine.input_layer.InputLayer object at 0x7fd6af4bf3a0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tf_transformers.models.encoder_decoder.encoder_decoder.EncoderDecoder object at 0x7fd6af666430> and <keras.engine.input_layer.InputLayer object at 0x7fd6af4bf3a0>).\n",
      "INFO:absl:Successful: Model checkpoints matched and loaded from /tmp/tf_transformers_cache/t5-small\n"
     ]
    }
   ],
   "source": [
    "# Load model auto regressive\n",
    "model_ar, config = T5Model.from_pretrained(model_name=model_name,\n",
    "                                       decoder_kwargs={'use_auto_regressive': True},\n",
    "                                       return_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cf61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce1d121",
   "metadata": {},
   "source": [
    "### 3. Test T5 with and without caching (Greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c2a1c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# T5 text generation without caching\n",
    "text = \"translate English to German: The house is wonderful and we wish to be here :)\"\n",
    "\n",
    "# Create inputs\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "inputs['decoder_input_ids']  = tf.constant([[DECODER_START_ID]])\n",
    "\n",
    "# Iterate\n",
    "predictions_non_auto_regressive = []\n",
    "predictions_prob_non_auto_regressive = []\n",
    "\n",
    "for i in range(13):\n",
    "    outputs = model(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"decoder_input_ids\"] = tf.concat([inputs[\"decoder_input_ids\"], predicted_ids], axis=1)\n",
    "    predictions_non_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_non_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_non_auto_regressive = tf.concat(predictions_non_auto_regressive, axis=1)\n",
    "predictions_prob_non_auto_regressive = tf.concat(predictions_prob_non_auto_regressive, axis=1)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------#\n",
    "# Text generation with cache\n",
    "encoder_hidden_dim = config['embedding_size']\n",
    "num_hidden_layers  = config['num_hidden_layers']\n",
    "num_attention_heads = config['num_attention_heads']\n",
    "attention_head_size = config['attention_head_size']\n",
    "\n",
    "# Inputs\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "encoder_input_ids = inputs_hf['input_ids']\n",
    "encoder_input_mask = inputs_hf['attention_mask']\n",
    "\n",
    "batch_size = tf.shape(encoder_input_ids)[0]\n",
    "seq_length = tf.shape(encoder_input_ids)[1]\n",
    "\n",
    "decoder_input_ids  = tf.reshape([0] * batch_size.numpy(), (batch_size,1))\n",
    "\n",
    "encoder_hidden_states = tf.zeros((batch_size, seq_length, encoder_hidden_dim))\n",
    "decoder_all_cache_key = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "decoder_all_cahce_value = tf.zeros((num_hidden_layers, \n",
    "                                  batch_size, \n",
    "                                  num_attention_heads, \n",
    "                                  seq_length, \n",
    "                                  attention_head_size))\n",
    "\n",
    "\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = encoder_input_ids\n",
    "inputs['encoder_input_mask']= encoder_input_mask\n",
    "inputs['decoder_input_ids'] = decoder_input_ids\n",
    "inputs['encoder_hidden_states'] = encoder_hidden_states\n",
    "inputs['decoder_all_cache_key'] = decoder_all_cache_key\n",
    "inputs['decoder_all_cache_value'] = decoder_all_cahce_value\n",
    "\n",
    "# Iterate\n",
    "predictions_auto_regressive = []\n",
    "predictions_prob_auto_regressive = []\n",
    "\n",
    "for i in range(13):\n",
    "    outputs = model_ar(inputs)\n",
    "    predicted_ids = tf.cast(tf.expand_dims(tf.argmax(outputs[\"last_token_logits\"], axis=1), 1), tf.int32)\n",
    "    inputs[\"decoder_input_ids\"] = predicted_ids\n",
    "    inputs[\"decoder_all_cache_key\"] = outputs[\"decoder_all_cache_key\"]\n",
    "    inputs[\"decoder_all_cache_value\"] = outputs[\"decoder_all_cache_value\"]\n",
    "    inputs[\"encoder_hidden_states\"] = outputs[\"encoder_hidden_states\"]\n",
    "    predictions_auto_regressive.append(predicted_ids)\n",
    "    predictions_prob_auto_regressive.append(\n",
    "        tf.expand_dims(tf.reduce_max(outputs[\"last_token_logits\"], axis=1), 1)\n",
    "    )\n",
    "predictions_auto_regressive = tf.concat(predictions_auto_regressive, axis=1)\n",
    "predictions_prob_auto_regressive = tf.concat(predictions_prob_auto_regressive, axis=1)\n",
    "\n",
    "#----------------------------------------------------------------------------------------#\n",
    "expected_outputs = [[  644,  4598,   229, 19250,    64,   558,  7805,  1382,  1110,\n",
    "            3,    10,    61,     1]]\n",
    "tf.assert_equal(predictions_non_auto_regressive, predictions_auto_regressive)\n",
    "assert(np.allclose(predictions_prob_non_auto_regressive.numpy(), \n",
    "            predictions_prob_auto_regressive.numpy(),expected_outputs) == True)\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b3802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3254207d",
   "metadata": {},
   "source": [
    "### 4. Test T5 with TextDecoder Saved Model (Greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85d4e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, word_embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1140). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    }
   ],
   "source": [
    "# Text generation using saved_model with TextDecoder\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoder\n",
    "text = \"translate English to German: The house is wonderful and we wish to be here :)\"\n",
    "\n",
    "# Save as saved model\n",
    "saved_model_dir = tempfile.mkdtemp()\n",
    "model_ar.save_as_serialize_module(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load saved model\n",
    "loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoder(\n",
    "    model = loaded, \n",
    "    decoder_start_token_id = DECODER_START_ID # for t5\n",
    ")\n",
    "\n",
    "# Inputs\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='greedy', \n",
    "               max_iterations=13, \n",
    "               eos_id=1)\n",
    "\n",
    "expected_outputs = [[  644,  4598,   229, 19250,    64,   558,  7805,  1382,  1110,\n",
    "            3,    10,    61,     1]]\n",
    "assert(decoder_results['predicted_ids'].numpy().tolist()[0] == expected_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b791f04",
   "metadata": {},
   "source": [
    "### 5. Test T5 with TextDecoder Saved Model (Beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam check\n",
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='beam',\n",
    "               num_beams=3,\n",
    "               max_iterations=13, \n",
    "               eos_id=1)\n",
    "top_prediction = decoder_results['predicted_ids'].numpy().tolist()[0][0]\n",
    "assert([top_prediction] == expected_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbc232",
   "metadata": {},
   "source": [
    "### 6. Test T5 with TextDecoder Saved Model (Top K top P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1b75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_results = decoder.decode(inputs, \n",
    "               mode='top_k_top_p',\n",
    "               num_return_sequences=1,\n",
    "                                 top_k=100,\n",
    "                                 top_p=0.6,\n",
    "               max_iterations=13, \n",
    "               eos_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a9824",
   "metadata": {},
   "source": [
    "### 7. Test T5 with TextDecoderSerializable (Greedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c66986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, word_embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1140). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    }
   ],
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "# loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    decoder_start_token_id = DECODER_START_ID,\n",
    "    max_iterations=15,\n",
    "    mode=\"greedy\",\n",
    "    do_sample=False,\n",
    "    eos_id=DECODER_EOS_ID\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"translate English to German: The house is wonderful and we wish to be here :)\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "expected_outputs = [[  644,  4598,   229, 19250,    64,   558,  7805,  1382,  1110,\n",
    "            3,    10,    61,     1]]\n",
    "assert(decoder_results_serialized['predicted_ids'].numpy().tolist()[0] == expected_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46bb5f",
   "metadata": {},
   "source": [
    "### 8. Test T5 with TextDecoderSerializable (Beam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5956ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using saved_model with TextDecoderSerializable\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "# loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    decoder_start_token_id = DECODER_START_ID,\n",
    "    max_iterations=15,\n",
    "    num_beams=3,\n",
    "    mode=\"beam\",\n",
    "    do_sample=False,\n",
    "    eos_id=DECODER_EOS_ID\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"translate English to German: The house is wonderful and we wish to be here :)\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)\n",
    "top_prediction = decoder_results_serialized['predicted_ids'].numpy().tolist()[0][0]\n",
    "assert([top_prediction] == expected_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1164b8",
   "metadata": {},
   "source": [
    "### 9. Test T5 with TextDecoderSerializable (Top K top P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71ca1552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as tf_transformers/t5_encoder_layer_call_and_return_conditional_losses, tf_transformers/t5_encoder_layer_call_fn, tf_transformers/t5_decoder_layer_call_and_return_conditional_losses, tf_transformers/t5_decoder_layer_call_fn, word_embeddings_layer_call_and_return_conditional_losses while saving (showing 5 of 1140). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxvbn3gk2/assets\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "from tf_transformers.text import TextDecoderSerializable\n",
    "\n",
    "# loaded   = tf.saved_model.load(saved_model_dir)\n",
    "decoder  = TextDecoderSerializable(\n",
    "    model = model_ar,\n",
    "    decoder_start_token_id = DECODER_START_ID,\n",
    "    max_iterations=15,\n",
    "    top_k=100,\n",
    "    top_p=0.7,\n",
    "    mode=\"top_k_top_p\",\n",
    "    do_sample=False,\n",
    "    eos_id=DECODER_EOS_ID\n",
    ")\n",
    "\n",
    "# Save\n",
    "decoder_model = decoder.get_model()\n",
    "decoder_model.save_serialized(saved_model_dir, overwrite=True)\n",
    "\n",
    "# Load\n",
    "loaded_decoder   = tf.saved_model.load(saved_model_dir)\n",
    "model_pb_decoder = loaded_decoder.signatures['serving_default']\n",
    "\n",
    "text = \"translate English to German: The house is wonderful and we wish to be here :)\"\n",
    "inputs_hf = tokenizer(text, return_tensors='tf')\n",
    "inputs = {}\n",
    "inputs['encoder_input_ids'] = inputs_hf['input_ids']\n",
    "inputs['encoder_input_mask'] = inputs_hf['attention_mask']\n",
    "\n",
    "\n",
    "decoder_results_serialized = model_pb_decoder(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ea606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aad6b4a",
   "metadata": {},
   "source": [
    "### 10. Test T5 lite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    convert_fn_type='tf',\n",
    "    encoder_kwargs={'batch_size': 1, 'sequence_length': 32},\n",
    "    decoder_kwargs={'batch_size': 1, 'sequence_length': 32},\n",
    ")\n",
    "\n",
    "tempdir = tempfile.mkdtemp()\n",
    "model.save_serialized(tempdir, overwrite=True)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"{}\".format(tempdir))  # path to the SavedModel directory\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open(\"{}/converted_model.tflite\".format(tempdir), \"wb\").write(tflite_model)\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"{}/converted_model.tflite\".format(tempdir))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Get result\n",
    "# encoder input_ids\n",
    "interpreter.set_tensor(\n",
    "    input_details[0]['index'],\n",
    "    tf.random.uniform(input_details[0]['shape'], minval=0, maxval=100, dtype=tf.int32),\n",
    ")\n",
    "# input_mask\n",
    "interpreter.set_tensor(input_details[1]['index'], tf.ones(input_details[1]['shape'], dtype=tf.int32))\n",
    "\n",
    "# decoder input ids\n",
    "interpreter.set_tensor(\n",
    "    input_details[2]['index'],\n",
    "    tf.random.uniform(input_details[2]['shape'], minval=0, maxval=100, dtype=tf.int32),\n",
    ")\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[-1]['index'])\n",
    "\n",
    "tf.debugging.assert_equal(tflite_output.shape, (1, 32, 32128))\n",
    "logging.info(\"Test: TFlite Conversion. ✅\")\n",
    "shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a6d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
