<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
<meta property="og:title" content="Philosophy" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="introduction_docs/philosophy.html" />
  
<meta property="og:description" content="tf-transformers is designed primarily for industrial research. Rather than showing dummy tutorials to enhance the concept, all the tutorials and design philo..." />
  
<meta property="og:image" content="png location" />
  
<meta property="og:image:alt" content="Philosophy" />
  
<meta name="twitter:image" content="png location">
<meta name="twitter:description" content="State-of-the-art Faster Transformer (NLP,CV,Audio) Based models in Tensorflow 2.0">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Philosophy &mdash; TF Transformers   documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom_from_huggingface.css" type="text/css" /><link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ALBERT" href="../model_doc/albert.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html"><img src="../_static/transformers_mix.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                 
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Philosophy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#main-concepts">Main concepts</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/parallelism.html">Model Parallelism</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/1_read_write_tfrecords.html">Writing and Reading TFRecords</a></li>
</ul>
<p class="caption"><span class="caption-text">Tokenizers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/albert_tokenizer.html">ALBERT Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/bigbird_tokenizer.html">BigBird Roberta Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_doc/t5_tokenizer.html">T5 Tokenizer</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../research/glue.html">Glue Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/long_block_sequencer.html">Long Block Sequencer</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/gpt2.html">Benchmark GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/t5.html">Benchmark T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/albert.html">Benchmark Albert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/vit.html">Benchmark ViT</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TF Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Philosophy</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/introduction_docs/philosophy.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="philosophy">
<h1>Philosophy<a class="headerlink" href="#philosophy" title="Permalink to this headline">¶</a></h1>
<p>tf-transformers is designed primarily for industrial research. Rather than showing dummy tutorials to enhance the concept,
all the tutorials and design philosophy focus on making life of researchers east with standard tools. Our aim is to
help Deep Learning practitioners to pretrain , fine-tune and infer a models with maximum ease and knowledge.</p>
<ul class="simple">
<li><p>NLP researchers and educators seeking to use/study/extend large-scale transformers models</p></li>
<li><p>hands-on practitioners who want to fine-tune those models and/or serve them in production</p></li>
<li><p>engineers who just want to download a pretrained model and use it to solve a given NLP task.</p></li>
</ul>
<p>The library was designed with two strong goals in mind:</p>
<ul>
<li><p>Be as easy and fast to use as possible:</p>
<blockquote>
<div><ul class="simple">
<li><p>We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,
just three standard classes required to use each model: <span class="xref std std-doc">configuration</span>,
<span class="xref std std-doc">models</span> .</p></li>
<li><p>All of these classes can be initialized in a simple and unified way from pretrained instances by using a common
<code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained()</span></code> instantiation method which will take care of downloading (if needed), caching and
loading the related class instance and associated data (configurations’ hyper-parameters, tokenizers’ vocabulary,
and models’ weights) from a pretrained checkpoint provided on <a class="reference external" href="https://huggingface.co/models">Hugging Face Hub</a> or your own saved checkpoint.</p></li>
<li><p>On top of those three base classes, <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> to quickly train or fine-tune a given model.</p></li>
<li><p>As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to
extend/build-upon the library, just use regular Python/TensorFlow/Keras modules and inherit from the base
classes of the library to reuse functionalities like model loading/saving.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Provide state-of-the-art models with performances as close as possible to the original models:</p>
<blockquote>
<div><ul class="simple">
<li><p>We provide at least one example for each architecture which reproduces a result provided by the official authors
of said architecture.</p></li>
<li><p>The code is written in pure Tensorflow 2.0 and most models supprt TFlite models also.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="section" id="main-concepts">
<h2>Main concepts<a class="headerlink" href="#main-concepts" title="Permalink to this headline">¶</a></h2>
<p>The library is built around three types of classes for each model:</p>
<a class="reference internal image-reference" href="../_images/philosophy.png"><img alt="Layout of the Model." src="../_images/philosophy.png" style="width: 400px;" /></a>
<ul class="simple">
<li><p><strong>Model classes</strong> such as <code class="xref py py-class docutils literal notranslate"><span class="pre">BertEncoder</span></code>. The hierarchical flow remains the same for all or most models.
The core architectural implementations is happening inside <a class="reference internal" href="../model_doc/bert.html#tf_transformers.models.BertEncoder" title="tf_transformers.models.BertEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">BertEncoder</span></code></a>.
Every encoder has <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerBert</span></code> and
corresponding attention implementations <code class="xref py py-class docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code>.</p></li>
<li><p><strong>Configuration classes</strong> such as <code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code>, which store all the parameters
required to build a model. You don’t always need to instantiate these yourself. In particular, if you are using a
pretrained model without any modification, creating the model will automatically take care of instantiating the configuration
(which is part of the model).</p></li>
<li><p><strong>Tokenizer classes</strong> such as <code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertTokenizer</span></code>, which store the vocabulary for each model
and provide methods for encoding/decoding strings in a list of token embeddings indices to be fed to a model.
This is a faster implementation based on <strong>tensorflow-text</strong>.</p></li>
<li><p>All encoders can be used as decoder with one argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">use_decoder=True</span></code>.</p></li>
</ul>
<p>All these classes can be instantiated from pretrained instances and saved locally using two methods:</p>
<ul class="simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_pretrained()</span></code> lets you instantiate a model/configuration/tokenizer from a pretrained version either
provided by the library itself (the supported models are provided in the list <span class="xref std std-doc">here</span>) or
stored locally (or on a server) by the user,</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_checkpoint()</span></code> lets you save a model/ as tensorflow checkpoints.</p></li>
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_transformers_serialized()</span></code> lets you save a model/ as tensorflow saved models.</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../model_doc/albert.html" class="btn btn-neutral float-right" title="ALBERT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, The TFT Team, Licenced under the Apache License, Version 2.0.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-83738774-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-83738774-2', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>