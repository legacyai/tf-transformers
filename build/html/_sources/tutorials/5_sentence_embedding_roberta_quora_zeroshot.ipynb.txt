{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a635cd6a",
   "metadata": {},
   "source": [
    "# Create Sentence Embedding Roberta Model + Zeroshot from Scratch\n",
    "\n",
    "This tutorial contains complete code to fine-tune Roberta to build meaningful sentence transformers using Quora Dataset from HuggingFace. \n",
    "In addition to training a model, you will learn how to preprocess text into an appropriate format.\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the Quora dataset from HuggingFace\n",
    "- Load Roberta Model using tf-transformers\n",
    "- Build train and validation dataset  feature preparation using\n",
    "tokenizer from transformers.\n",
    "- Build your own model by combining Roberta with a CustomWrapper\n",
    "- Train your own model, fine-tuning Roberta as part of that\n",
    "- Save your model and use it to extract sentence embeddings\n",
    "- Use the end-to-end (inference) in production setup\n",
    "\n",
    "If you're new to working with the Quora dataset, please see [QUORA](https://huggingface.co/datasets/quora) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090bead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-transformers\n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "!pip install wandb\n",
    "\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42175d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fda2757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.7.0\n",
      "Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import collections\n",
    "import wandb\n",
    "import tempfile\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "print(\"Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "from tf_transformers.models import RobertaModel, Classification_Model\n",
    "from tf_transformers.core import Trainer\n",
    "from tf_transformers.optimization import create_optimizer\n",
    "from tf_transformers.data import TFWriter, TFReader\n",
    "from tf_transformers.losses import cross_entropy_loss_for_classification\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca794e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e23b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset quora (/home/jovyan/.cache/huggingface/datasets/quora/default/0.0.0/36ba4cd42107f051a158016f1bea6ae3f4685c5df843529108a54e42d86c1e04)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629843610aa143b5844179e02574e5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset stsb_multi_mt (/home/jovyan/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30cba25a78b4b0c9428d646f905f55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "model_name = 'roberta-base'\n",
    "dataset = load_dataset(\"quora\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load validation dataset\n",
    "sts_b = load_dataset(\"stsb_multi_mt\", 'en')\n",
    "\n",
    "# Define length for examples\n",
    "max_sequence_length = 128\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994018d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e539c0",
   "metadata": {},
   "source": [
    "### Prepare Training TFRecords using Quora\n",
    "\n",
    "* 1. Download Quora dataset.\n",
    "* 2. We will take only those row where ```is_duplicate=True```. The model will be trained using ```in-batch``` negative loss.  \n",
    "* 3. Example data looks like a pair of sentences\n",
    "           ```sentence1 (left sentence): What is the best Android smartphone?,\n",
    "              sentence2 (right sentence): What is the best Android smartphone ever?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97536a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 404290 in 276.39959359169006 seconds\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "def parse_train(dataset, tokenizer, max_passage_length, key):\n",
    "    \"\"\"Function to parse examples which are is_duplicate=1\n",
    "\n",
    "    Args:\n",
    "        dataset (:obj:`dataet`): HF dataset\n",
    "        tokenizer (:obj:`tokenizer`): HF Tokenizer\n",
    "        max_passage_length (:obj:`int`): Passage Length\n",
    "        key (:obj:`str`): Key of dataset (`train`, `validation` etc)\n",
    "    \"\"\"    \n",
    "    result = {}\n",
    "    for f in dataset[key]:\n",
    "       \n",
    "        question_left , question_right = f['questions']['text']\n",
    "        question_left_input_ids =  tokenizer(question_left, max_length=max_passage_length, truncation=True)['input_ids'] \n",
    "        question_right_input_ids  =  tokenizer(question_right, max_length=max_passage_length, truncation=True)['input_ids']\n",
    "        \n",
    "        result = {}\n",
    "        result['input_ids_left'] = question_left_input_ids\n",
    "        result['input_ids_right'] = question_right_input_ids\n",
    "        \n",
    "        yield result\n",
    "        \n",
    "# Write using TF Writer\n",
    "schema = {\n",
    "    \"input_ids_left\": (\"var_len\", \"int\"),\n",
    "    \"input_ids_right\": (\"var_len\", \"int\")\n",
    "    \n",
    "}\n",
    "\n",
    "tfrecord_train_dir = tempfile.mkdtemp()\n",
    "tfrecord_filename = 'quora'\n",
    "\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_filename, \n",
    "                    model_dir=tfrecord_train_dir,\n",
    "                    tag='train',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "\n",
    "# Train dataset\n",
    "train_parser_fn = parse_train(dataset, tokenizer, max_sequence_length, key='train')\n",
    "tfwriter.process(parse_fn=train_parser_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a060cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad2be3b3",
   "metadata": {},
   "source": [
    "### Prepare Validation TFRecords using STS-b\n",
    "\n",
    "1. Download STS dataset.\n",
    "2. We will use this dataset to measure sentence embeddings by measuring the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1b5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Total individual observations/examples written is 1500 in 1.0107736587524414 seconds\n",
      "INFO:absl:All writer objects closed\n"
     ]
    }
   ],
   "source": [
    "def parse_dev(dataset, tokenizer, max_passage_length, key):\n",
    "    \"\"\"Function to parse examples which are is_duplicate=1\n",
    "\n",
    "    Args:\n",
    "        dataset (:obj:`dataet`): HF dataset\n",
    "        tokenizer (:obj:`tokenizer`): HF Tokenizer\n",
    "        max_passage_length (:obj:`int`): Passage Length\n",
    "        key (:obj:`str`): Key of dataset (`train`, `validation` etc)\n",
    "    \"\"\"    \n",
    "    result = {}\n",
    "    max_score = 5.0\n",
    "    min_score = 0.0\n",
    "    for f in dataset[key]:\n",
    "        \n",
    "        question_left = f['sentence1']\n",
    "        question_right = f['sentence2']\n",
    "        question_left_input_ids =  tokenizer(question_left, max_length=max_passage_length, truncation=True)['input_ids'] \n",
    "        question_right_input_ids  =  tokenizer(question_right, max_length=max_passage_length, truncation=True)['input_ids']\n",
    "        \n",
    "        result = {}\n",
    "        result['input_ids_left'] = question_left_input_ids\n",
    "        result['input_ids_right'] = question_right_input_ids\n",
    "        score = f['similarity_score']\n",
    "        # Normalize scores\n",
    "        result['score'] = (score - min_score) / (max_score - min_score)\n",
    "        yield result\n",
    "        \n",
    "# Write using TF Writer\n",
    "schema = {\n",
    "    \"input_ids_left\": (\"var_len\", \"int\"),\n",
    "    \"input_ids_right\": (\"var_len\", \"int\"),\n",
    "    \"score\": (\"var_len\", \"float\")\n",
    "    \n",
    "}\n",
    "\n",
    "tfrecord_validation_dir = tempfile.mkdtemp()\n",
    "tfrecord_validation_filename = 'sts'\n",
    "\n",
    "tfwriter = TFWriter(schema=schema, \n",
    "                    file_name=tfrecord_validation_filename, \n",
    "                    model_dir=tfrecord_validation_dir,\n",
    "                    tag='eval',\n",
    "                    overwrite=True\n",
    "                    )\n",
    "\n",
    "# Train dataset\n",
    "dev_parser_fn = parse_dev(sts_b, tokenizer, max_sequence_length, key='dev')\n",
    "tfwriter.process(parse_fn=dev_parser_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5a8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a97dd5f2",
   "metadata": {},
   "source": [
    "### Prepare  Training and Validation Dataset from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e03255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 01:10:07.501286: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-23 01:10:08.934282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30945 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\n",
      "2022-03-23 01:10:08.938622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30945 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# Read TFRecord\n",
    "\n",
    "def add_mask_type_ids(item):\n",
    "    \n",
    "    item['input_mask_left'] = tf.ones_like(item['input_ids_left'])\n",
    "    item['input_type_ids_left']= tf.zeros_like(item['input_ids_left'])\n",
    "    item['input_mask_right'] = tf.ones_like(item['input_ids_right'])\n",
    "    item['input_type_ids_right']= tf.zeros_like(item['input_ids_right'])\n",
    "    \n",
    "    labels = {}\n",
    "    if 'score' in item:\n",
    "        labels = {'score': item['score']}\n",
    "        del item['score']\n",
    "    \n",
    "    return item, labels\n",
    "\n",
    "# Train dataset\n",
    "schema = json.load(open(\"{}/schema.json\".format(tfrecord_train_dir)))\n",
    "total_train_examples = json.load(open(\"{}/stats.json\".format(tfrecord_train_dir)))['total_records']\n",
    "\n",
    "\n",
    "all_files = tf.io.gfile.glob(\"{}/*.tfrecord\".format(tfrecord_train_dir))\n",
    "tf_reader = TFReader(schema=schema, \n",
    "                    tfrecord_files=all_files)\n",
    "\n",
    "x_keys = ['input_ids_left', 'input_ids_right']\n",
    "train_dataset = tf_reader.read_record(auto_batch=False, \n",
    "                                   keys=x_keys,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys, \n",
    "                                   shuffle=True\n",
    "                                  )\n",
    "train_dataset = train_dataset.map(add_mask_type_ids, num_parallel_calls=tf.data.AUTOTUNE).padded_batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "# Validation dataset\n",
    "val_schema = json.load(open(\"{}/schema.json\".format(tfrecord_validation_dir)))\n",
    "all_val_files = tf.io.gfile.glob(\"{}/*.tfrecord\".format(tfrecord_validation_dir))\n",
    "tf_reader_val = TFReader(schema=val_schema, \n",
    "                    tfrecord_files=all_val_files)\n",
    "\n",
    "x_keys_val = ['input_ids_left', 'input_ids_right', 'score']\n",
    "validation_dataset = tf_reader_val.read_record(auto_batch=False, \n",
    "                                   keys=x_keys_val,\n",
    "                                   batch_size=batch_size, \n",
    "                                   x_keys = x_keys_val, \n",
    "                                   shuffle=True\n",
    "                                  )\n",
    "\n",
    "# Static shapes makes things faster inside tf.function\n",
    "# Especially for validation as we are passing batch examples to tf.function\n",
    "padded_shapes = ({'input_ids_left': [max_sequence_length,], \n",
    "                 'input_mask_left':[max_sequence_length,],\n",
    "                 'input_type_ids_left':[max_sequence_length,],\n",
    "                 'input_ids_right': [max_sequence_length,],\n",
    "                 'input_mask_right': [max_sequence_length,],\n",
    "                 'input_type_ids_right': [max_sequence_length,]\n",
    "                }, \n",
    "                 {'score': [None,]})\n",
    "validation_dataset = validation_dataset.map(add_mask_type_ids,\n",
    "                                            num_parallel_calls=tf.data.AUTOTUNE).padded_batch(batch_size,\n",
    "                                                                                              drop_remainder=False,\n",
    "                                                                                              padded_shapes=padded_shapes\n",
    "                                                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd01cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8335058",
   "metadata": {},
   "source": [
    "### Build Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1e8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_transformers.core import LegacyLayer, LegacyModel\n",
    "\n",
    "\n",
    "class Sentence_Embedding_Model(LegacyLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        is_training=False,\n",
    "        use_dropout=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Simple Sentence Embedding using Keras Layer\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`LegacyLayer/LegacyModel`):\n",
    "                Model.\n",
    "                Eg:`~tf_transformers.model.BertModel`.\n",
    "            is_training (:obj:`bool`, `optional`, defaults to False): To train\n",
    "            use_dropout (:obj:`bool`, `optional`, defaults to False): Use dropout\n",
    "            use_bias (:obj:`bool`, `optional`, defaults to True): use bias\n",
    "        \"\"\"\n",
    "        super(Sentence_Embedding_Model, self).__init__(\n",
    "            is_training=is_training, use_dropout=use_dropout, name=model.name, **kwargs\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        if isinstance(model, LegacyModel):\n",
    "            self.model_config = model.model_config\n",
    "        elif isinstance(model, tf.keras.layers.Layer):\n",
    "            self.model_config = model._config_dict\n",
    "        self._is_training = is_training\n",
    "        self._use_dropout = use_dropout\n",
    "\n",
    "        # Initialize model\n",
    "        self.model_inputs, self.model_outputs = self.get_model(initialize_only=True)\n",
    "        \n",
    "    def get_mean_embeddings(self, token_embeddings, input_mask):\n",
    "        \"\"\"\n",
    "        Mean embeddings\n",
    "        \"\"\"\n",
    "        cls_embeddings = token_embeddings[:, 0, :] # 0 is CLS (<s>)\n",
    "        # mask PAD tokens\n",
    "        token_emb_masked = token_embeddings * tf.cast(tf.expand_dims(input_mask, 2), tf.float32)\n",
    "        total_non_padded_tokens_per_batch = tf.cast(tf.reduce_sum(input_mask, axis=1), tf.float32)\n",
    "        # Convert to 2D\n",
    "        total_non_padded_tokens_per_batch = tf.expand_dims(total_non_padded_tokens_per_batch, 1)\n",
    "        mean_embeddings = tf.reduce_sum(token_emb_masked, axis=1)/ total_non_padded_tokens_per_batch\n",
    "        return mean_embeddings\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call\"\"\"\n",
    "        \n",
    "        # Extract left and right input pairs\n",
    "        left_inputs = {k.replace('_left', ''):v for k,v in inputs.items() if 'left' in k}\n",
    "        right_inputs = {k.replace('_right', ''):v for k,v in inputs.items() if 'right' in k}\n",
    "        model_outputs_left = self.model(left_inputs)\n",
    "        model_outputs_right = self.model(right_inputs)\n",
    "        \n",
    "        left_cls = model_outputs_left['cls_output']\n",
    "        right_cls = model_outputs_right['cls_output']        \n",
    "\n",
    "        left_mean_embeddings  = self.get_mean_embeddings(model_outputs_left['token_embeddings'], left_inputs['input_mask'])\n",
    "        right_mean_embeddings  = self.get_mean_embeddings(model_outputs_right['token_embeddings'], right_inputs['input_mask'])\n",
    "        \n",
    "        cls_logits = tf.matmul(left_cls, right_cls, transpose_b=True)\n",
    "        mean_logits = tf.matmul(left_mean_embeddings, right_mean_embeddings, transpose_b=True)\n",
    "        \n",
    "        \n",
    "        results = {'left_cls_output': left_cls, \n",
    "                   'right_cls_output': right_cls, \n",
    "                   'left_mean_embeddings': left_mean_embeddings,\n",
    "                   'right_mean_embeddings': right_mean_embeddings,\n",
    "                   'cls_logits': cls_logits, \n",
    "                   'mean_logits': mean_logits}\n",
    "        \n",
    "        return results\n",
    "        \n",
    "\n",
    "    def get_model(self, initialize_only=False):\n",
    "        \"\"\"Get model\"\"\"\n",
    "        inputs = self.model.input\n",
    "        # Left and Right inputs\n",
    "        main_inputs = {}\n",
    "        for k, v in inputs.items():\n",
    "            shape = v.shape\n",
    "            main_inputs[k+'_left'] = tf.keras.layers.Input(\n",
    "                            shape[1:], batch_size=v.shape[0], name=k+'_left', dtype=v.dtype\n",
    "                        )\n",
    "            \n",
    "        for k, v in inputs.items():\n",
    "            shape = v.shape\n",
    "            main_inputs[k+'_right'] = tf.keras.layers.Input(\n",
    "                            shape[1:], batch_size=v.shape[0], name=k+'_right', dtype=v.dtype\n",
    "                        )        \n",
    "        layer_outputs = self(main_inputs)\n",
    "        if initialize_only:\n",
    "            return main_inputs, layer_outputs\n",
    "        model = LegacyModel(inputs=main_inputs, outputs=layer_outputs, name=\"sentence_embedding_model\")\n",
    "        model.model_config = self.model_config\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b75439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9988fbb5",
   "metadata": {},
   "source": [
    "### Load Model, Optimizer , Trainer\n",
    "\n",
    "Our Trainer expects ```model```, ```optimizer``` and ```loss``` to be a function.\n",
    "\n",
    "* 1. We will use ```Roberta``` as the base model and pass it to ```Sentence_Embedding_Model```, layer we built\n",
    "* 2. We will use ```in-batch``` loss as the loss function, where every diagonal entry in the output is positive\n",
    "and rest is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be22032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def get_model(model_name, is_training, use_dropout):\n",
    "    \"\"\"Get Model\"\"\"\n",
    "    def model_fn():\n",
    "        model = RobertaModel.from_pretrained(model_name)\n",
    "        sentence_transformers_model = Sentence_Embedding_Model(model)\n",
    "        sentence_transformers_model = sentence_transformers_model.get_model()\n",
    "        return sentence_transformers_model\n",
    "    return model_fn\n",
    "\n",
    "# Load Optimizer\n",
    "def get_optimizer(learning_rate, examples, batch_size, epochs, use_constant_lr=False):\n",
    "    \"\"\"Get optimizer\"\"\"\n",
    "    steps_per_epoch = int(examples / batch_size)\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(0.1 * num_train_steps)\n",
    "\n",
    "    def optimizer_fn():\n",
    "        optimizer, learning_rate_fn = create_optimizer(learning_rate, num_train_steps, warmup_steps, use_constant_lr=use_constant_lr)\n",
    "        return optimizer\n",
    "\n",
    "    return optimizer_fn\n",
    "\n",
    "# Load trainer\n",
    "def get_trainer(distribution_strategy, num_gpus=0, tpu_address=None):\n",
    "    \"\"\"Get Trainer\"\"\"\n",
    "    trainer = Trainer(distribution_strategy, num_gpus=num_gpus, tpu_address=tpu_address)\n",
    "    return trainer\n",
    "\n",
    "# Create loss\n",
    "def in_batch_negative_loss():\n",
    "    \n",
    "    def loss_fn(y_true_dict, y_pred_dict):\n",
    "        \n",
    "        labels = tf.range(y_pred_dict['cls_logits'].shape[0])\n",
    "        cls_loss  = cross_entropy_loss_for_classification(labels=labels, logits=y_pred_dict['cls_logits'])\n",
    "        mean_loss = cross_entropy_loss_for_classification(labels=labels, logits=y_pred_dict['mean_logits'])\n",
    "        \n",
    "        result = {}\n",
    "        result['cls_loss'] = cls_loss\n",
    "        result['mean_loss'] = mean_loss\n",
    "        result['loss'] = (cls_loss + mean_loss)/2.0\n",
    "        return result\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155acb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24ee408",
   "metadata": {},
   "source": [
    "### Wandb Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8192d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"TUTORIALS\"\n",
    "display_name = \"roberta_quora_sentence_embedding\"\n",
    "wandb.init(project=project, name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed867e8c",
   "metadata": {},
   "source": [
    "### Zero-Shot on STS before Training\n",
    "\n",
    "* 1. Lets evaluate how good ```Roberta``` is to capture sentence embeddings before ```fine-tuning``` with Quora.\n",
    "* 2. This gives us an indication whether the model is learning something or not on downstream fine-tuning.\n",
    "* 3. We use ```CLS_OUTPUT```, pooler output of ```Roberta``` model as sentence embedding and evaluate using\n",
    "```pearson``` and ```spearman``` correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae633ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successful ✅✅: Model checkpoints matched and loaded from /home/jovyan/.cache/huggingface/hub/tftransformers__roberta-base-no-mlm.main.9e4aa91ba5936c6ac98586f85c152831e421d0ec/ckpt-1\n",
      "INFO:absl:Successful ✅: Loaded model from tftransformers/roberta-base-no-mlm\n",
      "12it [00:12,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.4278\tSpearman: 0.5293\n",
      "Manhattan-Distance:\tPearson: 0.4329\tSpearman: 0.5120\n",
      "Euclidean-Distance:\tPearson: 0.4365\tSpearman: 0.5125\n",
      "Dot-Product-Similarity:\tPearson: -0.0079\tSpearman: -0.0050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "model = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "sentence1_embeddings = []\n",
    "sentence2_embeddings = []\n",
    "sts_labels = []\n",
    "for batch_inputs, batch_labels in tqdm.tqdm(validation_dataset):\n",
    "    left_inputs = {k.replace('_left', ''):v for k,v in batch_inputs.items() if 'left' in k}\n",
    "    right_inputs = {k.replace('_right', ''):v for k,v in batch_inputs.items() if 'right' in k}\n",
    "    left_outputs = model(left_inputs)\n",
    "    right_outputs = model(right_inputs)\n",
    "    \n",
    "    # sentence 1 embeddings\n",
    "    sentence1_embeddings.append(left_outputs['cls_output'])\n",
    "    # sentence 2 embeddings\n",
    "    sentence2_embeddings.append(right_outputs['cls_output'])\n",
    "    sts_labels.append(batch_labels['score'])\n",
    "    \n",
    "sts_labels = tf.squeeze(tf.concat(sts_labels, axis=0), axis=1)\n",
    "sentence1_embeddings = tf.concat(sentence1_embeddings, axis=0)\n",
    "sentence2_embeddings = tf.concat(sentence2_embeddings, axis=0)\n",
    "\n",
    "cosine_scores = 1 - (paired_cosine_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy()))\n",
    "manhattan_distances = -paired_manhattan_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "euclidean_distances = -paired_euclidean_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "dot_products        = [np.dot(emb1, emb2) for emb1, emb2 in zip(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())]\n",
    "\n",
    "\n",
    "eval_pearson_cosine, _    = pearsonr(sts_labels, cosine_scores)\n",
    "eval_spearman_cosine, _   = spearmanr(sts_labels, cosine_scores)\n",
    "\n",
    "eval_pearson_manhattan, _  = pearsonr(sts_labels, manhattan_distances)\n",
    "eval_spearman_manhattan, _ = spearmanr(sts_labels, manhattan_distances)\n",
    "\n",
    "eval_pearson_euclidean, _  = pearsonr(sts_labels, euclidean_distances)\n",
    "eval_spearman_euclidean, _ = spearmanr(sts_labels, euclidean_distances)\n",
    "\n",
    "eval_pearson_dot, _  = pearsonr(sts_labels, dot_products)\n",
    "eval_spearman_dot, _ = spearmanr(sts_labels, dot_products)\n",
    "\n",
    "\n",
    "print(\"Cosine-Similarity :\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
    "    eval_pearson_cosine, eval_spearman_cosine))\n",
    "print(\"Manhattan-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
    "    eval_pearson_manhattan, eval_spearman_manhattan))\n",
    "print(\"Euclidean-Distance:\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
    "    eval_pearson_euclidean, eval_spearman_euclidean))\n",
    "print(\"Dot-Product-Similarity:\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
    "    eval_pearson_dot, eval_spearman_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3039e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063bd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "class STSEvaluationCallback:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, trainer_kwargs):\n",
    "\n",
    "        validation_dataset_distributed = iter(\n",
    "            trainer_kwargs[\"validation_dataset_distributed\"]\n",
    "        )\n",
    "        model = trainer_kwargs[\"model\"]\n",
    "        wandb = trainer_kwargs[\"wandb\"]\n",
    "        step = trainer_kwargs[\"global_step\"]\n",
    "        strategy = trainer_kwargs[\"strategy\"]\n",
    "        epoch = trainer_kwargs[\"epoch\"]\n",
    "        epochs = trainer_kwargs[\"epochs\"]\n",
    "        validation_steps = trainer_kwargs[\"validation_steps\"]\n",
    "\n",
    "        if validation_dataset_distributed is None:\n",
    "            raise ValueError(\n",
    "                \"No validation dataset has been provided either in the trainer class, \\\n",
    "                                 or when callback is initialized. Please provide a validation dataset\"\n",
    "            )\n",
    "\n",
    "        @tf.function\n",
    "        def validate_run(dist_inputs):\n",
    "            batch_inputs, batch_labels = dist_inputs\n",
    "            model_outputs = model(batch_inputs)\n",
    "            s1_cls = model_outputs['left_cls_output']\n",
    "            s2_cls = model_outputs['right_cls_output']\n",
    "            \n",
    "            s1_mean = model_outputs['left_mean_embeddings']\n",
    "            s2_mean = model_outputs['right_mean_embeddings']\n",
    "            return s1_cls, s2_cls, s1_mean, s2_mean, batch_labels['score']\n",
    "        \n",
    "        S1_cls = []\n",
    "        S2_cls = []\n",
    "        S1_mean = []\n",
    "        S2_mean = []\n",
    "        sts_labels = []\n",
    "        # This is a hack to make tqdm to print colour bar\n",
    "        # TODO: fix it .\n",
    "        pbar = tqdm.trange(validation_steps, colour=\"magenta\", unit=\"batch\")\n",
    "        for step_counter in pbar:\n",
    "            dist_inputs = next(validation_dataset_distributed)\n",
    "            s1_cls, s2_cls, s1_mean, s2_mean, batch_scores = strategy.run(\n",
    "                validate_run, args=(dist_inputs,)\n",
    "            )\n",
    "            s1_cls = tf.concat(\n",
    "                trainer.distribution_strategy.experimental_local_results(s1_cls),\n",
    "                axis=0,\n",
    "            )\n",
    "            s2_cls = tf.concat(\n",
    "                            trainer.distribution_strategy.experimental_local_results(s2_cls),\n",
    "                            axis=0,\n",
    "                        )\n",
    "            s1_mean = tf.concat(\n",
    "                            trainer.distribution_strategy.experimental_local_results(s1_mean),\n",
    "                            axis=0,\n",
    "                        )\n",
    "            s2_mean = tf.concat(\n",
    "                                trainer.distribution_strategy.experimental_local_results(s2_mean),\n",
    "                                        axis=0,\n",
    "                                    )\n",
    "            \n",
    "            scores = tf.concat(\n",
    "                trainer.distribution_strategy.experimental_local_results(\n",
    "                    batch_scores\n",
    "                ),\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            S1_cls.append(s1_cls)\n",
    "            S2_cls.append(s2_cls)\n",
    "            S1_mean.append(s1_mean)\n",
    "            S2_mean.append(s2_mean)\n",
    "            sts_labels.append(scores)\n",
    "            pbar.set_description(\n",
    "                \"Callback: Epoch {}/{} --- Step {}/{} \".format(\n",
    "                    epoch, epochs, step_counter, validation_steps\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "        sts_labels = tf.squeeze(tf.concat(sts_labels, axis=0), axis=1)\n",
    "        sentence1_embeddings = tf.concat(S1_cls, axis=0)\n",
    "        sentence2_embeddings = tf.concat(S2_cls, axis=0)\n",
    "\n",
    "        cosine_scores = 1 - (paired_cosine_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy()))\n",
    "        manhattan_distances = -paired_manhattan_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "        euclidean_distances = -paired_euclidean_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "        dot_products        = [np.dot(emb1, emb2) for emb1, emb2 in zip(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())]\n",
    "\n",
    "\n",
    "        eval_pearson_cosine, _    = pearsonr(sts_labels, cosine_scores)\n",
    "        eval_spearman_cosine, _   = spearmanr(sts_labels, cosine_scores)\n",
    "\n",
    "        eval_pearson_manhattan, _  = pearsonr(sts_labels, manhattan_distances)\n",
    "        eval_spearman_manhattan, _ = spearmanr(sts_labels, manhattan_distances)\n",
    "\n",
    "        eval_pearson_euclidean, _  = pearsonr(sts_labels, euclidean_distances)\n",
    "        eval_spearman_euclidean, _ = spearmanr(sts_labels, euclidean_distances)\n",
    "\n",
    "        eval_pearson_dot, _  = pearsonr(sts_labels, dot_products)\n",
    "        eval_spearman_dot, _ = spearmanr(sts_labels, dot_products)\n",
    "\n",
    "        metrics_result = {'pearson_cosine_cls': eval_pearson_cosine,\n",
    "                          'spearman_cosine_cls': eval_spearman_cosine,\n",
    "                          'pearson_manhattan_cls': eval_pearson_manhattan, \n",
    "                          'spearman_manhattan_cls': eval_spearman_manhattan, \n",
    "                          'pearson_euclidean_cls': eval_pearson_euclidean, \n",
    "                          'spearman_euclidean_cls': eval_spearman_euclidean, \n",
    "                          'pearson_dot_cls': eval_pearson_dot, \n",
    "                          'spearman_dot_cls': eval_spearman_dot}\n",
    "        \n",
    "        sentence1_embeddings = tf.concat(S1_mean, axis=0)\n",
    "        sentence2_embeddings = tf.concat(S2_mean, axis=0)\n",
    "\n",
    "        cosine_scores = 1 - (paired_cosine_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy()))\n",
    "        manhattan_distances = -paired_manhattan_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "        euclidean_distances = -paired_euclidean_distances(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())\n",
    "        dot_products        = [np.dot(emb1, emb2) for emb1, emb2 in zip(sentence1_embeddings.numpy(), sentence2_embeddings.numpy())]\n",
    "\n",
    "\n",
    "        eval_pearson_cosine, _    = pearsonr(sts_labels, cosine_scores)\n",
    "        eval_spearman_cosine, _   = spearmanr(sts_labels, cosine_scores)\n",
    "\n",
    "        eval_pearson_manhattan, _  = pearsonr(sts_labels, manhattan_distances)\n",
    "        eval_spearman_manhattan, _ = spearmanr(sts_labels, manhattan_distances)\n",
    "\n",
    "        eval_pearson_euclidean, _  = pearsonr(sts_labels, euclidean_distances)\n",
    "        eval_spearman_euclidean, _ = spearmanr(sts_labels, euclidean_distances)\n",
    "\n",
    "        eval_pearson_dot, _  = pearsonr(sts_labels, dot_products)\n",
    "        eval_spearman_dot, _ = spearmanr(sts_labels, dot_products)\n",
    "        \n",
    "        metrics_result_mean = {'pearson_cosine_mean': eval_pearson_cosine,\n",
    "                          'spearman_cosine_mean': eval_spearman_cosine,\n",
    "                          'pearson_manhattan_mean': eval_pearson_manhattan, \n",
    "                          'spearman_manhattan_mean': eval_spearman_manhattan, \n",
    "                          'pearson_euclidean_mean': eval_pearson_euclidean, \n",
    "                          'spearman_euclidean_mean': eval_spearman_euclidean, \n",
    "                          'pearson_dot_mean': eval_pearson_dot, \n",
    "                          'spearman_dot_mean': eval_spearman_dot}\n",
    "        \n",
    "        metrics_result.update(metrics_result_mean)\n",
    "        pbar.set_postfix(**metrics_result)\n",
    "        \n",
    "        if wandb:\n",
    "            wandb.log(metrics_result, step=step)\n",
    "\n",
    "        return metrics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02777496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f9b69a7",
   "metadata": {},
   "source": [
    "### Set Hyperparameters and Configs\n",
    "\n",
    "1. Set necessay hyperparameters.\n",
    "2. Prepare ```train dataset```, ```validation dataset```.\n",
    "3. Load ```model```, ```optimizer```, ```loss``` and ```trainer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57756cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# Model configs\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "model_checkpoint_dir = 'MODELS/roberta_quora_embeddings'\n",
    "\n",
    "\n",
    "# Total train examples\n",
    "steps_per_epoch = total_train_examples // batch_size\n",
    "\n",
    "# model\n",
    "model_fn =  get_model(model_name, is_training=True, use_dropout=True)\n",
    "# optimizer\n",
    "optimizer_fn = get_optimizer(learning_rate, total_train_examples, batch_size, epochs)\n",
    "# trainer (multi gpu strategy)\n",
    "trainer = get_trainer(distribution_strategy='mirrored', num_gpus=2)\n",
    "# loss\n",
    "loss_fn = in_batch_negative_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082505d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c4246a",
   "metadata": {},
   "source": [
    "### Train :-)\n",
    "\n",
    "* 1. Loss is coming down in epoch 1 itself.\n",
    "* 2. Zershot evaluation after ```epoch 1``` shows that, ```pearson``` and ```spearman``` correlation increases to\n",
    "```0.80```, which is significant improvemnet over ```Roberta``` base model, where we got ```0.43```.\n",
    "* 3. Without training on ```STS-B```, we got a good evaluation score on ```STS-B dev``` using Zeroshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fcd18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Make sure `steps_per_epoch` should be less than or equal to number of batches in dataset.\n",
      "INFO:absl:Policy: ----> float32\n",
      "INFO:absl:Strategy: ---> <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f0b5c0287d0>\n",
      "INFO:absl:Num GPU Devices: ---> 2\n",
      "INFO:absl:Successful ✅✅: Model checkpoints matched and loaded from /home/jovyan/.cache/huggingface/hub/tftransformers__roberta-base-no-mlm.main.9e4aa91ba5936c6ac98586f85c152831e421d0ec/ckpt-1\n",
      "INFO:absl:Successful ✅: Loaded model from tftransformers/roberta-base-no-mlm\n",
      "INFO:absl:Using linear optimization warmup\n",
      "INFO:absl:Using Adamw optimizer\n",
      "INFO:absl:No ❌❌ checkpoint found in MODELS/roberta_quora_embeddings\n",
      "Train: Epoch 1/4 --- Step 10/3158 --- total examples 0 , trainable variables 199:   0%|\u001b[32m          \u001b[0m| 0/315 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 198 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Train: Epoch 1/4 --- Step 3150/3158 --- total examples 401920 , trainable variables 199: 100%|\u001b[32m██████████\u001b[0m| 315/315 [27:15<00:00,  5.19s/batch , _runtime=1803, _timestamp=1.65e+9, cls_loss=0.504, learning_rate=1.34e-5, loss=0.493, mean_loss=0.482]\n",
      "INFO:absl:Model saved at epoch 1 at MODELS/roberta_quora_embeddings/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[34m          \u001b[0m| 0/12 [00:00<?, ?batch /s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Validation: Epoch 1/4 --- Step 0/12 :   8%|\u001b[34m▊         \u001b[0m| 1/12 [00:11<02:07, 11.57s/batch , cls_loss=2.63, loss=2.8, mean_loss=2.96]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Validation: Epoch 1/4 --- Step 11/12 : 100%|\u001b[34m██████████\u001b[0m| 12/12 [00:24<00:00,  2.07s/batch , cls_loss=1.62, loss=1.65, mean_loss=1.68]\n",
      "INFO:absl:Validation result at epcoh 1 and                 global step 3150 is {'cls_loss': 1.6162163, 'mean_loss': 1.6796235, 'loss': 1.6479198}\n",
      "INFO:absl:Callbacks in progress at epoch end 1 . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Callback: Epoch 1/3 --- Step 11/12 : 100%|\u001b[35m██████████\u001b[0m| 12/12 [00:21<00:00,  1.76s/batch]\n",
      "INFO:absl:Callback score {'pearson_cosine_cls': 0.8199941582015672, 'spearman_cosine_cls': 0.8220972132455343, 'pearson_manhattan_cls': 0.8184392097896854, 'spearman_manhattan_cls': 0.8164570492108482, 'pearson_euclidean_cls': 0.8190927383440411, 'spearman_euclidean_cls': 0.8172409394315345, 'pearson_dot_cls': 0.7828011052166998, 'spearman_dot_cls': 0.7807641366784325, 'pearson_cosine_mean': 0.8151801531088095, 'spearman_cosine_mean': 0.8162854946579012, 'pearson_manhattan_mean': 0.8145520799669964, 'spearman_manhattan_mean': 0.8123405339144811, 'pearson_euclidean_mean': 0.8148764479876582, 'spearman_euclidean_mean': 0.8132354135356057, 'pearson_dot_mean': 0.7381403305760383, 'spearman_dot_mean': 0.7337835384203213, '_timestamp': 1647999708, '_runtime': 1854} at epoch 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 1 < 3150; dropping {'pearson_cosine_cls': 0.8199941582015672, 'spearman_cosine_cls': 0.8220972132455343, 'pearson_manhattan_cls': 0.8184392097896854, 'spearman_manhattan_cls': 0.8164570492108482, 'pearson_euclidean_cls': 0.8190927383440411, 'spearman_euclidean_cls': 0.8172409394315345, 'pearson_dot_cls': 0.7828011052166998, 'spearman_dot_cls': 0.7807641366784325, 'pearson_cosine_mean': 0.8151801531088095, 'spearman_cosine_mean': 0.8162854946579012, 'pearson_manhattan_mean': 0.8145520799669964, 'spearman_manhattan_mean': 0.8123405339144811, 'pearson_euclidean_mean': 0.8148764479876582, 'spearman_euclidean_mean': 0.8132354135356057, 'pearson_dot_mean': 0.7381403305760383, 'spearman_dot_mean': 0.7337835384203213, '_timestamp': 1647999708, '_runtime': 1854}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Epoch 2/4 --- Step 3150/3158 --- total examples 805120 , trainable variables 199: 100%|\u001b[32m██████████\u001b[0m| 315/315 [26:13<00:00,  5.00s/batch , _runtime=3428, _timestamp=1.65e+9, cls_loss=0.304, learning_rate=6.71e-6, loss=0.305, mean_loss=0.305]\n",
      "INFO:absl:Model saved at epoch 2 at MODELS/roberta_quora_embeddings/ckpt-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 2/4 --- Step 11/12 : 100%|\u001b[34m██████████\u001b[0m| 12/12 [00:05<00:00,  2.27batch /s, cls_loss=1.78, loss=1.82, mean_loss=1.85]\n",
      "INFO:absl:Validation result at epcoh 2 and                 global step 6300 is {'cls_loss': 1.778288, 'mean_loss': 1.8532048, 'loss': 1.8157464}\n",
      "INFO:absl:Callbacks in progress at epoch end 2 . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Callback: Epoch 2/3 --- Step 11/12 : 100%|\u001b[35m██████████\u001b[0m| 12/12 [00:19<00:00,  1.61s/batch]\n",
      "INFO:absl:Callback score {'pearson_cosine_cls': 0.8082112012752523, 'spearman_cosine_cls': 0.8088788767212841, 'pearson_manhattan_cls': 0.7977193919551161, 'spearman_manhattan_cls': 0.79662337716043, 'pearson_euclidean_cls': 0.7982407615058535, 'spearman_euclidean_cls': 0.7970524557483568, 'pearson_dot_cls': 0.7645641878510724, 'spearman_dot_cls': 0.7678639160320804, 'pearson_cosine_mean': 0.8030011391493671, 'spearman_cosine_mean': 0.8044760711917577, 'pearson_manhattan_mean': 0.7959895612836713, 'spearman_manhattan_mean': 0.7952571982816723, 'pearson_euclidean_mean': 0.7974056893147314, 'spearman_euclidean_mean': 0.7970287600024667, 'pearson_dot_mean': 0.7324014153178778, 'spearman_dot_mean': 0.7335963354441554, '_timestamp': 1648001312, '_runtime': 3458} at epoch 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 2 < 6300; dropping {'pearson_cosine_cls': 0.8082112012752523, 'spearman_cosine_cls': 0.8088788767212841, 'pearson_manhattan_cls': 0.7977193919551161, 'spearman_manhattan_cls': 0.79662337716043, 'pearson_euclidean_cls': 0.7982407615058535, 'spearman_euclidean_cls': 0.7970524557483568, 'pearson_dot_cls': 0.7645641878510724, 'spearman_dot_cls': 0.7678639160320804, 'pearson_cosine_mean': 0.8030011391493671, 'spearman_cosine_mean': 0.8044760711917577, 'pearson_manhattan_mean': 0.7959895612836713, 'spearman_manhattan_mean': 0.7952571982816723, 'pearson_euclidean_mean': 0.7974056893147314, 'spearman_euclidean_mean': 0.7970287600024667, 'pearson_dot_mean': 0.7324014153178778, 'spearman_dot_mean': 0.7335963354441554, '_timestamp': 1648001312, '_runtime': 3458}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: Epoch 3/4 --- Step 3150/3158 --- total examples 1208320 , trainable variables 199: 100%|\u001b[32m██████████\u001b[0m| 315/315 [26:08<00:00,  4.98s/batch , _runtime=5027, _timestamp=1.65e+9, cls_loss=0.275, learning_rate=6.02e-8, loss=0.278, mean_loss=0.282]\n",
      "INFO:absl:Model saved at epoch 3 at MODELS/roberta_quora_embeddings/ckpt-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: Epoch 3/4 --- Step 11/12 : 100%|\u001b[34m██████████\u001b[0m| 12/12 [00:05<00:00,  2.27batch /s, cls_loss=2.3, loss=2.29, mean_loss=2.28] \n",
      "INFO:absl:Validation result at epcoh 3 and                 global step 9450 is {'cls_loss': 2.2950742, 'mean_loss': 2.2835078, 'loss': 2.2892911}\n",
      "INFO:absl:Callbacks in progress at epoch end 3 . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Callback: Epoch 3/3 --- Step 11/12 : 100%|\u001b[35m██████████\u001b[0m| 12/12 [00:18<00:00,  1.56s/batch]\n",
      "INFO:absl:Callback score {'pearson_cosine_cls': 0.8118276970012877, 'spearman_cosine_cls': 0.8110754654257855, 'pearson_manhattan_cls': 0.7893045752002403, 'spearman_manhattan_cls': 0.7901086696302247, 'pearson_euclidean_cls': 0.789695365243242, 'spearman_euclidean_cls': 0.7900715861621009, 'pearson_dot_cls': 0.7764208929832053, 'spearman_dot_cls': 0.7831285760325771, 'pearson_cosine_mean': 0.8074910790403766, 'spearman_cosine_mean': 0.8084473888790257, 'pearson_manhattan_mean': 0.792546459118103, 'spearman_manhattan_mean': 0.794987013041834, 'pearson_euclidean_mean': 0.7943711503130662, 'spearman_euclidean_mean': 0.7970291923871069, 'pearson_dot_mean': 0.7619295041302732, 'spearman_dot_mean': 0.7644860560497375, '_timestamp': 1648002910, '_runtime': 5056} at epoch 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 3 < 9450; dropping {'pearson_cosine_cls': 0.8118276970012877, 'spearman_cosine_cls': 0.8110754654257855, 'pearson_manhattan_cls': 0.7893045752002403, 'spearman_manhattan_cls': 0.7901086696302247, 'pearson_euclidean_cls': 0.789695365243242, 'spearman_euclidean_cls': 0.7900715861621009, 'pearson_dot_cls': 0.7764208929832053, 'spearman_dot_cls': 0.7831285760325771, 'pearson_cosine_mean': 0.8074910790403766, 'spearman_cosine_mean': 0.8084473888790257, 'pearson_manhattan_mean': 0.792546459118103, 'spearman_manhattan_mean': 0.794987013041834, 'pearson_euclidean_mean': 0.7943711503130662, 'spearman_euclidean_mean': 0.7970291923871069, 'pearson_dot_mean': 0.7619295041302732, 'spearman_dot_mean': 0.7644860560497375, '_timestamp': 1648002910, '_runtime': 5056}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sts_callback = STSEvaluationCallback()\n",
    "history = trainer.run(\n",
    "    model_fn=model_fn,\n",
    "    optimizer_fn=optimizer_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    train_loss_fn=loss_fn,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    model_checkpoint_dir=model_checkpoint_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_dataset=validation_dataset,\n",
    "    validation_loss_fn=loss_fn,\n",
    "    training_loss_names = ['cls_loss', 'mean_loss'],\n",
    "    validation_loss_names = ['cls_loss', 'mean_loss'],\n",
    "    steps_per_call=10,\n",
    "    callbacks=[sts_callback],\n",
    "    wandb=wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac167a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36a82a27",
   "metadata": {},
   "source": [
    "### Visualize the Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir MODELS/roberta_quora_embeddings/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1ff85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44843a0",
   "metadata": {},
   "source": [
    "### Load Trained Model for Testing and Save it as serialzed model\n",
    "\n",
    "* 1. To get good sentence embedding , we need only ```Roberta``` model, which has been used as the ```base``` for\n",
    "```Sentence_Embedding_Model``` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e737fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as word_embeddings_layer_call_fn, word_embeddings_layer_call_and_return_conditional_losses, type_embeddings_layer_call_fn, type_embeddings_layer_call_and_return_conditional_losses, positional_embeddings_layer_call_fn while saving (showing 5 of 870). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MODELS/roberta_quora_embeddings/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MODELS/roberta_quora_embeddings/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save serialized version of the model\n",
    "\n",
    "# Note: Ignore checkpoint warnings, it is because we save optimizer with checkpoint\n",
    "# while we restoring, we take only model.\n",
    "\n",
    "\n",
    "model_fn =  get_model(model_name, is_training=False, use_dropout=False)\n",
    "model = model_fn()\n",
    "model.load_checkpoint(model_checkpoint_dir)\n",
    "\n",
    "# Roberta base (model.layers[-1] is Sentence_Embedding_Model )\n",
    "model = model.layers[-1].model\n",
    "model.save_transformers_serialized('{}/saved_model/'.format(model_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4bc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a02d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f27c98c7",
   "metadata": {},
   "source": [
    "### Model Serialization (Production)\n",
    "\n",
    "* 1. Lets see how we can use this model to extract sentence embeddings\n",
    "* 2. Print top K similar sentences from our embeddings from Quora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6e6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load serialized model\n",
    "\n",
    "loaded = tf.saved_model.load(\"{}/saved_model/\".format(model_checkpoint_dir))\n",
    "model = loaded.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f40b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff7a3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences 100000\n"
     ]
    }
   ],
   "source": [
    "# Take 100000 sentences from Quora and calculate embeddings of that\n",
    "quora_questions = []\n",
    "for item in dataset['train']:\n",
    "    quora_questions.extend(item['questions']['text'])\n",
    "    \n",
    "quora_questions = list(set(quora_questions))\n",
    "quora_questions = quora_questions[:100000] # Take 100000\n",
    "print(\"Total sentences {}\".format(len(quora_questions)))\n",
    "\n",
    "# Prepare Dataset\n",
    "quora_dataset = tf.data.Dataset.from_tensor_slices({'questions': quora_questions})\n",
    "quora_dataset = quora_dataset.batch(batch_size, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cd734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738190ce",
   "metadata": {},
   "source": [
    "### Quora Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d129c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [03:30<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "quora_sentence_embeddings = []\n",
    "for batch_questions in tqdm.tqdm(quora_dataset):\n",
    "    batch_questions = batch_questions['questions'].numpy().tolist()\n",
    "    batch_questions = [q.decode() for q in batch_questions]\n",
    "    \n",
    "    # Tokenize\n",
    "    quora_inputs = tokenizer(batch_questions, max_length=max_sequence_length, padding=True, truncation=True, return_tensors='tf')\n",
    "    quora_inputs['input_mask'] = quora_inputs['attention_mask']\n",
    "    quora_inputs['input_type_ids'] = tf.zeros_like(quora_inputs['input_ids'])\n",
    "    del quora_inputs['attention_mask'] # we dont want this\n",
    "\n",
    "    model_outputs = model(**quora_inputs)\n",
    "    quora_sentence_embeddings.append(model_outputs['cls_output'])\n",
    "    \n",
    "# Pack and Normalize\n",
    "quora_sentence_embeddings = tf.nn.l2_normalize(tf.concat(quora_sentence_embeddings, axis=0), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed3459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05121d6e",
   "metadata": {},
   "source": [
    "### Most Similar Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbe40eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(input_question, top_k=10):\n",
    "    quora_inputs = tokenizer([input_question], max_length=max_sequence_length, padding=True, truncation=True, return_tensors='tf')\n",
    "    quora_inputs['input_mask'] = quora_inputs['attention_mask']\n",
    "    quora_inputs['input_type_ids'] = tf.zeros_like(quora_inputs['input_ids'])\n",
    "    del quora_inputs['attention_mask'] # we dont want this\n",
    "    model_outputs = model(**quora_inputs)\n",
    "    query_vector = model_outputs['cls_output']\n",
    "    query_vector = tf.nn.l2_normalize(query_vector, axis=1)\n",
    "\n",
    "    scores = tf.matmul(query_vector, quora_sentence_embeddings, transpose_b=True)\n",
    "    top_k_values = tf.nn.top_k(scores, k=top_k)\n",
    "    for i in range(top_k):\n",
    "        best_index = top_k_values.indices.numpy()[0][i]\n",
    "        best_prob = top_k_values.values.numpy()[0][i]\n",
    "        print(quora_questions[best_index], '-->', best_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb054ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e8a93c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should I propose a girl? --> 0.9225553\n",
      "How do I propose to a girl? --> 0.8723614\n",
      "Which is the most romantic way to propose a girl? --> 0.8557819\n",
      "How do I propose a girl for sex? --> 0.80544627\n",
      "How did you propose your girlfriend? --> 0.69494146\n",
      "What are some of the best and unique ways to propose marriage? --> 0.6611091\n",
      "How can I propose to my crush? --> 0.64724606\n",
      "If I want to propose to a girl should I give her hints in advance? --> 0.6309003\n",
      "What doesit take for a man to propose? --> 0.6253518\n",
      "What is the right time to propose someone ? --> 0.5932445\n"
     ]
    }
   ],
   "source": [
    "input_question = 'What is the best way to propose a girl?'\n",
    "most_similar(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4700a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd5c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the most effective way to get started with Deep Learning? --> 0.83670104\n",
      "How do I learn deep learning in 1 month? --> 0.7423597\n",
      "Why is deep learning so important in machine learning? --> 0.7327932\n",
      "Does Quora use Deep Learning? --> 0.7260519\n",
      "Should a machine learning beginner go straight for deep learning? --> 0.719143\n",
      "Where should I start for machine learning? --> 0.71324116\n",
      "How do i get started on machine learning? --> 0.7123989\n",
      "I am New to Deep Learning. How do I start with Python? --> 0.710938\n",
      "How do I start learning machine learning? --> 0.7106862\n",
      "What is deep learning? How is related to AI and machine learning? --> 0.70124084\n"
     ]
    }
   ],
   "source": [
    "input_question = 'How can I start learning Deep Learning?'\n",
    "most_similar(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437da0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3552f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the must-visit and affordable tourist destinations in India? --> 0.8237094\n",
      "What is the most overrated tourist destination in India? --> 0.7374817\n",
      "What is the best sex tourism destination in India? --> 0.73366314\n",
      "What are the most popular tourist destinations? --> 0.7160436\n",
      "What are the best destination for a solo traveler in India? --> 0.7078299\n",
      "What is the best holiday destination? --> 0.675949\n",
      "Which places I should not visit in India as a Indian? --> 0.6656152\n",
      "What are the best places to go as a tourist? --> 0.66551954\n",
      "Which are some best places to visit in India? --> 0.66457677\n",
      "Which is your best holiday destination? --> 0.6640895\n"
     ]
    }
   ],
   "source": [
    "input_question = 'Best tourist destinations in India'\n",
    "most_similar(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e67afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e26ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your favourite piece of classical music and why? --> 0.75282526\n",
      "What are the benefits of listening to classical music? --> 0.7361862\n",
      "Why do some people only listen to classical music? --> 0.7289536\n",
      "Which music is the best for relaxation? --> 0.6762159\n",
      "Why is classical music better than most pop music? --> 0.6651089\n",
      "What are some classical and operant conditioning in education? --> 0.64240026\n",
      "Classical music in movies? --> 0.6344438\n",
      "Which classic music is this? --> 0.59156764\n",
      "Which ones are some of the most soothing tunes composed on a piano? --> 0.57644486\n",
      "What are the differences between Hindustani classical music and Carnatic music? --> 0.57415533\n"
     ]
    }
   ],
   "source": [
    "input_question = 'Why classical music is so relaxing?'\n",
    "most_similar(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21956dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b77af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
