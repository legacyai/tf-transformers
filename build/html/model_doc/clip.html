<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
<meta property="og:title" content="CLIP" />
  
<meta property="og:type" content="website" />
  
<meta property="og:url" content="model_doc/clip.html" />
  
<meta property="og:description" content="Overview: The CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision]( https://arxiv.org/abs/2103.00020) by Alec R..." />
  
<meta property="og:image" content="png location" />
  
<meta property="og:image:alt" content="CLIP" />
  
<meta name="twitter:image" content="png location">
<meta name="twitter:description" content="State-of-the-art Faster Transformer (NLP,CV,Audio) Based models in Tensorflow 2.0">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CLIP &mdash; TF Transformers   documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom_from_huggingface.css" type="text/css" /><link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sentence Transformer" href="sentence_transformer.html" />
    <link rel="prev" title="Vision Transformer (ViT)" href="vit.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html"><img src="../_static/transformers_mix.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                 
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction_docs/quicktour.html">Quick tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction_docs/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction_docs/philosophy.html">Philosophy</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="mt5.html">MT5</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">CLIP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cliptextconfig">CLIPTextConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clipimageconfig">CLIPImageConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clipmodel">CLIPModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clipencoder">CLIPEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clipimageencoder">CLIPImageEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cliptextencoder">CLIPTextEncoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clipfeatureextractortf">CLIPFeatureExtractorTF</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sentence_transformer.html">Sentence Transformer</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/1_read_write_tfrecords.html">Writing and Reading TFRecords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/2_text_classification_imdb_albert.html">Classify text (MRPC) with Albert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/3_masked_lm_tpu.html">Train (Masked Language Model) with tf-transformers in TPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/4_image_classification_vit_multi_gpu.html">Classify Flowers (Image Classification) with ViT using multi-GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/5_sentence_embedding_roberta_quora_zeroshot.html">Create Sentence Embedding Roberta Model + Zeroshot from Scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/6_prompt_engineering_clip.html">Prompt Engineering using CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/7_gpt2_question_answering_squad.html">GPT2 for QA using Squad V1 ( Causal LM )</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/8_code_code_java_to_csharp_t5.html">Code Java to C# using T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/9_images_tfrecords.html">Read and Write Images as TFRecords</a></li>
</ul>
<p class="caption"><span class="caption-text">TFLite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tflite_tutorials/albert_tflite.html">Albert TFlite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tflite_tutorials/bert_tflite.html">Bert TFLite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tflite_tutorials/roberta_tflite.html">Roberta TFLite</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_usage/text_generation_using_gpt2.html">Text Generation using GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_usage/text_generation_using_t5.html">Text Generation using T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_usage/sentence_transformers.html">Sentence Transformer in tf-transformers</a></li>
</ul>
<p class="caption"><span class="caption-text">Tokenizers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="albert_tokenizer.html">ALBERT Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="bigbird_tokenizer.html">BigBird Roberta Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="t5_tokenizer.html">T5 Tokenizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="clip_feature_extractor.html">CLIP Feature Extractor</a></li>
<li class="toctree-l1"><a class="reference internal" href="vit_feature_extractor.html">ViT Feature Extractor</a></li>
</ul>
<p class="caption"><span class="caption-text">Research</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../research/glue.html">Glue Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/long_block_sequencer.html">Long Block Sequencer</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/gpt2.html">Benchmark GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/t5.html">Benchmark T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/albert.html">Benchmark Albert</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/vit.html">Benchmark ViT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/imagenet_clip_benchmark.html">Benchmark CLIP</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TF Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>CLIP</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/model_doc/clip.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="clip">
<h1>CLIP<a class="headerlink" href="#clip" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¬∂</a></h2>
<p>The CLIP model was proposed in [Learning Transferable Visual Models From Natural Language Supervision](<a class="reference external" href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a>) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. CLIP
(Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be
instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing
for the task, similarly to the zero-shot capabilities of GPT-2 and 3.</p>
<p>The abstract from the paper is the following:</p>
<p><em>State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This
restricted form of supervision limits their generality and usability since additional labeled data is needed to specify
any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a
much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes
with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400
million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference
learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study
the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks
such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The
model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need
for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot
without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained
model weights at this https URL.</em></p>
<p>CLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image
classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text
features. Both the text and visual features are then projected to a latent space with identical dimension. The dot
product between the projected image and text features is then used as a similar score.</p>
<p><a class="reference external" href="https://arxiv.org/abs/2103.00020">PaperüëÜ</a>
<a class="reference external" href="https://github.com/openai/CLIP">Official CodeüëÜ</a></p>
</div>
<div class="section" id="cliptextconfig">
<h2>CLIPTextConfig<a class="headerlink" href="#cliptextconfig" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="tf_transformers.models.clip.CLIPTextConfig">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">tf_transformers.models.clip.</span></code><code class="sig-name descname"><span class="pre">CLIPTextConfig</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">49408</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_hidden_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_head_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'quick_gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'quick_gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_probs_dropout_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_position_embeddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">77</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.02</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_embedding_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tf_transformers/models/clip/configuration_clip.html#CLIPTextConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tf_transformers.models.clip.CLIPTextConfig" title="Permalink to this definition">¬∂</a></dt>
<dd><p>This is the configuration class to store the configuration of a <code class="xref py py-class docutils literal notranslate"><span class="pre">ViTModel</span></code>.
It is used to instantiate an ViT model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ViT <a class="reference external" href="https://huggingface.co/google/vit-base-patch16-224">google/vit-base-patch16-224</a> architecture.</p>
<p>Configuration objects inherit from <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerConfig</span></code> and can be used to control the model
outputs. Read the documentation from <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerConfig</span></code> for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 30522) ‚Äì Vocabulary size of the ALBERT model. Defines the number of different tokens that can be represented by the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> passed when calling <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">BertEncoder</span></code>.</p></li>
<li><p><strong>embedding_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 128) ‚Äì Dimensionality of vocabulary embeddings.</p></li>
<li><p><strong>embedding_projection_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Dimensionality of the encoder layers and the pooler layer. Useful for Bert.</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>num_attention_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>attention_head_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Size of attention heads in each layer. Normally (embedding_size//num_attention_heads).</p></li>
<li><p><strong>intermediate_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 3072) ‚Äì The dimensionality of the ‚Äúintermediate‚Äù (often named feed-forward) layer in the Transformer encoder.</p></li>
<li><p><strong>hidden_act</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;gelu&quot;</span></code>) ‚Äì The non-linear activation function (function or string) in the encoder and pooler. If string,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;gelu&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relu&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;silu&quot;</span></code> and many are supported.</p></li>
<li><p><strong>hidden_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 512) ‚Äì The maximum sequence length that this model might ever be used with. Typically set this to something large
(e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>type_vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 2) ‚Äì The vocabulary size of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_type_ids</span></code> passed when calling <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertModel</span></code>.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) ‚Äì The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>layer_norm_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-12) ‚Äì The epsilon used by the layer normalization layers.</p></li>
<li><p><strong>classifier_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) ‚Äì The dropout ratio for attached classifiers.</p></li>
<li><p><strong>position_embedding_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>) ‚Äì Type of position embedding. Choose one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key_query&quot;</span></code>. For positional embeddings use <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>. For more information on
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key&quot;</span></code>, please refer to <a class="reference external" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations (Shaw et al.)</a>. For more information on <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key_query&quot;</span></code>, please refer to
<cite>Method 4</cite> in <a class="reference external" href="https://arxiv.org/abs/2009.13658">Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)</a>.</p></li>
<li><p><strong>num_hidden_groups</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) ‚Äì Number of groups for the hidden layers, parameters in the same group are shared.</p></li>
<li><p><strong>image_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">224</span></code>) ‚Äì The size (resolution) of each image.</p></li>
<li><p><strong>patch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">16</span></code>) ‚Äì The size (resolution) of each patch.</p></li>
<li><p><strong>num_channels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">3</span></code>) ‚Äì The number of input channels.</p></li>
<li><p><strong>num_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">1000</span></code>) ‚Äì Total number of labels by which model has been pre-trained</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tf_transformers.models</span> <span class="kn">import</span> <span class="n">CLIPImageConfig</span><span class="p">,</span> <span class="n">CLIPImageEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing an &#39;openai/clip-vit-base-patch32&#39; style configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">CLIPImageConfig</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">configuration</span><span class="p">[</span><span class="s1">&#39;vision_config&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_config</span>   <span class="o">=</span> <span class="n">configuration</span><span class="p">[</span><span class="s1">&#39;text_config]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a model from the original configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vision_encoder</span> <span class="o">=</span> <span class="n">CLIPImageEncoder</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">vision_config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_encoder</span> <span class="o">=</span> <span class="n">CLIPTextEncoder</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">text_config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">CLIPEncoder</span><span class="p">(</span><span class="n">vision_encoder</span><span class="p">,</span> <span class="n">text_encoder</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_config_dict</span> <span class="c1"># This has more details than original configuration</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># To get a model config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;openai/clip-vit-base-patch32&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CLIPImage</span><span class="o">.</span><span class="n">get_config</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="clipimageconfig">
<h2>CLIPImageConfig<a class="headerlink" href="#clipimageconfig" title="Permalink to this headline">¬∂</a></h2>
<dl class="py class">
<dt id="tf_transformers.models.clip.CLIPImageConfig">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">tf_transformers.models.clip.</span></code><code class="sig-name descname"><span class="pre">CLIPImageConfig</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_hidden_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_head_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'quick_gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'quick_gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_probs_dropout_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_position_embeddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">type_vocab_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.02</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_embedding_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">224</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">projection_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tf_transformers/models/clip/configuration_clip.html#CLIPImageConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tf_transformers.models.clip.CLIPImageConfig" title="Permalink to this definition">¬∂</a></dt>
<dd><p>This is the configuration class to store the configuration of a <code class="xref py py-class docutils literal notranslate"><span class="pre">ViTModel</span></code>.
It is used to instantiate an ViT model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ViT <a class="reference external" href="https://huggingface.co/google/vit-base-patch16-224">google/vit-base-patch16-224</a> architecture.</p>
<p>Configuration objects inherit from <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerConfig</span></code> and can be used to control the model
outputs. Read the documentation from <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformerConfig</span></code> for more information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 30522) ‚Äì Vocabulary size of the ALBERT model. Defines the number of different tokens that can be represented by the
<code class="xref py py-obj docutils literal notranslate"><span class="pre">inputs_ids</span></code> passed when calling <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">BertEncoder</span></code>.</p></li>
<li><p><strong>embedding_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 128) ‚Äì Dimensionality of vocabulary embeddings.</p></li>
<li><p><strong>embedding_projection_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Dimensionality of the encoder layers and the pooler layer. Useful for Bert.</p></li>
<li><p><strong>num_hidden_layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of hidden layers in the Transformer encoder.</p></li>
<li><p><strong>num_attention_heads</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 12) ‚Äì Number of attention heads for each attention layer in the Transformer encoder.</p></li>
<li><p><strong>attention_head_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>) ‚Äì Size of attention heads in each layer. Normally (embedding_size//num_attention_heads).</p></li>
<li><p><strong>intermediate_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 3072) ‚Äì The dimensionality of the ‚Äúintermediate‚Äù (often named feed-forward) layer in the Transformer encoder.</p></li>
<li><p><strong>hidden_act</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code> or <code class="xref py py-obj docutils literal notranslate"><span class="pre">Callable</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;gelu&quot;</span></code>) ‚Äì The non-linear activation function (function or string) in the encoder and pooler. If string,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;gelu&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relu&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;silu&quot;</span></code> and many are supported.</p></li>
<li><p><strong>hidden_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0) ‚Äì The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p></li>
<li><p><strong>max_position_embeddings</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 512) ‚Äì The maximum sequence length that this model might ever be used with. Typically set this to something large
(e.g., 512 or 1024 or 2048).</p></li>
<li><p><strong>type_vocab_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 2) ‚Äì The vocabulary size of the <code class="xref py py-obj docutils literal notranslate"><span class="pre">token_type_ids</span></code> passed when calling <code class="xref py py-class docutils literal notranslate"><span class="pre">BertModel</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">TFBertModel</span></code>.</p></li>
<li><p><strong>initializer_range</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.02) ‚Äì The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p></li>
<li><p><strong>layer_norm_epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 1e-12) ‚Äì The epsilon used by the layer normalization layers.</p></li>
<li><p><strong>classifier_dropout_prob</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, <cite>optional</cite>, defaults to 0.1) ‚Äì The dropout ratio for attached classifiers.</p></li>
<li><p><strong>position_embedding_type</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">str</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>) ‚Äì Type of position embedding. Choose one of <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key&quot;</span></code>,
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key_query&quot;</span></code>. For positional embeddings use <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;absolute&quot;</span></code>. For more information on
<code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key&quot;</span></code>, please refer to <a class="reference external" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations (Shaw et al.)</a>. For more information on <code class="xref py py-obj docutils literal notranslate"><span class="pre">&quot;relative_key_query&quot;</span></code>, please refer to
<cite>Method 4</cite> in <a class="reference external" href="https://arxiv.org/abs/2009.13658">Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)</a>.</p></li>
<li><p><strong>num_hidden_groups</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to 1) ‚Äì Number of groups for the hidden layers, parameters in the same group are shared.</p></li>
<li><p><strong>image_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">224</span></code>) ‚Äì The size (resolution) of each image.</p></li>
<li><p><strong>patch_size</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">16</span></code>) ‚Äì The size (resolution) of each patch.</p></li>
<li><p><strong>num_channels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">3</span></code>) ‚Äì The number of input channels.</p></li>
<li><p><strong>num_labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">1000</span></code>) ‚Äì Total number of labels by which model has been pre-trained</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tf_transformers.models</span> <span class="kn">import</span> <span class="n">CLIPImageConfig</span><span class="p">,</span> <span class="n">CLIPImageEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing an &#39;google/vit-base-patch16-224&#39; style configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">CLIPImageConfig</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing an ViT different style configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration_new</span> <span class="o">=</span> <span class="n">CLIPImageConfig</span><span class="p">(</span>
<span class="gp">... </span>     <span class="n">embedding_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
<span class="gp">... </span>     <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
<span class="gp">... </span>     <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
<span class="gp">... </span> <span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initializing a model from the original configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">CLIPImageEncoder</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Accessing the model configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">_config_dict</span> <span class="c1"># This has more details than original configuration</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># To get a config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;openai/clip-vit-base-patch32&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">CLIPImage</span><span class="o">.</span><span class="n">get_config</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="clipmodel">
<h2>CLIPModel<a class="headerlink" href="#clipmodel" title="Permalink to this headline">¬∂</a></h2>
</div>
<div class="section" id="clipencoder">
<h2>CLIPEncoder<a class="headerlink" href="#clipencoder" title="Permalink to this headline">¬∂</a></h2>
</div>
<div class="section" id="clipimageencoder">
<h2>CLIPImageEncoder<a class="headerlink" href="#clipimageencoder" title="Permalink to this headline">¬∂</a></h2>
</div>
<div class="section" id="cliptextencoder">
<h2>CLIPTextEncoder<a class="headerlink" href="#cliptextencoder" title="Permalink to this headline">¬∂</a></h2>
</div>
<div class="section" id="clipfeatureextractortf">
<h2>CLIPFeatureExtractorTF<a class="headerlink" href="#clipfeatureextractortf" title="Permalink to this headline">¬∂</a></h2>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="vit.html" class="btn btn-neutral float-left" title="Vision Transformer (ViT)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sentence_transformer.html" class="btn btn-neutral float-right" title="Sentence Transformer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, The TFT Team, Licenced under the Apache License, Version 2.0.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-83738774-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-83738774-2', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>